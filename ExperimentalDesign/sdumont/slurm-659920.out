sdumont8072
Norm of error 0.000101716 iterations 641
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:31:12 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           2.229e+00     1.000   2.229e+00
Objects:              2.600e+01     1.000   2.600e+01
Flop:                 9.603e+08     1.001   9.602e+08  1.920e+10
Flop/sec:             4.309e+08     1.001   4.308e+08  8.616e+09
MPI Messages:         1.288e+03     2.000   1.224e+03  2.447e+04
MPI Message Lengths:  1.028e+07     2.000   7.981e+03  1.953e+08
MPI Reductions:       1.943e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 2.2289e+00 100.0%  1.9204e+10 100.0%  2.447e+04 100.0%  7.981e+03      100.0%  1.936e+03  99.6%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 6.7823e-0383.3 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 5.8511e-03113.1 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult              642 1.0 7.3635e-01 1.1 2.89e+08 1.0 2.4e+04 8.0e+03 0.0e+00 32 30100100  0  32 30100100  0  7840       0      0 0.00e+00    0 0.00e+00  0
MatSolve             642 1.0 7.7188e-01 1.1 2.86e+08 1.0 0.0e+00 0.0e+00 0.0e+00 33 30  0  0  0  33 30  0  0  0  7416       0      0 0.00e+00    0 0.00e+00  0
MatLUFactorNum         1 1.0 7.6819e-03 1.0 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1413       0      0 0.00e+00    0 0.00e+00  0
MatILUFactorSym        1 1.0 3.4585e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 5.9025e-0355.9 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7368e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetRowIJ            1 1.0 2.7150e-06 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetOrdering         1 1.0 6.9802e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecTDot             1282 1.0 1.4742e-01 1.4 1.28e+08 1.0 0.0e+00 0.0e+00 1.3e+03  6 13  0  0 66   6 13  0  0 66 17393       0      0 0.00e+00    0 0.00e+00  0
VecNorm              643 1.0 1.3080e-01 2.1 6.43e+07 1.0 0.0e+00 0.0e+00 6.4e+02  4  7  0  0 33   4  7  0  0 33  9832       0      0 0.00e+00    0 0.00e+00  0
VecCopy                2 1.0 1.5366e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet               645 1.0 4.8508e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             1283 1.0 5.8741e-02 1.1 1.28e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3 13  0  0  0   3 13  0  0  0 43684       0      0 0.00e+00    0 0.00e+00  0
VecAYPX              640 1.0 8.6471e-02 1.1 6.40e+07 1.0 0.0e+00 0.0e+00 0.0e+00  4  7  0  0  0   4  7  0  0  0 14803       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin      642 1.0 8.9815e-03 1.4 0.00e+00 0.0 2.4e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd        642 1.0 1.0494e-01 8.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.8230e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.2082e-03 3.1 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin       642 1.0 8.1351e-03 1.4 0.00e+00 0.0 2.4e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd         642 1.0 1.0432e-01 8.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack               642 1.0 1.4849e-03 5.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack             642 1.0 3.4368e-04 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 6.4201e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 1.8505e+00 1.0 9.60e+08 1.0 2.4e+04 8.0e+03 1.9e+03 83100100100 99  83100100100 99 10370       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                2 1.0 2.6835e-02 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0   404       0      0 0.00e+00    0 0.00e+00  0
PCSetUpOnBlocks        1 1.0 1.1864e-02 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0   915       0      0 0.00e+00    0 0.00e+00  0
PCApply              642 1.0 7.8096e-01 1.1 2.86e+08 1.0 0.0e+00 0.0e+00 0.0e+00 33 30  0  0  0  33 30  0  0  0  7330       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              4     11791508     0.
         Vec Scatter     1              1          816     0.
              Vector    10             10      2424896     0.
           Index Set     5              5       608512     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     2              2         2904     0.
      Preconditioner     2              2         1944     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.03e-08
Average time for MPI_Barrier(): 1.16164e-05
Average time for zero size MPI_Send(): 6.36115e-06
#PETSc Option Table entries:
-ksp_type cg
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type bjacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 4.69193e-05 iterations 1715
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:31:17 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.262e+00     1.000   3.262e+00
Objects:              1.900e+01     1.000   1.900e+01
Flop:                 1.887e+09     1.002   1.887e+09  3.773e+10
Flop/sec:             5.785e+08     1.002   5.783e+08  1.157e+10
MPI Messages:         3.436e+03     2.000   3.264e+03  6.528e+04
MPI Message Lengths:  2.746e+07     2.000   7.993e+03  5.218e+08
MPI Reductions:       5.167e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.2619e+00 100.0%  3.7730e+10 100.0%  6.528e+04 100.0%  7.993e+03      100.0%  5.160e+03  99.9%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 3.6648e-0343.3 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 3.6298e-0366.6 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             1716 1.0 1.8584e+00 1.0 7.72e+08 1.0 6.5e+04 8.0e+03 0.0e+00 56 41100100  0  56 41100100  0  8303       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 3.6830e-0333.9 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7583e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecTDot             3430 1.0 3.3045e-01 1.3 3.43e+08 1.0 0.0e+00 0.0e+00 3.4e+03  9 18  0  0 66   9 18  0  0 66 20759       0      0 0.00e+00    0 0.00e+00  0
VecNorm             1717 1.0 1.6547e-01 1.2 1.72e+08 1.0 0.0e+00 0.0e+00 1.7e+03  5  9  0  0 33   5  9  0  0 33 20753       0      0 0.00e+00    0 0.00e+00  0
VecCopy                2 1.0 9.7022e-05 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 3 1.0 2.1709e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             3431 1.0 1.6118e-01 1.1 3.43e+08 1.0 0.0e+00 0.0e+00 0.0e+00  5 18  0  0  0   5 18  0  0  0 42574       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             1714 1.0 2.2315e-01 1.1 1.71e+08 1.0 0.0e+00 0.0e+00 0.0e+00  7  9  0  0  0   7  9  0  0  0 15362       0      0 0.00e+00    0 0.00e+00  0
VecPointwiseMult    1716 1.0 2.4593e-01 1.1 8.58e+07 1.0 0.0e+00 0.0e+00 0.0e+00  7  5  0  0  0   7  5  0  0  0  6978       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     1716 1.0 1.8909e-02 1.6 0.00e+00 0.0 6.5e+04 8.0e+03 0.0e+00  1  0100100  0   1  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       1716 1.0 1.5652e-01 4.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  3  0  0  0  0   3  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.6250e-06 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.5271e-03 4.0 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      1716 1.0 1.7009e-02 1.7 0.00e+00 0.0 6.5e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        1716 1.0 1.5479e-01 5.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  3  0  0  0  0   3  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              1716 1.0 3.6413e-0310.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            1716 1.0 8.5173e-04 2.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 6.2521e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.8833e+00 1.0 1.89e+09 1.0 6.5e+04 8.0e+03 5.1e+03 88100100100100  88100100100100 13081       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 7.4400e-07 2.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply             1716 1.0 2.4879e-01 1.1 8.58e+07 1.0 0.0e+00 0.0e+00 2.0e+00  7  5  0  0  0   7  5  0  0  0  6897       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      8013484     0.
         Vec Scatter     1              1          816     0.
              Vector     9              9      2823424     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1488     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.97e-08
Average time for MPI_Barrier(): 1.09332e-05
Average time for zero size MPI_Send(): 6.5916e-06
#PETSc Option Table entries:
-ksp_type cg
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type jacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.00061416 iterations 692
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:31:21 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           2.494e+00     1.000   2.494e+00
Objects:              1.800e+01     1.000   1.800e+01
Flop:                 1.071e+09     1.001   1.071e+09  2.141e+10
Flop/sec:             4.293e+08     1.001   4.293e+08  8.586e+09
MPI Messages:         1.390e+03     2.000   1.320e+03  2.641e+04
MPI Message Lengths:  1.110e+07     2.000   7.983e+03  2.108e+08
MPI Reductions:       2.096e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 2.4939e+00 100.0%  2.1412e+10 100.0%  2.641e+04 100.0%  7.983e+03      100.0%  2.089e+03  99.7%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 2.7455e-0332.4 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 2.7067e-0351.8 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult              693 1.0 7.1022e-01 1.0 3.12e+08 1.0 2.6e+04 8.0e+03 0.0e+00 28 29100100  0  28 29100100  0  8774       0      0 0.00e+00    0 0.00e+00  0
MatSOR               693 1.0 1.0434e+00 1.0 3.44e+08 1.0 0.0e+00 0.0e+00 0.0e+00 41 32  0  0  0  41 32  0  0  0  6587       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 2.7577e-0326.3 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7544e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecTDot             1384 1.0 1.4046e-01 1.1 1.38e+08 1.0 0.0e+00 0.0e+00 1.4e+03  5 13  0  0 66   5 13  0  0 66 19706       0      0 0.00e+00    0 0.00e+00  0
VecNorm              694 1.0 9.1903e-02 1.4 6.94e+07 1.0 0.0e+00 0.0e+00 6.9e+02  3  6  0  0 33   3  6  0  0 33 15103       0      0 0.00e+00    0 0.00e+00  0
VecCopy                2 1.0 1.2243e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 3 1.0 2.1493e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             1385 1.0 7.8865e-02 1.2 1.38e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3 13  0  0  0   3 13  0  0  0 35123       0      0 0.00e+00    0 0.00e+00  0
VecAYPX              691 1.0 9.1490e-02 1.0 6.91e+07 1.0 0.0e+00 0.0e+00 0.0e+00  4  6  0  0  0   4  6  0  0  0 15106       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin      693 1.0 1.1720e-02 1.7 0.00e+00 0.0 2.6e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd        693 1.0 2.5450e-02 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 1.0224e-05 2.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.0543e-03 2.6 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin       693 1.0 1.0671e-02 1.8 0.00e+00 0.0 2.6e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd         693 1.0 2.4518e-02 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack               693 1.0 2.4417e-0311.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack             693 1.0 4.6413e-04 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 6.3274e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.1177e+00 1.0 1.07e+09 1.0 2.6e+04 8.0e+03 2.1e+03 85100100100 99  85100100100 99 10105       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 2.4900e-07 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply              693 1.0 1.0449e+00 1.0 3.44e+08 1.0 0.0e+00 0.0e+00 0.0e+00 41 32  0  0  0  41 32  0  0  0  6578       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector     8              8      2421696     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1488     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.1e-08
Average time for MPI_Barrier(): 1.1662e-05
Average time for zero size MPI_Send(): 6.2921e-06
#PETSc Option Table entries:
-ksp_type cg
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type sor
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.000603072 iterations 383
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:31:26 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           2.990e+00     1.000   2.990e+00
Objects:              4.100e+01     1.000   4.100e+01
Flop:                 1.134e+09     1.001   1.133e+09  2.267e+10
Flop/sec:             3.791e+08     1.001   3.791e+08  7.582e+09
MPI Messages:         1.560e+03     2.000   1.482e+03  2.964e+04
MPI Message Lengths:  1.246e+07     2.000   7.985e+03  2.367e+08
MPI Reductions:       1.192e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 2.9899e+00 100.0%  2.2668e+10 100.0%  2.964e+04 100.0%  7.985e+03      100.0%  1.185e+03  99.4%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 3.1329e-0339.3 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 3.0947e-0366.9 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult              778 1.0 8.5325e-01 1.0 3.50e+08 1.0 3.0e+04 8.0e+03 0.0e+00 28 31100100  0  28 31100100  0  8199       0      0 0.00e+00    0 0.00e+00  0
MatSOR               779 1.0 1.2065e+00 1.0 3.86e+08 1.0 0.0e+00 0.0e+00 0.0e+00 40 34  0  0  0  40 34  0  0  0  6403       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 3.1451e-0332.0 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.8021e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecMDot               10 1.0 4.4272e-03 1.3 5.50e+06 1.0 0.0e+00 0.0e+00 1.0e+01  0  0  0  0  1   0  0  0  0  1 24846       0      0 0.00e+00    0 0.00e+00  0
VecTDot              766 1.0 8.4622e-02 1.1 7.66e+07 1.0 0.0e+00 0.0e+00 7.7e+02  3  7  0  0 64   3  7  0  0 65 18104       0      0 0.00e+00    0 0.00e+00  0
VecNorm              396 1.0 7.5656e-02 1.5 3.96e+07 1.0 0.0e+00 0.0e+00 4.0e+02  2  3  0  0 33   2  3  0  0 33 10468       0      0 0.00e+00    0 0.00e+00  0
VecScale              11 1.0 2.5713e-04 1.1 5.50e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 42779       0      0 0.00e+00    0 0.00e+00  0
VecCopy              771 1.0 3.3994e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet               773 1.0 5.7105e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY              768 1.0 5.1774e-02 1.1 7.68e+07 1.0 0.0e+00 0.0e+00 0.0e+00  2  7  0  0  0   2  7  0  0  0 29668       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             1150 1.0 1.6008e-01 1.0 9.58e+07 1.0 0.0e+00 0.0e+00 0.0e+00  5  8  0  0  0   5  8  0  0  0 11969       0      0 0.00e+00    0 0.00e+00  0
VecAXPBYCZ           384 1.0 6.8966e-02 1.1 9.60e+07 1.0 0.0e+00 0.0e+00 0.0e+00  2  8  0  0  0   2  8  0  0  0 27840       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY              11 1.0 4.8689e-03 1.0 6.50e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 26700       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin      778 1.0 1.6588e-02 1.6 0.00e+00 0.0 3.0e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd        778 1.0 6.2492e-02 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecNormalize          11 1.0 7.7557e-03 1.1 1.65e+06 1.0 0.0e+00 0.0e+00 1.1e+01  0  0  0  0  1   0  0  0  0  1  4255       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 2.1631e-05 4.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.1086e-03 2.9 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin       778 1.0 1.4752e-02 1.7 0.00e+00 0.0 3.0e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd         778 1.0 6.1369e-02 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack               778 1.0 2.9379e-03 9.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack             778 1.0 4.8673e-04 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.4333e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  5  2  1  1  2   5  2  1  1  2  3314       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.6050e+00 1.0 1.13e+09 1.0 3.0e+04 8.0e+03 1.2e+03 87100100100 98  87100100100 99  8697       0      0 0.00e+00    0 0.00e+00  0
KSPGMRESOrthog        10 1.0 1.2624e-02 1.1 1.10e+07 1.0 0.0e+00 0.0e+00 1.0e+01  0  1  0  0  1   0  1  0  0  1 17426       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 1.4294e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  5  2  1  1  2   5  2  1  1  2  3323       0      0 0.00e+00    0 0.00e+00  0
PCApply              395 1.0 1.8447e+00 1.0 7.13e+08 1.0 1.5e+04 8.0e+03 0.0e+00 61 63 49 49  0  61 63 49 49  0  7725       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    27             27     10054528     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     3              3        33504     0.
      Preconditioner     3              3         2936     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.06e-08
Average time for MPI_Barrier(): 9.8612e-06
Average time for zero size MPI_Send(): 6.1802e-06
#PETSc Option Table entries:
-ksp_type cg
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.0125906 iterations 6472
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:31:55 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           2.702e+01     1.000   2.702e+01
Objects:              5.700e+01     1.000   5.700e+01
Flop:                 2.773e+10     1.000   2.773e+10  5.546e+11
Flop/sec:             1.026e+09     1.000   1.026e+09  2.052e+10
MPI Messages:         1.338e+04     2.000   1.271e+04  2.542e+05
MPI Message Lengths:  1.070e+08     2.000   7.998e+03  2.033e+09
MPI Reductions:       1.318e+04     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 2.7020e+01 100.0%  5.5456e+11 100.0%  2.542e+05 100.0%  7.998e+03      100.0%  1.317e+04  99.9%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 1.0103e-0287.8 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 9.0055e-03131.5 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             6688 1.0 6.9780e+00 1.0 3.01e+09 1.0 2.5e+05 8.0e+03 0.0e+00 25 11100100  0  25 11100100  0  8618       0      0 0.00e+00    0 0.00e+00  0
MatSolve            6688 1.0 7.3596e+00 1.0 2.98e+09 1.0 0.0e+00 0.0e+00 0.0e+00 27 11  0  0  0  27 11  0  0  0  8102       0      0 0.00e+00    0 0.00e+00  0
MatLUFactorNum         1 1.0 7.5248e-03 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1442       0      0 0.00e+00    0 0.00e+00  0
MatILUFactorSym        1 1.0 3.4281e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 9.0542e-0375.0 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7207e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetRowIJ            1 1.0 4.2580e-06 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetOrdering         1 1.0 6.9978e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecMDot             6472 1.0 4.9375e+00 1.0 1.00e+10 1.0 0.0e+00 0.0e+00 6.5e+03 18 36  0  0 49  18 36  0  0 49 40598       0      0 0.00e+00    0 0.00e+00  0
VecNorm             6689 1.0 6.3504e-01 1.1 6.69e+08 1.0 0.0e+00 0.0e+00 6.7e+03  2  2  0  0 51   2  2  0  0 51 21067       0      0 0.00e+00    0 0.00e+00  0
VecScale            6688 1.0 1.3332e-01 1.0 3.34e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 50164       0      0 0.00e+00    0 0.00e+00  0
VecCopy              216 1.0 1.9101e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet              6907 1.0 4.1807e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY              432 1.0 2.0859e-02 1.1 4.32e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 41421       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY            6688 1.0 6.3590e+00 1.0 1.07e+10 1.0 0.0e+00 0.0e+00 0.0e+00 23 38  0  0  0  23 38  0  0  0 33559       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     6688 1.0 9.1388e-02 1.5 0.00e+00 0.0 2.5e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       6688 1.0 3.9663e-01 3.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecNormalize        6688 1.0 7.7764e-01 1.1 1.00e+09 1.0 0.0e+00 0.0e+00 6.7e+03  3  4  0  0 51   3  4  0  0 51 25801       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.6760e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.2584e-03 3.0 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      6688 1.0 8.1024e-02 1.7 0.00e+00 0.0 2.5e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        6688 1.0 3.8806e-01 3.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              6688 1.0 2.1806e-02 7.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            6688 1.0 4.2253e-03 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.0349e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.6641e+01 1.0 2.77e+10 1.0 2.5e+05 8.0e+03 1.3e+04 99100100100100  99100100100100 20816       0      0 0.00e+00    0 0.00e+00  0
KSPGMRESOrthog      6472 1.0 1.0893e+01 1.0 2.00e+10 1.0 0.0e+00 0.0e+00 6.5e+03 40 72  0  0 49  40 72  0  0 49 36805       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                2 1.0 1.7423e-02 1.2 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   623       0      0 0.00e+00    0 0.00e+00  0
PCSetUpOnBlocks        1 1.0 1.1646e-02 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   932       0      0 0.00e+00    0 0.00e+00  0
PCApply             6688 1.0 7.8193e+00 1.0 2.98e+09 1.0 0.0e+00 0.0e+00 0.0e+00 29 11  0  0  0  29 11  0  0  0  7626       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              4     11791508     0.
         Vec Scatter     1              1          816     0.
              Vector    41             41     14878464     0.
           Index Set     5              5       608512     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     2              2        20072     0.
      Preconditioner     2              2         1944     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.95e-08
Average time for MPI_Barrier(): 1.24758e-05
Average time for zero size MPI_Send(): 6.3592e-06
#PETSc Option Table entries:
-ksp_type gmres
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type bjacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 186.34 iterations 10000
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:32:29 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.187e+01     1.000   3.187e+01
Objects:              5.000e+01     1.000   5.000e+01
Flop:                 3.876e+10     1.001   3.876e+10  7.752e+11
Flop/sec:             1.216e+09     1.001   1.216e+09  2.432e+10
MPI Messages:         2.067e+04     2.000   1.964e+04  3.928e+05
MPI Message Lengths:  1.654e+08     2.000   7.999e+03  3.142e+09
MPI Reductions:       2.036e+04     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.1870e+01 100.0%  7.7519e+11 100.0%  3.928e+05 100.0%  7.999e+03      100.0%  2.035e+04 100.0%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 3.0047e-0339.5 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 2.9803e-0352.0 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult            10334 1.0 1.0557e+01 1.0 4.65e+09 1.0 3.9e+05 8.0e+03 0.0e+00 33 12100100  0  33 12100100  0  8802       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 3.0310e-0326.2 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7592e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecMDot            10000 1.0 7.9233e+00 1.0 1.55e+10 1.0 0.0e+00 0.0e+00 1.0e+04 24 40  0  0 49  24 40  0  0 49 39100       0      0 0.00e+00    0 0.00e+00  0
VecNorm            10335 1.0 9.6462e-01 1.1 1.03e+09 1.0 0.0e+00 0.0e+00 1.0e+04  3  3  0  0 51   3  3  0  0 51 21428       0      0 0.00e+00    0 0.00e+00  0
VecScale           10334 1.0 2.0860e-01 1.0 5.17e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1  1  0  0  0   1  1  0  0  0 49541       0      0 0.00e+00    0 0.00e+00  0
VecCopy              334 1.0 2.8719e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet               337 1.0 2.1184e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY              668 1.0 3.2041e-02 1.0 6.68e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 41697       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY           10334 1.0 1.0406e+01 1.0 1.65e+10 1.0 0.0e+00 0.0e+00 0.0e+00 32 43  0  0  0  32 43  0  0  0 31693       0      0 0.00e+00    0 0.00e+00  0
VecPointwiseMult   10334 1.0 1.5793e+00 1.0 5.17e+08 1.0 0.0e+00 0.0e+00 0.0e+00  5  1  0  0  0   5  1  0  0  0  6543       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin    10334 1.0 1.4882e-01 1.7 0.00e+00 0.0 3.9e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd      10334 1.0 3.6703e-01 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecNormalize       10334 1.0 1.1834e+00 1.1 1.55e+09 1.0 0.0e+00 0.0e+00 1.0e+04  4  4  0  0 51   4  4  0  0 51 26197       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.7830e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.1509e-03 2.8 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin     10334 1.0 1.3100e-01 1.8 0.00e+00 0.0 3.9e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd       10334 1.0 3.5307e-01 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack             10334 1.0 3.4126e-02 8.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack           10334 1.0 6.2730e-03 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 1.0348e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 3.1489e+01 1.0 3.88e+10 1.0 3.9e+05 8.0e+03 2.0e+04 99100100100100  99100100100100 24617       0      0 0.00e+00    0 0.00e+00  0
KSPGMRESOrthog     10000 1.0 1.7657e+01 1.0 3.10e+10 1.0 0.0e+00 0.0e+00 1.0e+04 55 80  0  0 49  55 80  0  0 49 35092       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 7.1600e-07 2.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply            10334 1.0 1.5941e+00 1.0 5.17e+08 1.0 0.0e+00 0.0e+00 2.0e+00  5  1  0  0  0   5  1  0  0  0  6483       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      8013484     0.
         Vec Scatter     1              1          816     0.
              Vector    40             40     15276992     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1        18656     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.96e-08
Average time for MPI_Barrier(): 1.06584e-05
Average time for zero size MPI_Send(): 6.31655e-06
#PETSc Option Table entries:
-ksp_type gmres
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type jacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.0487126 iterations 10000
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:33:18 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           4.687e+01     1.000   4.687e+01
Objects:              4.900e+01     1.000   4.900e+01
Flop:                 4.337e+10     1.000   4.337e+10  8.673e+11
Flop/sec:             9.252e+08     1.000   9.252e+08  1.850e+10
MPI Messages:         2.067e+04     2.000   1.964e+04  3.928e+05
MPI Message Lengths:  1.654e+08     2.000   7.999e+03  3.142e+09
MPI Reductions:       2.035e+04     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 4.6874e+01 100.0%  8.6733e+11 100.0%  3.928e+05 100.0%  7.999e+03      100.0%  2.035e+04 100.0%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 4.8282e-0362.4 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 4.5248e-0386.6 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult            10334 1.0 1.0496e+01 1.0 4.65e+09 1.0 3.9e+05 8.0e+03 0.0e+00 22 11100100  0  22 11100100  0  8853       0      0 0.00e+00    0 0.00e+00  0
MatSOR             10334 1.0 1.5772e+01 1.0 5.12e+09 1.0 0.0e+00 0.0e+00 0.0e+00 33 12  0  0  0  33 12  0  0  0  6497       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 4.5763e-0344.9 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7637e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecMDot            10000 1.0 8.8630e+00 1.1 1.55e+10 1.0 0.0e+00 0.0e+00 1.0e+04 19 36  0  0 49  19 36  0  0 49 34954       0      0 0.00e+00    0 0.00e+00  0
VecNorm            10335 1.0 1.0026e+00 1.1 1.03e+09 1.0 0.0e+00 0.0e+00 1.0e+04  2  2  0  0 51   2  2  0  0 51 20616       0      0 0.00e+00    0 0.00e+00  0
VecScale           10334 1.0 2.1069e-01 1.0 5.17e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 49049       0      0 0.00e+00    0 0.00e+00  0
VecCopy              334 1.0 2.8746e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet               337 1.0 2.1198e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY              668 1.0 3.2338e-02 1.1 6.68e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 41314       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY           10334 1.0 1.0576e+01 1.0 1.65e+10 1.0 0.0e+00 0.0e+00 0.0e+00 22 38  0  0  0  22 38  0  0  0 31182       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin    10334 1.0 1.4607e-01 1.5 0.00e+00 0.0 3.9e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd      10334 1.0 3.2429e-01 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecNormalize       10334 1.0 1.2215e+00 1.1 1.55e+09 1.0 0.0e+00 0.0e+00 1.0e+04  2  4  0  0 51   2  4  0  0 51 25381       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.5020e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 2.1325e-03 5.5 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin     10334 1.0 1.2904e-01 1.7 0.00e+00 0.0 3.9e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd       10334 1.0 3.1093e-01 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack             10334 1.0 3.6784e-02 8.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack           10334 1.0 5.8286e-03 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 1.0325e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 4.6497e+01 1.0 4.34e+10 1.0 3.9e+05 8.0e+03 2.0e+04 99100100100100  99100100100100 18653       0      0 0.00e+00    0 0.00e+00  0
KSPGMRESOrthog     10000 1.0 1.8820e+01 1.0 3.10e+10 1.0 0.0e+00 0.0e+00 1.0e+04 40 71  0  0 49  40 71  0  0 49 32923       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 2.6400e-07 2.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply            10334 1.0 1.5792e+01 1.0 5.12e+09 1.0 0.0e+00 0.0e+00 0.0e+00 33 12  0  0  0  33 12  0  0  0  6489       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    39             39     14875264     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1        18656     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.14e-08
Average time for MPI_Barrier(): 1.34248e-05
Average time for zero size MPI_Send(): 6.556e-06
#PETSc Option Table entries:
-ksp_type gmres
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type sor
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.00984153 iterations 3132
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:33:45 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           2.510e+01     1.000   2.510e+01
Objects:              7.200e+01     1.000   7.200e+01
Flop:                 1.795e+10     1.001   1.795e+10  3.589e+11
Flop/sec:             7.149e+08     1.001   7.149e+08  1.430e+10
MPI Messages:         1.297e+04     2.000   1.232e+04  2.465e+05
MPI Message Lengths:  1.038e+08     2.000   7.998e+03  1.971e+09
MPI Reductions:       6.411e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 2.5105e+01 100.0%  3.5893e+11 100.0%  2.465e+05 100.0%  7.998e+03      100.0%  6.404e+03  99.9%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 3.7102e-0339.9 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 3.6781e-0378.8 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             6484 1.0 6.6126e+00 1.0 2.92e+09 1.0 2.5e+05 8.0e+03 0.0e+00 26 16100100  0  26 16100100  0  8817       0      0 0.00e+00    0 0.00e+00  0
MatSOR              6485 1.0 9.7827e+00 1.0 3.22e+09 1.0 0.0e+00 0.0e+00 0.0e+00 38 18  0  0  0  38 18  0  0  0  6573       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 3.7294e-0337.6 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7609e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecMDot             3142 1.0 2.8247e+00 1.1 4.85e+09 1.0 0.0e+00 0.0e+00 3.1e+03 11 27  0  0 49  11 27  0  0 49 34335       0      0 0.00e+00    0 0.00e+00  0
VecNorm             3249 1.0 3.4354e-01 1.2 3.25e+08 1.0 0.0e+00 0.0e+00 3.2e+03  1  2  0  0 51   1  2  0  0 51 18915       0      0 0.00e+00    0 0.00e+00  0
VecScale            3248 1.0 6.5643e-02 1.1 1.62e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 49480       0      0 0.00e+00    0 0.00e+00  0
VecCopy             6580 1.0 3.4059e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet              6584 1.0 2.5685e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY              211 1.0 1.0178e-02 1.1 2.11e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 41462       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             6474 1.0 8.8306e-01 1.0 4.86e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3  3  0  0  0   3  3  0  0  0 10997       0      0 0.00e+00    0 0.00e+00  0
VecAXPBYCZ          3237 1.0 5.5559e-01 1.0 8.09e+08 1.0 0.0e+00 0.0e+00 0.0e+00  2  5  0  0  0   2  5  0  0  0 29131       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY            3248 1.0 3.1543e+00 1.0 5.16e+09 1.0 0.0e+00 0.0e+00 0.0e+00 12 29  0  0  0  12 29  0  0  0 32739       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     6484 1.0 1.1004e-01 1.6 0.00e+00 0.0 2.5e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       6484 1.0 2.7419e-01 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecNormalize        3248 1.0 4.1032e-01 1.1 4.87e+08 1.0 0.0e+00 0.0e+00 3.2e+03  2  3  0  0 51   2  3  0  0 51 23747       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.7170e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 2.2278e-03 5.5 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      6484 1.0 9.7822e-02 1.7 0.00e+00 0.0 2.5e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        6484 1.0 2.6549e-01 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              6484 1.0 2.4337e-02 7.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            6484 1.0 4.2402e-03 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.4599e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  1  0  0  0  0   1  0  0  0  0  3254       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.4722e+01 1.0 1.79e+10 1.0 2.5e+05 8.0e+03 6.4e+03 98100100100100  98100100100100 14518       0      0 0.00e+00    0 0.00e+00  0
KSPGMRESOrthog      3142 1.0 5.7657e+00 1.0 9.70e+09 1.0 0.0e+00 0.0e+00 3.1e+03 23 54  0  0 49  23 54  0  0 49 33642       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 1.4524e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  1  0  0  0  0   1  0  0  0  0  3270       0      0 0.00e+00    0 0.00e+00  0
PCApply             3248 1.0 1.5103e+01 1.0 5.97e+09 1.0 1.2e+05 8.0e+03 0.0e+00 60 33 50 50  0  60 33 50 50  0  7900       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    58             58     22508096     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     3              3        50672     0.
      Preconditioner     3              3         2936     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.76e-08
Average time for MPI_Barrier(): 1.2657e-05
Average time for zero size MPI_Send(): 6.3554e-06
#PETSc Option Table entries:
-ksp_type gmres
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.000101716 iterations 641
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:33:50 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.349e+00     1.000   3.349e+00
Objects:              8.700e+01     1.000   8.700e+01
Flop:                 2.892e+09     1.000   2.892e+09  5.784e+10
Flop/sec:             8.635e+08     1.000   8.635e+08  1.727e+10
MPI Messages:         1.288e+03     2.000   1.224e+03  2.447e+04
MPI Message Lengths:  1.028e+07     2.000   7.981e+03  1.953e+08
MPI Reductions:       2.583e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.3494e+00 100.0%  5.7844e+10 100.0%  2.447e+04 100.0%  7.981e+03      100.0%  2.576e+03  99.7%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 4.5795e-0357.6 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 4.5222e-0395.3 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult              642 1.0 6.8374e-01 1.0 2.89e+08 1.0 2.4e+04 8.0e+03 0.0e+00 20 10100100  0  20 10100100  0  8443       0      0 0.00e+00    0 0.00e+00  0
MatSolve             642 1.0 7.3735e-01 1.0 2.86e+08 1.0 0.0e+00 0.0e+00 0.0e+00 22 10  0  0  0  22 10  0  0  0  7763       0      0 0.00e+00    0 0.00e+00  0
MatLUFactorNum         1 1.0 7.4814e-03 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1451       0      0 0.00e+00    0 0.00e+00  0
MatILUFactorSym        1 1.0 3.3986e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 4.5735e-0345.8 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7742e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetRowIJ            1 1.0 2.7200e-06 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetOrdering         1 1.0 7.0020e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecTDot             1282 1.0 1.4281e-01 1.2 1.28e+08 1.0 0.0e+00 0.0e+00 1.3e+03  4  4  0  0 50   4  4  0  0 50 17954       0      0 0.00e+00    0 0.00e+00  0
VecMTDot             640 1.0 5.5107e-01 1.0 9.82e+08 1.0 0.0e+00 0.0e+00 6.4e+02 16 34  0  0 25  16 34  0  0 25 35639       0      0 0.00e+00    0 0.00e+00  0
VecNorm              643 1.0 9.1546e-02 1.6 6.43e+07 1.0 0.0e+00 0.0e+00 6.4e+02  2  2  0  0 25   2  2  0  0 25 14048       0      0 0.00e+00    0 0.00e+00  0
VecScale             640 1.0 2.2582e-02 1.1 3.20e+07 1.0 0.0e+00 0.0e+00 0.0e+00  1  1  0  0  0   1  1  0  0  0 28341       0      0 0.00e+00    0 0.00e+00  0
VecCopy              642 1.0 3.9259e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet               645 1.0 1.4473e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             1283 1.0 7.3012e-02 1.1 1.28e+08 1.0 0.0e+00 0.0e+00 0.0e+00  2  4  0  0  0   2  4  0  0  0 35145       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY             640 1.0 6.3011e-01 1.0 9.82e+08 1.0 0.0e+00 0.0e+00 0.0e+00 19 34  0  0  0  19 34  0  0  0 31169       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin      642 1.0 1.2384e-02 1.5 0.00e+00 0.0 2.4e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd        642 1.0 3.9076e-02 2.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.5450e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.1768e-03 2.9 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin       642 1.0 1.0933e-02 1.5 0.00e+00 0.0 2.4e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd         642 1.0 3.8155e-02 2.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack               642 1.0 2.1970e-03 7.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack             642 1.0 3.7062e-04 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 4.5736e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.9738e+00 1.0 2.89e+09 1.0 2.4e+04 8.0e+03 2.6e+03 89100100100 99  89100100100100 19447       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                2 1.0 2.6513e-02 1.9 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0   409       0      0 0.00e+00    0 0.00e+00  0
PCSetUpOnBlocks        1 1.0 1.1633e-02 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   933       0      0 0.00e+00    0 0.00e+00  0
PCApply              642 1.0 7.5838e-01 1.0 2.86e+08 1.0 0.0e+00 0.0e+00 0.0e+00 22 10  0  0  0  22 10  0  0  0  7548       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              4     11791508     0.
         Vec Scatter     1              1          816     0.
              Vector    71             71     26930304     0.
           Index Set     5              5       608512     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     2              2         4080     0.
      Preconditioner     2              2         1944     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.27e-08
Average time for MPI_Barrier(): 1.22846e-05
Average time for zero size MPI_Send(): 6.42245e-06
#PETSc Option Table entries:
-ksp_type fcg
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type bjacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 4.69193e-05 iterations 1715
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:33:59 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           6.397e+00     1.000   6.397e+00
Objects:              8.000e+01     1.000   8.000e+01
Flop:                 7.104e+09     1.000   7.104e+09  1.421e+11
Flop/sec:             1.110e+09     1.000   1.110e+09  2.221e+10
MPI Messages:         3.436e+03     2.000   3.264e+03  6.528e+04
MPI Message Lengths:  2.746e+07     2.000   7.993e+03  5.218e+08
MPI Reductions:       6.881e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 6.3973e+00 100.0%  1.4208e+11 100.0%  6.528e+04 100.0%  7.993e+03      100.0%  6.874e+03  99.9%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 7.4536e-03108.0 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 7.3995e-03159.5 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             1716 1.0 1.7869e+00 1.0 7.72e+08 1.0 6.5e+04 8.0e+03 0.0e+00 28 11100100  0  28 11100100  0  8635       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 7.4550e-0376.2 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7741e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecTDot             3430 1.0 3.6501e-01 1.2 3.43e+08 1.0 0.0e+00 0.0e+00 3.4e+03  5  5  0  0 50   5  5  0  0 50 18794       0      0 0.00e+00    0 0.00e+00  0
VecMTDot            1714 1.0 1.4028e+00 1.0 2.65e+09 1.0 0.0e+00 0.0e+00 1.7e+03 22 37  0  0 25  22 37  0  0 25 37802       0      0 0.00e+00    0 0.00e+00  0
VecNorm             1717 1.0 1.6111e-01 1.2 1.72e+08 1.0 0.0e+00 0.0e+00 1.7e+03  2  2  0  0 25   2  2  0  0 25 21314       0      0 0.00e+00    0 0.00e+00  0
VecScale            1714 1.0 4.7827e-02 1.1 8.57e+07 1.0 0.0e+00 0.0e+00 0.0e+00  1  1  0  0  0   1  1  0  0  0 35837       0      0 0.00e+00    0 0.00e+00  0
VecCopy             1716 1.0 1.0987e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 3 1.0 2.1965e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             3431 1.0 2.0232e-01 1.1 3.43e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3  5  0  0  0   3  5  0  0  0 33917       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY            1714 1.0 1.7605e+00 1.0 2.65e+09 1.0 0.0e+00 0.0e+00 0.0e+00 27 37  0  0  0  27 37  0  0  0 30122       0      0 0.00e+00    0 0.00e+00  0
VecPointwiseMult    1716 1.0 2.4803e-01 1.0 8.58e+07 1.0 0.0e+00 0.0e+00 0.0e+00  4  1  0  0  0   4  1  0  0  0  6919       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     1716 1.0 2.4351e-02 1.5 0.00e+00 0.0 6.5e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       1716 1.0 8.8433e-02 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.6120e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.1337e-03 2.8 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      1716 1.0 2.1846e-02 1.6 0.00e+00 0.0 6.5e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        1716 1.0 8.6274e-02 2.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              1716 1.0 5.1779e-03 9.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            1716 1.0 1.0292e-03 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 4.5243e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 6.0114e+00 1.0 7.10e+09 1.0 6.5e+04 8.0e+03 6.9e+03 94100100100100  94100100100100 23632       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 1.0050e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply             1716 1.0 2.5260e-01 1.0 8.58e+07 1.0 0.0e+00 0.0e+00 2.0e+00  4  1  0  0  0   4  1  0  0  0  6793       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      8013484     0.
         Vec Scatter     1              1          816     0.
              Vector    70             70     27328832     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         2664     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.05e-08
Average time for MPI_Barrier(): 1.37008e-05
Average time for zero size MPI_Send(): 6.47015e-06
#PETSc Option Table entries:
-ksp_type fcg
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type jacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.000614161 iterations 692
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:34:05 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.932e+00     1.000   3.932e+00
Objects:              7.900e+01     1.000   7.900e+01
Flop:                 3.175e+09     1.000   3.175e+09  6.351e+10
Flop/sec:             8.075e+08     1.000   8.075e+08  1.615e+10
MPI Messages:         1.390e+03     2.000   1.320e+03  2.641e+04
MPI Message Lengths:  1.110e+07     2.000   7.983e+03  2.108e+08
MPI Reductions:       2.787e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.9323e+00 100.0%  6.3505e+10 100.0%  2.641e+04 100.0%  7.983e+03      100.0%  2.780e+03  99.7%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 8.4177e-0399.8 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 8.3527e-03164.3 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult              693 1.0 7.1424e-01 1.0 3.12e+08 1.0 2.6e+04 8.0e+03 0.0e+00 18 10100100  0  18 10100100  0  8725       0      0 0.00e+00    0 0.00e+00  0
MatSOR               693 1.0 1.0548e+00 1.0 3.44e+08 1.0 0.0e+00 0.0e+00 0.0e+00 27 11  0  0  0  27 11  0  0  0  6516       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 8.4031e-0376.7 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7467e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecTDot             1384 1.0 1.5887e-01 1.1 1.38e+08 1.0 0.0e+00 0.0e+00 1.4e+03  4  4  0  0 50   4  4  0  0 50 17423       0      0 0.00e+00    0 0.00e+00  0
VecMTDot             691 1.0 6.8753e-01 1.0 1.07e+09 1.0 0.0e+00 0.0e+00 6.9e+02 17 34  0  0 25  17 34  0  0 25 31114       0      0 0.00e+00    0 0.00e+00  0
VecNorm              694 1.0 9.4319e-02 1.3 6.94e+07 1.0 0.0e+00 0.0e+00 6.9e+02  2  2  0  0 25   2  2  0  0 25 14716       0      0 0.00e+00    0 0.00e+00  0
VecScale             691 1.0 3.1910e-02 1.0 3.46e+07 1.0 0.0e+00 0.0e+00 0.0e+00  1  1  0  0  0   1  1  0  0  0 21655       0      0 0.00e+00    0 0.00e+00  0
VecCopy              693 1.0 4.3724e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 3 1.0 2.2150e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             1385 1.0 9.1518e-02 1.1 1.38e+08 1.0 0.0e+00 0.0e+00 0.0e+00  2  4  0  0  0   2  4  0  0  0 30267       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY             691 1.0 7.0573e-01 1.0 1.07e+09 1.0 0.0e+00 0.0e+00 0.0e+00 18 34  0  0  0  18 34  0  0  0 30312       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin      693 1.0 1.3808e-02 1.5 0.00e+00 0.0 2.6e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd        693 1.0 2.5736e-02 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.9410e-06 2.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.1755e-03 3.1 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin       693 1.0 1.2100e-02 1.6 0.00e+00 0.0 2.6e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd         693 1.0 2.4802e-02 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack               693 1.0 2.8081e-03 8.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack             693 1.0 5.6300e-04 2.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 4.5415e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 3.5573e+00 1.0 3.17e+09 1.0 2.6e+04 8.0e+03 2.8e+03 90100100100 99  90100100100100 17848       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 6.2900e-07 3.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply              693 1.0 1.0563e+00 1.0 3.44e+08 1.0 0.0e+00 0.0e+00 0.0e+00 27 11  0  0  0  27 11  0  0  0  6507       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    69             69     26927104     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         2664     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.06e-08
Average time for MPI_Barrier(): 1.0346e-05
Average time for zero size MPI_Send(): 6.30515e-06
#PETSc Option Table entries:
-ksp_type fcg
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type sor
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.000603072 iterations 383
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:34:10 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.783e+00     1.000   3.783e+00
Objects:              1.020e+02     1.000   1.020e+02
Flop:                 2.281e+09     1.001   2.281e+09  4.562e+10
Flop/sec:             6.029e+08     1.001   6.029e+08  1.206e+10
MPI Messages:         1.560e+03     2.000   1.482e+03  2.964e+04
MPI Message Lengths:  1.246e+07     2.000   7.985e+03  2.367e+08
MPI Reductions:       1.574e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.7832e+00 100.0%  4.5618e+10 100.0%  2.964e+04 100.0%  7.985e+03      100.0%  1.567e+03  99.6%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 3.2167e-0343.9 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 3.1326e-0358.2 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult              778 1.0 8.3534e-01 1.0 3.50e+08 1.0 3.0e+04 8.0e+03 0.0e+00 22 15100100  0  22 15100100  0  8375       0      0 0.00e+00    0 0.00e+00  0
MatSOR               779 1.0 1.2179e+00 1.0 3.86e+08 1.0 0.0e+00 0.0e+00 0.0e+00 32 17  0  0  0  32 17  0  0  0  6343       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 3.1860e-0329.3 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7879e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecMDot               10 1.0 4.8479e-03 1.5 5.50e+06 1.0 0.0e+00 0.0e+00 1.0e+01  0  0  0  0  1   0  0  0  0  1 22690       0      0 0.00e+00    0 0.00e+00  0
VecTDot              766 1.0 9.4615e-02 1.1 7.66e+07 1.0 0.0e+00 0.0e+00 7.7e+02  2  3  0  0 49   2  3  0  0 49 16192       0      0 0.00e+00    0 0.00e+00  0
VecMTDot             382 1.0 4.0359e-01 1.0 5.83e+08 1.0 0.0e+00 0.0e+00 3.8e+02 11 26  0  0 24  11 26  0  0 24 28906       0      0 0.00e+00    0 0.00e+00  0
VecNorm              396 1.0 7.4686e-02 1.6 3.96e+07 1.0 0.0e+00 0.0e+00 4.0e+02  2  2  0  0 25   2  2  0  0 25 10604       0      0 0.00e+00    0 0.00e+00  0
VecScale             393 1.0 2.3580e-02 1.1 1.96e+07 1.0 0.0e+00 0.0e+00 0.0e+00  1  1  0  0  0   1  1  0  0  0 16667       0      0 0.00e+00    0 0.00e+00  0
VecCopy             1153 1.0 6.0072e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet               773 1.0 1.7153e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY              768 1.0 5.0833e-02 1.1 7.68e+07 1.0 0.0e+00 0.0e+00 0.0e+00  1  3  0  0  0   1  3  0  0  0 30216       0      0 0.00e+00    0 0.00e+00  0
VecAYPX              768 1.0 1.0884e-01 1.0 5.76e+07 1.0 0.0e+00 0.0e+00 0.0e+00  3  3  0  0  0   3  3  0  0  0 10585       0      0 0.00e+00    0 0.00e+00  0
VecAXPBYCZ           384 1.0 6.8411e-02 1.1 9.60e+07 1.0 0.0e+00 0.0e+00 0.0e+00  2  4  0  0  0   2  4  0  0  0 28066       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY             393 1.0 3.7865e-01 1.0 5.90e+08 1.0 0.0e+00 0.0e+00 0.0e+00 10 26  0  0  0  10 26  0  0  0 31153       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin      778 1.0 1.6168e-02 1.6 0.00e+00 0.0 3.0e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd        778 1.0 5.3927e-02 2.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecNormalize          11 1.0 8.0948e-03 1.1 1.65e+06 1.0 0.0e+00 0.0e+00 1.1e+01  0  0  0  0  1   0  0  0  0  1  4077       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.8240e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 2.3491e-03 6.1 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin       778 1.0 1.4307e-02 1.6 0.00e+00 0.0 3.0e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd         778 1.0 5.2928e-02 2.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack               778 1.0 2.9886e-0310.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack             778 1.0 5.9072e-04 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.5176e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  4  1  1  1  1   4  1  1  1  1  3130       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 3.3958e+00 1.0 2.28e+09 1.0 3.0e+04 8.0e+03 1.6e+03 90100100100 99  90100100100 99 13430       0      0 0.00e+00    0 0.00e+00  0
KSPGMRESOrthog        10 1.0 1.3201e-02 1.1 1.10e+07 1.0 0.0e+00 0.0e+00 1.0e+01  0  0  0  0  1   0  0  0  0  1 16665       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 1.4760e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  4  1  1  1  1   4  1  1  1  1  3218       0      0 0.00e+00    0 0.00e+00  0
PCApply              395 1.0 1.8522e+00 1.0 7.13e+08 1.0 1.5e+04 8.0e+03 0.0e+00 49 31 49 49  0  49 31 49 49  0  7694       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    88             88     34559936     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     3              3        34680     0.
      Preconditioner     3              3         2936     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.96e-08
Average time for MPI_Barrier(): 1.10578e-05
Average time for zero size MPI_Send(): 6.53805e-06
#PETSc Option Table entries:
-ksp_type fcg
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 8.47251e-05 iterations 1671
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:34:26 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           1.403e+01     1.000   1.403e+01
Objects:              4.000e+01     1.000   4.000e+01
Flop:                 7.332e+09     1.001   7.331e+09  1.466e+11
Flop/sec:             5.227e+08     1.001   5.226e+08  1.045e+10
MPI Messages:         1.003e+04     2.000   9.530e+03  1.906e+05
MPI Message Lengths:  8.023e+07     2.000   7.998e+03  1.524e+09
MPI Reductions:       8.375e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 1.4028e+01 100.0%  1.4662e+11 100.0%  1.906e+05 100.0%  7.998e+03      100.0%  8.368e+03  99.9%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 4.6205e-0332.1 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 4.4734e-0390.1 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             5014 1.0 5.1331e+00 1.0 2.26e+09 1.0 1.9e+05 8.0e+03 0.0e+00 36 31100100  0  36 31100100  0  8783       0      0 0.00e+00    0 0.00e+00  0
MatSolve            5014 1.0 5.4782e+00 1.0 2.24e+09 1.0 0.0e+00 0.0e+00 0.0e+00 39 30  0  0  0  39 30  0  0  0  8161       0      0 0.00e+00    0 0.00e+00  0
MatLUFactorNum         1 1.0 7.5306e-03 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1441       0      0 0.00e+00    0 0.00e+00  0
MatILUFactorSym        1 1.0 3.3923e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 4.5236e-0344.9 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7838e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetRowIJ            1 1.0 2.9280e-06 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetOrdering         1 1.0 7.0044e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot              5013 1.0 5.1489e-01 1.1 5.01e+08 1.0 0.0e+00 0.0e+00 5.0e+03  3  7  0  0 60   3  7  0  0 60 19472       0      0 0.00e+00    0 0.00e+00  0
VecNorm             3344 1.0 3.9320e-01 1.4 3.34e+08 1.0 0.0e+00 0.0e+00 3.3e+03  2  5  0  0 40   2  5  0  0 40 17009       0      0 0.00e+00    0 0.00e+00  0
VecScale            6685 1.0 1.4036e-01 1.0 3.34e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1  5  0  0  0   1  5  0  0  0 47628       0      0 0.00e+00    0 0.00e+00  0
VecCopy            15043 1.0 7.1788e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  5  0  0  0  0   5  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet              5022 1.0 3.0700e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY            15037 1.0 8.5349e-01 1.0 1.50e+09 1.0 0.0e+00 0.0e+00 0.0e+00  6 21  0  0  0   6 21  0  0  0 35236       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             1671 1.0 2.2694e-01 1.0 1.67e+08 1.0 0.0e+00 0.0e+00 0.0e+00  2  2  0  0  0   2  2  0  0  0 14726       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     5014 1.0 8.4430e-02 1.7 0.00e+00 0.0 1.9e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       5014 1.0 2.0928e-01 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.4140e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.0696e-03 2.8 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      5014 1.0 7.5652e-02 1.9 0.00e+00 0.0 1.9e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        5014 1.0 2.0357e-01 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              5014 1.0 1.8432e-02 8.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            5014 1.0 2.3671e-03 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 3.4516e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 1.3646e+01 1.0 7.33e+09 1.0 1.9e+05 8.0e+03 8.4e+03 97100100100100  97100100100100 10743       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                2 1.0 2.5685e-02 1.8 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   423       0      0 0.00e+00    0 0.00e+00  0
PCSetUpOnBlocks        1 1.0 1.1658e-02 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   931       0      0 0.00e+00    0 0.00e+00  0
PCApply             5014 1.0 5.8287e+00 1.0 2.24e+09 1.0 0.0e+00 0.0e+00 0.0e+00 41 30  0  0  0  41 30  0  0  0  7670       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              4     11791508     0.
         Vec Scatter     1              1          816     0.
              Vector    24             24      8049088     0.
           Index Set     5              5       608512     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     2              2         2832     0.
      Preconditioner     2              2         1944     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.05e-08
Average time for MPI_Barrier(): 1.37728e-05
Average time for zero size MPI_Send(): 6.4795e-06
#PETSc Option Table entries:
-ksp_type tcqmr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type bjacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 2.62209e-05 iterations 6622
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:35:02 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.376e+01     1.000   3.376e+01
Objects:              3.300e+01     1.000   3.300e+01
Flop:                 2.119e+10     1.002   2.118e+10  4.237e+11
Flop/sec:             6.275e+08     1.002   6.274e+08  1.255e+10
MPI Messages:         3.974e+04     2.000   3.775e+04  7.550e+05
MPI Message Lengths:  3.179e+08     2.000   7.999e+03  6.040e+09
MPI Reductions:       3.313e+04     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.3764e+01 100.0%  4.2366e+11 100.0%  7.550e+05 100.0%  7.999e+03      100.0%  3.312e+04 100.0%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 4.1909e-0349.4 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 4.1661e-0371.4 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult            19867 1.0 2.0077e+01 1.0 8.94e+09 1.0 7.5e+05 8.0e+03 0.0e+00 59 42100100  0  59 42100100  0  8898       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 4.2197e-0337.9 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7195e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot             19866 1.0 1.8743e+00 1.1 1.99e+09 1.0 0.0e+00 0.0e+00 2.0e+04  5  9  0  0 60   5  9  0  0 60 21198       0      0 0.00e+00    0 0.00e+00  0
VecNorm            13246 1.0 1.3157e+00 1.3 1.32e+09 1.0 0.0e+00 0.0e+00 1.3e+04  3  6  0  0 40   3  6  0  0 40 20135       0      0 0.00e+00    0 0.00e+00  0
VecScale           26489 1.0 5.0471e-01 1.0 1.32e+09 1.0 0.0e+00 0.0e+00 0.0e+00  1  6  0  0  0   1  6  0  0  0 52484       0      0 0.00e+00    0 0.00e+00  0
VecCopy            59602 1.0 2.8498e+00 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  8  0  0  0  0   8  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 8 1.0 5.2861e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY            59596 1.0 3.4426e+00 1.0 5.96e+09 1.0 0.0e+00 0.0e+00 0.0e+00 10 28  0  0  0  10 28  0  0  0 34623       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             6622 1.0 8.6852e-01 1.0 6.62e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3  3  0  0  0   3  3  0  0  0 15249       0      0 0.00e+00    0 0.00e+00  0
VecPointwiseMult   19867 1.0 2.8805e+00 1.0 9.93e+08 1.0 0.0e+00 0.0e+00 0.0e+00  8  5  0  0  0   8  5  0  0  0  6897       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin    19867 1.0 2.9326e-01 1.7 0.00e+00 0.0 7.5e+05 8.0e+03 0.0e+00  1  0100100  0   1  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd      19867 1.0 6.8147e-01 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.4240e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.2892e-03 3.4 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin     19867 1.0 2.5957e-01 1.8 0.00e+00 0.0 7.5e+05 8.0e+03 0.0e+00  1  0100100  0   1  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd       19867 1.0 6.6210e-01 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack             19867 1.0 7.2037e-02 8.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack           19867 1.0 1.1090e-02 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 3.4956e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 3.3389e+01 1.0 2.12e+10 1.0 7.5e+05 8.0e+03 3.3e+04 99100100100100  99100100100100 12688       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 9.7700e-07 2.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply            19867 1.0 2.9043e+00 1.0 9.93e+08 1.0 0.0e+00 0.0e+00 2.0e+00  9  5  0  0  0   9  5  0  0  0  6841       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      8013484     0.
         Vec Scatter     1              1          816     0.
              Vector    23             23      8447616     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1416     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.08e-08
Average time for MPI_Barrier(): 1.39648e-05
Average time for zero size MPI_Send(): 6.5151e-06
#PETSc Option Table entries:
-ksp_type tcqmr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type jacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.000177372 iterations 1033
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:35:14 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           1.003e+01     1.000   1.003e+01
Objects:              3.200e+01     1.000   3.200e+01
Flop:                 4.688e+09     1.001   4.687e+09  9.374e+10
Flop/sec:             4.673e+08     1.001   4.673e+08  9.346e+09
MPI Messages:         6.204e+03     2.000   5.894e+03  1.179e+05
MPI Message Lengths:  4.961e+07     2.000   7.996e+03  9.426e+08
MPI Reductions:       5.185e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 1.0030e+01 100.0%  9.3739e+10 100.0%  1.179e+05 100.0%  7.996e+03      100.0%  5.178e+03  99.9%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 7.1631e-0374.4 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 6.9655e-03116.8 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             3100 1.0 3.1947e+00 1.0 1.39e+09 1.0 1.2e+05 8.0e+03 0.0e+00 32 30100100  0  32 30100100  0  8726       0      0 0.00e+00    0 0.00e+00  0
MatSOR              3100 1.0 4.6539e+00 1.0 1.54e+09 1.0 0.0e+00 0.0e+00 0.0e+00 46 33  0  0  0  46 33  0  0  0  6605       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 7.0144e-0362.0 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7060e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot              3099 1.0 3.6404e-01 1.2 3.10e+08 1.0 0.0e+00 0.0e+00 3.1e+03  3  7  0  0 60   3  7  0  0 60 17025       0      0 0.00e+00    0 0.00e+00  0
VecNorm             2068 1.0 2.8581e-01 1.5 2.07e+08 1.0 0.0e+00 0.0e+00 2.1e+03  2  4  0  0 40   2  4  0  0 40 14471       0      0 0.00e+00    0 0.00e+00  0
VecScale            4133 1.0 8.8897e-02 1.0 2.07e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1  4  0  0  0   1  4  0  0  0 46492       0      0 0.00e+00    0 0.00e+00  0
VecCopy             9301 1.0 5.0649e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  5  0  0  0  0   5  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 8 1.0 5.3857e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             9295 1.0 5.5857e-01 1.0 9.30e+08 1.0 0.0e+00 0.0e+00 0.0e+00  5 20  0  0  0   5 20  0  0  0 33281       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             1033 1.0 1.5330e-01 1.1 1.03e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1  2  0  0  0   1  2  0  0  0 13477       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     3100 1.0 6.3218e-02 1.9 0.00e+00 0.0 1.2e+05 8.0e+03 0.0e+00  1  0100100  0   1  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       3100 1.0 1.3354e-01 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.6830e-06 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.4553e-03 3.7 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      3100 1.0 5.6035e-02 2.0 0.00e+00 0.0 1.2e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        3100 1.0 1.2991e-01 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              3100 1.0 1.5279e-0210.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            3100 1.0 1.8834e-03 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 3.5221e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 9.6572e+00 1.0 4.69e+09 1.0 1.2e+05 8.0e+03 5.2e+03 96100100100100  96100100100100  9705       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 4.9400e-07 3.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply             3100 1.0 4.6600e+00 1.0 1.54e+09 1.0 0.0e+00 0.0e+00 0.0e+00 46 33  0  0  0  46 33  0  0  0  6597       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    22             22      8045888     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1416     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.1e-08
Average time for MPI_Barrier(): 1.15468e-05
Average time for zero size MPI_Send(): 6.20355e-06
#PETSc Option Table entries:
-ksp_type tcqmr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type sor
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.000147819 iterations 474
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:35:26 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           9.671e+00     1.000   9.671e+00
Objects:              5.500e+01     1.000   5.500e+01
Flop:                 4.090e+09     1.001   4.089e+09  8.179e+10
Flop/sec:             4.229e+08     1.001   4.229e+08  8.457e+09
MPI Messages:         5.716e+03     2.000   5.430e+03  1.086e+05
MPI Message Lengths:  4.570e+07     2.000   7.996e+03  8.684e+08
MPI Reductions:       2.413e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 9.6709e+00 100.0%  8.1788e+10 100.0%  1.086e+05 100.0%  7.996e+03      100.0%  2.406e+03  99.7%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 6.1054e-0380.5 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 5.0570e-03107.4 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             2856 1.0 3.0603e+00 1.0 1.28e+09 1.0 1.1e+05 8.0e+03 0.0e+00 31 31100100  0  31 31100100  0  8392       0      0 0.00e+00    0 0.00e+00  0
MatSOR              2857 1.0 4.3920e+00 1.0 1.42e+09 1.0 0.0e+00 0.0e+00 0.0e+00 45 35  0  0  0  45 35  0  0  0  6451       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 5.1064e-0351.2 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7684e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot              1422 1.0 2.0267e-01 1.4 1.42e+08 1.0 0.0e+00 0.0e+00 1.4e+03  2  3  0  0 59   2  3  0  0 59 14032       0      0 0.00e+00    0 0.00e+00  0
VecMDot               10 1.0 4.0831e-03 1.2 5.50e+06 1.0 0.0e+00 0.0e+00 1.0e+01  0  0  0  0  0   0  0  0  0  0 26940       0      0 0.00e+00    0 0.00e+00  0
VecNorm              961 1.0 2.2170e-01 2.3 9.61e+07 1.0 0.0e+00 0.0e+00 9.6e+02  2  2  0  0 40   2  2  0  0 40  8669       0      0 0.00e+00    0 0.00e+00  0
VecScale            1908 1.0 4.2641e-02 1.1 9.54e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  2  0  0  0   0  2  0  0  0 44746       0      0 0.00e+00    0 0.00e+00  0
VecCopy             7117 1.0 3.9274e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  4  0  0  0  0   4  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet              2856 1.0 1.1298e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             4265 1.0 2.4781e-01 1.0 4.26e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3 10  0  0  0   3 10  0  0  0 34422       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             3320 1.0 4.6645e-01 1.0 2.61e+08 1.0 0.0e+00 0.0e+00 0.0e+00  5  6  0  0  0   5  6  0  0  0 11184       0      0 0.00e+00    0 0.00e+00  0
VecAXPBYCZ          1423 1.0 2.4986e-01 1.0 3.56e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3  9  0  0  0   3  9  0  0  0 28476       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY              11 1.0 4.8986e-03 1.0 6.50e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 26538       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     2856 1.0 4.8568e-02 1.5 0.00e+00 0.0 1.1e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       2856 1.0 2.4002e-01 2.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecNormalize          11 1.0 7.9247e-03 1.1 1.65e+06 1.0 0.0e+00 0.0e+00 1.1e+01  0  0  0  0  0   0  0  0  0  0  4164       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.7360e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.1641e-03 3.0 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      2856 1.0 4.2930e-02 1.6 0.00e+00 0.0 1.1e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        2856 1.0 2.3641e-01 2.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              2856 1.0 1.0584e-02 9.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            2856 1.0 1.4851e-03 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.4902e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  2  1  0  0  1   2  1  0  0  1  3187       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 9.2840e+00 1.0 4.09e+09 1.0 1.1e+05 8.0e+03 2.4e+03 96100100100 99  96100100100100  8808       0      0 0.00e+00    0 0.00e+00  0
KSPGMRESOrthog        10 1.0 1.2482e-02 1.1 1.10e+07 1.0 0.0e+00 0.0e+00 1.0e+01  0  0  0  0  0   0  0  0  0  0 17626       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 1.4578e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  1  1  0  0  1   1  1  0  0  1  3258       0      0 0.00e+00    0 0.00e+00  0
PCApply             1434 1.0 6.8027e+00 1.0 2.63e+09 1.0 5.4e+04 8.0e+03 0.0e+00 70 64 50 50  0  70 64 50 50  0  7719       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    41             41     15678720     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     3              3        33432     0.
      Preconditioner     3              3         2936     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.87e-08
Average time for MPI_Barrier(): 1.03818e-05
Average time for zero size MPI_Send(): 6.1115e-06
#PETSc Option Table entries:
-ksp_type tcqmr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 7.08235e-05 iterations 490
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:35:31 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.017e+00     1.000   3.017e+00
Objects:              3.000e+01     1.000   3.000e+01
Flop:                 1.345e+09     1.001   1.345e+09  2.689e+10
Flop/sec:             4.457e+08     1.001   4.456e+08  8.912e+09
MPI Messages:         1.966e+03     2.000   1.868e+03  3.735e+04
MPI Message Lengths:  1.570e+07     2.000   7.988e+03  2.984e+08
MPI Reductions:       1.491e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.0173e+00 100.0%  2.6891e+10 100.0%  3.735e+04 100.0%  7.988e+03      100.0%  1.484e+03  99.5%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 4.6941e-03 7.2 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 4.6038e-03100.8 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult              981 1.0 1.0088e+00 1.0 4.41e+08 1.0 3.7e+04 8.0e+03 0.0e+00 33 33100100  0  33 33100100  0  8744       0      0 0.00e+00    0 0.00e+00  0
MatSolve             981 1.0 1.0931e+00 1.0 4.37e+08 1.0 0.0e+00 0.0e+00 0.0e+00 36 33  0  0  0  36 33  0  0  0  8002       0      0 0.00e+00    0 0.00e+00  0
MatLUFactorNum         1 1.0 7.5328e-03 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1441       0      0 0.00e+00    0 0.00e+00  0
MatILUFactorSym        1 1.0 3.4315e-03 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 4.6589e-0347.8 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7544e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetRowIJ            1 1.0 3.0850e-06 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetOrdering         1 1.0 6.9243e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot               981 1.0 1.3972e-01 1.3 9.81e+07 1.0 0.0e+00 0.0e+00 9.8e+02  4  7  0  0 66   4  7  0  0 66 14042       0      0 0.00e+00    0 0.00e+00  0
VecNorm              492 1.0 4.7819e-02 1.2 4.92e+07 1.0 0.0e+00 0.0e+00 4.9e+02  1  4  0  0 33   1  4  0  0 33 20578       0      0 0.00e+00    0 0.00e+00  0
VecCopy                4 1.0 2.8181e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet               984 1.0 1.6748e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             1470 1.0 8.7715e-02 1.1 1.47e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3 11  0  0  0   3 11  0  0  0 33518       0      0 0.00e+00    0 0.00e+00  0
VecWAXPY            1958 1.0 2.5474e-01 1.0 1.71e+08 1.0 0.0e+00 0.0e+00 0.0e+00  8 13  0  0  0   8 13  0  0  0 13449       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin      981 1.0 1.3083e-02 1.4 0.00e+00 0.0 3.7e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd        981 1.0 4.1737e-02 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.8310e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.7142e-03 4.5 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin       981 1.0 1.1752e-02 1.4 0.00e+00 0.0 3.7e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd         981 1.0 4.0829e-02 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack               981 1.0 2.4171e-03 7.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack             981 1.0 4.7791e-04 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.4482e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.6394e+00 1.0 1.34e+09 1.0 3.7e+04 8.0e+03 1.5e+03 87100100100 99  87100100100 99 10183       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                2 1.0 2.5150e-02 1.3 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0   432       0      0 0.00e+00    0 0.00e+00  0
PCSetUpOnBlocks        1 1.0 1.1713e-02 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   927       0      0 0.00e+00    0 0.00e+00  0
PCApply              981 1.0 1.1165e+00 1.0 4.37e+08 1.0 0.0e+00 0.0e+00 0.0e+00 37 33  0  0  0  37 33  0  0  0  7834       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              4     11791508     0.
         Vec Scatter     1              1          816     0.
              Vector    14             14      4031808     0.
           Index Set     5              5       608512     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     2              2         2832     0.
      Preconditioner     2              2         1944     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.03e-08
Average time for MPI_Barrier(): 1.07756e-05
Average time for zero size MPI_Send(): 6.36795e-06
#PETSc Option Table entries:
-ksp_type cgs
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type bjacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 8.26951e-08 iterations 1553
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:35:38 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           5.561e+00     1.000   5.561e+00
Objects:              2.300e+01     1.000   2.300e+01
Flop:                 3.028e+09     1.002   3.028e+09  6.055e+10
Flop/sec:             5.446e+08     1.002   5.444e+08  1.089e+10
MPI Messages:         6.218e+03     2.000   5.907e+03  1.181e+05
MPI Message Lengths:  4.972e+07     2.000   7.996e+03  9.447e+08
MPI Reductions:       4.682e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 5.5611e+00 100.0%  6.0554e+10 100.0%  1.181e+05 100.0%  7.996e+03      100.0%  4.675e+03  99.9%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 6.8074e-0377.6 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 5.5699e-03121.4 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             3107 1.0 3.2702e+00 1.0 1.40e+09 1.0 1.2e+05 8.0e+03 0.0e+00 58 46100100  0  58 46100100  0  8543       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 5.6227e-0356.6 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7618e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot              3107 1.0 4.2226e-01 1.5 3.11e+08 1.0 0.0e+00 0.0e+00 3.1e+03  6 10  0  0 66   6 10  0  0 66 14716       0      0 0.00e+00    0 0.00e+00  0
VecNorm             1555 1.0 1.2546e-01 1.0 1.56e+08 1.0 0.0e+00 0.0e+00 1.6e+03  2  5  0  0 33   2  5  0  0 33 24788       0      0 0.00e+00    0 0.00e+00  0
VecCopy                4 1.0 2.3491e-04 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 3 1.0 2.1866e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             4659 1.0 2.6379e-01 1.1 4.66e+08 1.0 0.0e+00 0.0e+00 0.0e+00  5 15  0  0  0   5 15  0  0  0 35324       0      0 0.00e+00    0 0.00e+00  0
VecWAXPY            6210 1.0 8.2650e-01 1.1 5.43e+08 1.0 0.0e+00 0.0e+00 0.0e+00 14 18  0  0  0  14 18  0  0  0 13148       0      0 0.00e+00    0 0.00e+00  0
VecPointwiseMult    3107 1.0 4.3148e-01 1.0 1.55e+08 1.0 0.0e+00 0.0e+00 0.0e+00  8  5  0  0  0   8  5  0  0  0  7201       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     3107 1.0 3.7409e-02 1.6 0.00e+00 0.0 1.2e+05 8.0e+03 0.0e+00  1  0100100  0   1  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       3107 1.0 2.4136e-01 3.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  3  0  0  0  0   3  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.8490e-06 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.3639e-03 3.4 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      3107 1.0 3.2514e-02 1.6 0.00e+00 0.0 1.2e+05 8.0e+03 0.0e+00  1  0100100  0   1  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        3107 1.0 2.3870e-01 3.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              3107 1.0 7.5172e-03 6.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            3107 1.0 1.4852e-03 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 1.4504e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 5.1890e+00 1.0 3.03e+09 1.0 1.2e+05 8.0e+03 4.7e+03 93100100100100  93100100100100 11667       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 8.4000e-07 2.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply             3107 1.0 4.3594e-01 1.0 1.55e+08 1.0 0.0e+00 0.0e+00 2.0e+00  8  5  0  0  0   8  5  0  0  0  7127       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      8013484     0.
         Vec Scatter     1              1          816     0.
              Vector    13             13      4430336     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1416     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.96e-08
Average time for MPI_Barrier(): 1.12694e-05
Average time for zero size MPI_Send(): 6.24895e-06
#PETSc Option Table entries:
-ksp_type cgs
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type jacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 8.535e-05 iterations 546
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:35:44 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.769e+00     1.000   3.769e+00
Objects:              2.200e+01     1.000   2.200e+01
Flop:                 1.552e+09     1.001   1.552e+09  3.104e+10
Flop/sec:             4.119e+08     1.001   4.118e+08  8.236e+09
MPI Messages:         2.190e+03     2.000   2.080e+03  4.161e+04
MPI Message Lengths:  1.750e+07     2.000   7.989e+03  3.324e+08
MPI Reductions:       1.659e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.7690e+00 100.0%  3.1043e+10 100.0%  4.161e+04 100.0%  7.989e+03      100.0%  1.652e+03  99.6%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 2.5852e-0321.8 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 2.5197e-0340.4 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             1093 1.0 1.1346e+00 1.0 4.92e+08 1.0 4.2e+04 8.0e+03 0.0e+00 30 32100100  0  30 32100100  0  8662       0      0 0.00e+00    0 0.00e+00  0
MatSOR              1093 1.0 1.6517e+00 1.0 5.42e+08 1.0 0.0e+00 0.0e+00 0.0e+00 43 35  0  0  0  43 35  0  0  0  6563       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 2.5752e-0322.4 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7508e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot              1093 1.0 1.9950e-01 1.4 1.09e+08 1.0 0.0e+00 0.0e+00 1.1e+03  4  7  0  0 66   4  7  0  0 66 10957       0      0 0.00e+00    0 0.00e+00  0
VecNorm              548 1.0 5.4049e-02 1.1 5.48e+07 1.0 0.0e+00 0.0e+00 5.5e+02  1  4  0  0 33   1  4  0  0 33 20278       0      0 0.00e+00    0 0.00e+00  0
VecCopy                4 1.0 2.4922e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 3 1.0 2.2710e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             1638 1.0 1.1271e-01 1.1 1.64e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3 11  0  0  0   3 11  0  0  0 29065       0      0 0.00e+00    0 0.00e+00  0
VecWAXPY            2182 1.0 3.0581e-01 1.0 1.91e+08 1.0 0.0e+00 0.0e+00 0.0e+00  8 12  0  0  0   8 12  0  0  0 12485       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     1093 1.0 1.8545e-02 1.5 0.00e+00 0.0 4.2e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       1093 1.0 6.2986e-02 2.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.6670e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.3779e-03 3.5 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      1093 1.0 1.6680e-02 1.6 0.00e+00 0.0 4.2e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        1093 1.0 6.1654e-02 2.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              1093 1.0 3.9035e-03 6.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            1093 1.0 6.5185e-04 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 1.4564e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 3.3954e+00 1.0 1.55e+09 1.0 4.1e+04 8.0e+03 1.6e+03 90100100100 99  90100100100 99  9139       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 3.5100e-07 2.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply             1093 1.0 1.6540e+00 1.0 5.42e+08 1.0 0.0e+00 0.0e+00 0.0e+00 43 35  0  0  0  43 35  0  0  0  6553       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    12             12      4028608     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1416     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.25e-08
Average time for MPI_Barrier(): 1.198e-05
Average time for zero size MPI_Send(): 6.37275e-06
#PETSc Option Table entries:
-ksp_type cgs
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type sor
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.000193207 iterations 288
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:35:50 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           4.354e+00     1.000   4.354e+00
Objects:              4.500e+01     1.000   4.500e+01
Flop:                 1.619e+09     1.001   1.619e+09  3.238e+10
Flop/sec:             3.719e+08     1.001   3.719e+08  7.437e+09
MPI Messages:         2.332e+03     2.000   2.215e+03  4.431e+04
MPI Message Lengths:  1.863e+07     2.000   7.990e+03  3.540e+08
MPI Reductions:       9.080e+02     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 4.3543e+00 100.0%  3.2385e+10 100.0%  4.431e+04 100.0%  7.990e+03      100.0%  9.010e+02  99.2%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 5.0325e-0355.9 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 3.8557e-0372.2 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             1164 1.0 1.3540e+00 1.1 5.24e+08 1.0 4.4e+04 8.0e+03 0.0e+00 30 32100100  0  30 32100100  0  7730       0      0 0.00e+00    0 0.00e+00  0
MatSOR              1165 1.0 1.8788e+00 1.1 5.78e+08 1.0 0.0e+00 0.0e+00 0.0e+00 41 36  0  0  0  41 36  0  0  0  6149       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 3.9127e-0336.5 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7109e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  1   1  0  0  0  1     0       0      0 0.00e+00    0 0.00e+00  0
VecDot               577 1.0 1.8058e-01 2.7 5.77e+07 1.0 0.0e+00 0.0e+00 5.8e+02  3  4  0  0 64   3  4  0  0 64  6391       0      0 0.00e+00    0 0.00e+00  0
VecMDot               10 1.0 4.7713e-03 1.4 5.50e+06 1.0 0.0e+00 0.0e+00 1.0e+01  0  0  0  0  1   0  0  0  0  1 23054       0      0 0.00e+00    0 0.00e+00  0
VecNorm              301 1.0 3.4561e-02 1.1 3.01e+07 1.0 0.0e+00 0.0e+00 3.0e+02  1  2  0  0 33   1  2  0  0 33 17418       0      0 0.00e+00    0 0.00e+00  0
VecScale              11 1.0 2.6513e-04 1.1 5.50e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 41490       0      0 0.00e+00    0 0.00e+00  0
VecCopy             1159 1.0 5.3313e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet              1159 1.0 3.4331e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY              865 1.0 6.0855e-02 1.1 8.65e+07 1.0 0.0e+00 0.0e+00 0.0e+00  1  5  0  0  0   1  5  0  0  0 28428       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             1154 1.0 1.7351e-01 1.1 8.66e+07 1.0 0.0e+00 0.0e+00 0.0e+00  4  5  0  0  0   4  5  0  0  0  9976       0      0 0.00e+00    0 0.00e+00  0
VecAXPBYCZ           577 1.0 1.0984e-01 1.1 1.44e+08 1.0 0.0e+00 0.0e+00 0.0e+00  2  9  0  0  0   2  9  0  0  0 26265       0      0 0.00e+00    0 0.00e+00  0
VecWAXPY            1150 1.0 1.7598e-01 1.1 1.01e+08 1.0 0.0e+00 0.0e+00 0.0e+00  4  6  0  0  0   4  6  0  0  0 11433       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY              11 1.0 5.0368e-03 1.1 6.50e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 25810       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     1164 1.0 2.3443e-02 1.5 0.00e+00 0.0 4.4e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       1164 1.0 2.0226e-01 8.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  3  0  0  0  0   3  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecNormalize          11 1.0 7.9595e-03 1.1 1.65e+06 1.0 0.0e+00 0.0e+00 1.1e+01  0  0  0  0  1   0  0  0  0  1  4146       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.8290e-06 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.5321e-03 3.8 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      1164 1.0 2.0966e-02 1.6 0.00e+00 0.0 4.4e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        1164 1.0 2.0083e-01 8.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  3  0  0  0  0   3  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              1164 1.0 4.2527e-03 9.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            1164 1.0 7.4497e-04 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.4600e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  3  1  1  1  3   3  1  1  1  3  3253       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 3.9723e+00 1.0 1.62e+09 1.0 4.4e+04 8.0e+03 8.9e+02 91100100100 98  91100100100 99  8149       0      0 0.00e+00    0 0.00e+00  0
KSPGMRESOrthog        10 1.0 1.2991e-02 1.1 1.10e+07 1.0 0.0e+00 0.0e+00 1.0e+01  0  1  0  0  1   0  1  0  0  1 16935       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 1.4486e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  3  1  1  1  3   3  1  1  1  3  3279       0      0 0.00e+00    0 0.00e+00  0
PCApply              588 1.0 2.8866e+00 1.0 1.07e+09 1.0 2.2e+04 8.0e+03 0.0e+00 65 66 49 50  0  65 66 49 50  0  7399       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    31             31     11661440     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     3              3        33432     0.
      Preconditioner     3              3         2936     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.82e-08
Average time for MPI_Barrier(): 1.382e-05
Average time for zero size MPI_Send(): 6.1021e-06
#PETSc Option Table entries:
-ksp_type cgs
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.00114039 iterations 479
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:35:55 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.017e+00     1.000   3.017e+00
Objects:              2.900e+01     1.000   2.900e+01
Flop:                 1.387e+09     1.001   1.386e+09  2.773e+10
Flop/sec:             4.597e+08     1.001   4.596e+08  9.192e+09
MPI Messages:         1.922e+03     2.000   1.826e+03  3.652e+04
MPI Message Lengths:  1.535e+07     2.000   7.988e+03  2.917e+08
MPI Reductions:       1.936e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.0166e+00 100.0%  2.7729e+10 100.0%  3.652e+04 100.0%  7.988e+03      100.0%  1.929e+03  99.6%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 3.7170e-0328.4 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 3.3468e-0363.8 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult              959 1.0 9.9607e-01 1.0 4.31e+08 1.0 3.6e+04 8.0e+03 0.0e+00 33 31100100  0  33 31100100  0  8657       0      0 0.00e+00    0 0.00e+00  0
MatSolve             959 1.0 1.0842e+00 1.0 4.28e+08 1.0 0.0e+00 0.0e+00 0.0e+00 35 31  0  0  0  35 31  0  0  0  7887       0      0 0.00e+00    0 0.00e+00  0
MatLUFactorNum         1 1.0 7.5075e-03 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1446       0      0 0.00e+00    0 0.00e+00  0
MatILUFactorSym        1 1.0 3.3886e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 3.4030e-0331.6 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.8209e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetRowIJ            1 1.0 2.8240e-06 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetOrdering         1 1.0 6.8933e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot               958 1.0 1.2374e-01 1.3 9.58e+07 1.0 0.0e+00 0.0e+00 9.6e+02  4  7  0  0 49   4  7  0  0 50 15484       0      0 0.00e+00    0 0.00e+00  0
VecDotNorm2          479 1.0 1.2072e-01 1.3 9.58e+07 1.0 0.0e+00 0.0e+00 4.8e+02  4  7  0  0 25   4  7  0  0 25 15872       0      0 0.00e+00    0 0.00e+00  0
VecNorm              481 1.0 4.8698e-02 1.2 4.81e+07 1.0 0.0e+00 0.0e+00 4.8e+02  2  3  0  0 25   2  3  0  0 25 19754       0      0 0.00e+00    0 0.00e+00  0
VecCopy                2 1.0 1.5138e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet               964 1.0 2.1540e-02 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY                1 1.0 8.9525e-05 1.1 1.00e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 22340       0      0 0.00e+00    0 0.00e+00  0
VecAXPBYCZ           958 1.0 1.5960e-01 1.1 1.92e+08 1.0 0.0e+00 0.0e+00 0.0e+00  5 14  0  0  0   5 14  0  0  0 24011       0      0 0.00e+00    0 0.00e+00  0
VecWAXPY             958 1.0 1.2485e-01 1.0 9.58e+07 1.0 0.0e+00 0.0e+00 0.0e+00  4  7  0  0  0   4  7  0  0  0 15347       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin      959 1.0 1.5244e-02 1.6 0.00e+00 0.0 3.6e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd        959 1.0 5.6737e-02 3.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.5550e-06 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.2188e-03 3.1 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin       959 1.0 1.3467e-02 1.6 0.00e+00 0.0 3.6e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd         959 1.0 5.5810e-02 3.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack               959 1.0 2.5593e-03 7.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack             959 1.0 5.5135e-04 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.2501e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.6392e+00 1.0 1.39e+09 1.0 3.6e+04 8.0e+03 1.9e+03 87100100100 99  87100100100 99 10502       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                2 1.0 1.6764e-02 1.2 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0   647       0      0 0.00e+00    0 0.00e+00  0
PCSetUpOnBlocks        1 1.0 1.1646e-02 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   932       0      0 0.00e+00    0 0.00e+00  0
PCApply              959 1.0 1.1093e+00 1.0 4.28e+08 1.0 0.0e+00 0.0e+00 0.0e+00 36 31  0  0  0  36 31  0  0  0  7708       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              4     11791508     0.
         Vec Scatter     1              1          816     0.
              Vector    13             13      3630080     0.
           Index Set     5              5       608512     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     2              2         2840     0.
      Preconditioner     2              2         1944     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.25e-08
Average time for MPI_Barrier(): 1.07444e-05
Average time for zero size MPI_Send(): 6.36865e-06
#PETSc Option Table entries:
-ksp_type bcgs
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type bjacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.00878969 iterations 1368
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:36:02 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           5.145e+00     1.000   5.145e+00
Objects:              2.200e+01     1.000   2.200e+01
Flop:                 2.873e+09     1.002   2.873e+09  5.745e+10
Flop/sec:             5.585e+08     1.002   5.583e+08  1.117e+10
MPI Messages:         5.478e+03     2.000   5.204e+03  1.041e+05
MPI Message Lengths:  4.380e+07     2.000   7.996e+03  8.322e+08
MPI Reductions:       5.494e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 5.1446e+00 100.0%  5.7450e+10 100.0%  1.041e+05 100.0%  7.996e+03      100.0%  5.487e+03  99.9%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 4.2505e-0368.4 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 3.7762e-0380.9 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             2737 1.0 2.8861e+00 1.1 1.23e+09 1.0 1.0e+05 8.0e+03 0.0e+00 55 43100100  0  55 43100100  0  8528       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 3.8301e-0339.3 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7742e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot              2736 1.0 3.4907e-01 1.3 2.74e+08 1.0 0.0e+00 0.0e+00 2.7e+03  6 10  0  0 50   6 10  0  0 50 15676       0      0 0.00e+00    0 0.00e+00  0
VecDotNorm2         1368 1.0 3.6313e-01 1.2 2.74e+08 1.0 0.0e+00 0.0e+00 1.4e+03  6 10  0  0 25   6 10  0  0 25 15069       0      0 0.00e+00    0 0.00e+00  0
VecNorm             1370 1.0 1.3483e-01 1.2 1.37e+08 1.0 0.0e+00 0.0e+00 1.4e+03  2  5  0  0 25   2  5  0  0 25 20322       0      0 0.00e+00    0 0.00e+00  0
VecCopy                2 1.0 1.2634e-04 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 5 1.0 3.3069e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY                1 1.0 8.6743e-05 1.1 1.00e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 23057       0      0 0.00e+00    0 0.00e+00  0
VecAXPBYCZ          2736 1.0 4.5398e-01 1.0 5.47e+08 1.0 0.0e+00 0.0e+00 0.0e+00  9 19  0  0  0   9 19  0  0  0 24107       0      0 0.00e+00    0 0.00e+00  0
VecWAXPY            2736 1.0 3.5702e-01 1.0 2.74e+08 1.0 0.0e+00 0.0e+00 0.0e+00  7 10  0  0  0   7 10  0  0  0 15327       0      0 0.00e+00    0 0.00e+00  0
VecPointwiseMult    2737 1.0 3.8820e-01 1.1 1.37e+08 1.0 0.0e+00 0.0e+00 0.0e+00  7  5  0  0  0   7  5  0  0  0  7050       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     2737 1.0 3.4936e-02 1.7 0.00e+00 0.0 1.0e+05 8.0e+03 0.0e+00  1  0100100  0   1  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       2737 1.0 1.7485e-01 3.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.7660e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.1470e-03 3.1 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      2737 1.0 3.0734e-02 1.7 0.00e+00 0.0 1.0e+05 8.0e+03 0.0e+00  1  0100100  0   1  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        2737 1.0 1.7202e-01 3.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              2737 1.0 7.3140e-03 7.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            2737 1.0 1.5548e-03 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 1.2297e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 4.7642e+00 1.0 2.87e+09 1.0 1.0e+05 8.0e+03 5.5e+03 93100100100100  93100100100100 12056       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 8.4100e-07 2.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply             2737 1.0 3.9501e-01 1.1 1.37e+08 1.0 0.0e+00 0.0e+00 2.0e+00  7  5  0  0  0   7  5  0  0  0  6929       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      8013484     0.
         Vec Scatter     1              1          816     0.
              Vector    12             12      4028608     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1424     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.06e-08
Average time for MPI_Barrier(): 1.24228e-05
Average time for zero size MPI_Send(): 6.1401e-06
#PETSc Option Table entries:
-ksp_type bcgs
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type jacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.00987163 iterations 515
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:36:08 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.573e+00     1.000   3.573e+00
Objects:              2.100e+01     1.000   2.100e+01
Flop:                 1.542e+09     1.001   1.542e+09  3.083e+10
Flop/sec:             4.315e+08     1.001   4.314e+08  8.628e+09
MPI Messages:         2.066e+03     2.000   1.963e+03  3.925e+04
MPI Message Lengths:  1.650e+07     2.000   7.988e+03  3.136e+08
MPI Reductions:       2.080e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.5733e+00 100.0%  3.0831e+10 100.0%  3.925e+04 100.0%  7.988e+03      100.0%  2.073e+03  99.7%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 3.5225e-0330.1 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 3.4711e-0357.4 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             1031 1.0 1.0505e+00 1.0 4.64e+08 1.0 3.9e+04 8.0e+03 0.0e+00 29 30100100  0  29 30100100  0  8825       0      0 0.00e+00    0 0.00e+00  0
MatSOR              1031 1.0 1.5331e+00 1.0 5.11e+08 1.0 0.0e+00 0.0e+00 0.0e+00 43 33  0  0  0  43 33  0  0  0  6669       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 3.5222e-0330.6 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7629e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot              1030 1.0 1.3361e-01 1.1 1.03e+08 1.0 0.0e+00 0.0e+00 1.0e+03  4  7  0  0 50   4  7  0  0 50 15417       0      0 0.00e+00    0 0.00e+00  0
VecDotNorm2          515 1.0 1.2533e-01 1.1 1.03e+08 1.0 0.0e+00 0.0e+00 5.2e+02  3  7  0  0 25   3  7  0  0 25 16437       0      0 0.00e+00    0 0.00e+00  0
VecNorm              517 1.0 5.5268e-02 1.1 5.17e+07 1.0 0.0e+00 0.0e+00 5.2e+02  1  3  0  0 25   1  3  0  0 25 18709       0      0 0.00e+00    0 0.00e+00  0
VecCopy                2 1.0 1.1760e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 5 1.0 3.4337e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY                1 1.0 9.0331e-05 1.2 1.00e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 22141       0      0 0.00e+00    0 0.00e+00  0
VecAXPBYCZ          1030 1.0 1.7388e-01 1.0 2.06e+08 1.0 0.0e+00 0.0e+00 0.0e+00  5 13  0  0  0   5 13  0  0  0 23695       0      0 0.00e+00    0 0.00e+00  0
VecWAXPY            1030 1.0 1.4751e-01 1.0 1.03e+08 1.0 0.0e+00 0.0e+00 0.0e+00  4  7  0  0  0   4  7  0  0  0 13965       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     1031 1.0 1.8600e-02 1.5 0.00e+00 0.0 3.9e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       1031 1.0 3.3767e-02 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 1.0083e-05 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.3194e-03 3.3 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      1031 1.0 1.6611e-02 1.6 0.00e+00 0.0 3.9e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        1031 1.0 3.2419e-02 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              1031 1.0 4.0592e-03 9.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            1031 1.0 5.3503e-04 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 1.2499e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 3.1984e+00 1.0 1.54e+09 1.0 3.9e+04 8.0e+03 2.1e+03 89100100100 99  89100100100 99  9636       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 5.1400e-07 3.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply             1031 1.0 1.5352e+00 1.0 5.11e+08 1.0 0.0e+00 0.0e+00 0.0e+00 43 33  0  0  0  43 33  0  0  0  6660       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    11             11      3626880     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1424     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.22e-08
Average time for MPI_Barrier(): 1.33674e-05
Average time for zero size MPI_Send(): 6.28365e-06
#PETSc Option Table entries:
-ksp_type bcgs
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type sor
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.000785404 iterations 301
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:36:14 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           4.452e+00     1.000   4.452e+00
Objects:              4.400e+01     1.000   4.400e+01
Flop:                 1.737e+09     1.001   1.737e+09  3.473e+10
Flop/sec:             3.901e+08     1.001   3.901e+08  7.801e+09
MPI Messages:         2.436e+03     2.000   2.314e+03  4.628e+04
MPI Message Lengths:  1.946e+07     2.000   7.990e+03  3.698e+08
MPI Reductions:       1.247e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 4.4518e+00 100.0%  3.4730e+10 100.0%  4.628e+04 100.0%  7.990e+03      100.0%  1.240e+03  99.4%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 5.2830e-0365.8 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 5.2409e-0382.0 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             1216 1.0 1.3352e+00 1.0 5.47e+08 1.0 4.6e+04 8.0e+03 0.0e+00 29 31100100  0  29 31100100  0  8189       0      0 0.00e+00    0 0.00e+00  0
MatSOR              1217 1.0 1.8937e+00 1.0 6.03e+08 1.0 0.0e+00 0.0e+00 0.0e+00 42 35  0  0  0  42 35  0  0  0  6373       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 5.2915e-0344.3 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7807e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot               602 1.0 1.0978e-01 1.4 6.02e+07 1.0 0.0e+00 0.0e+00 6.0e+02  2  3  0  0 48   2  3  0  0 49 10967       0      0 0.00e+00    0 0.00e+00  0
VecDotNorm2          301 1.0 1.0998e-01 1.5 6.02e+07 1.0 0.0e+00 0.0e+00 3.0e+02  2  3  0  0 24   2  3  0  0 24 10947       0      0 0.00e+00    0 0.00e+00  0
VecMDot               10 1.0 4.7699e-03 1.4 5.50e+06 1.0 0.0e+00 0.0e+00 1.0e+01  0  0  0  0  1   0  0  0  0  1 23061       0      0 0.00e+00    0 0.00e+00  0
VecNorm              314 1.0 3.9548e-02 1.1 3.14e+07 1.0 0.0e+00 0.0e+00 3.1e+02  1  2  0  0 25   1  2  0  0 25 15879       0      0 0.00e+00    0 0.00e+00  0
VecScale              11 1.0 2.6013e-04 1.1 5.50e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 42286       0      0 0.00e+00    0 0.00e+00  0
VecCopy             1209 1.0 5.6458e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet              1213 1.0 4.1566e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY                2 1.0 1.5683e-04 1.1 2.00e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 25505       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             1206 1.0 1.7122e-01 1.1 9.04e+07 1.0 0.0e+00 0.0e+00 0.0e+00  4  5  0  0  0   4  5  0  0  0 10565       0      0 0.00e+00    0 0.00e+00  0
VecAXPBYCZ          1205 1.0 2.1184e-01 1.0 2.71e+08 1.0 0.0e+00 0.0e+00 0.0e+00  5 16  0  0  0   5 16  0  0  0 25600       0      0 0.00e+00    0 0.00e+00  0
VecWAXPY             602 1.0 9.0564e-02 1.0 6.02e+07 1.0 0.0e+00 0.0e+00 0.0e+00  2  3  0  0  0   2  3  0  0  0 13294       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY              11 1.0 5.0102e-03 1.1 6.50e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 25947       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     1216 1.0 2.1664e-02 1.6 0.00e+00 0.0 4.6e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       1216 1.0 1.2531e-01 3.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecNormalize          11 1.0 8.3711e-03 1.1 1.65e+06 1.0 0.0e+00 0.0e+00 1.1e+01  0  0  0  0  1   0  0  0  0  1  3942       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.7210e-06 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 2.5019e-03 6.5 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      1216 1.0 1.9108e-02 1.6 0.00e+00 0.0 4.6e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        1216 1.0 1.2375e-01 4.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              1216 1.0 4.2503e-03 8.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            1216 1.0 6.5120e-04 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.4768e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  3  1  1  1  2   3  1  1  1  2  3216       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 4.0712e+00 1.0 1.74e+09 1.0 4.6e+04 8.0e+03 1.2e+03 91100100100 98  91100100100 99  8528       0      0 0.00e+00    0 0.00e+00  0
KSPGMRESOrthog        10 1.0 1.3130e-02 1.1 1.10e+07 1.0 0.0e+00 0.0e+00 1.0e+01  0  1  0  0  1   0  1  0  0  1 16755       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 1.4673e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  3  1  1  1  2   3  1  1  1  2  3237       0      0 0.00e+00    0 0.00e+00  0
PCApply              614 1.0 2.9253e+00 1.0 1.12e+09 1.0 2.3e+04 8.0e+03 0.0e+00 65 64 50 50  0  65 64 50 50  0  7628       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    30             30     11259712     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     3              3        33440     0.
      Preconditioner     3              3         2936     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.85e-08
Average time for MPI_Barrier(): 1.40288e-05
Average time for zero size MPI_Send(): 6.13e-06
#PETSc Option Table entries:
-ksp_type bcgs
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 8.37695e-05 iterations 475
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:36:19 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.132e+00     1.000   3.132e+00
Objects:              3.200e+01     1.000   3.200e+01
Flop:                 1.446e+09     1.001   1.446e+09  2.891e+10
Flop/sec:             4.617e+08     1.001   4.616e+08  9.232e+09
MPI Messages:         1.906e+03     2.000   1.811e+03  3.621e+04
MPI Message Lengths:  1.522e+07     2.000   7.987e+03  2.893e+08
MPI Reductions:       1.445e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.1316e+00 100.0%  2.8910e+10 100.0%  3.621e+04 100.0%  7.987e+03      100.0%  1.438e+03  99.5%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 4.2329e-0343.1 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 4.1974e-0372.9 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult              951 1.0 9.9235e-01 1.0 4.28e+08 1.0 3.6e+04 8.0e+03 0.0e+00 32 30100100  0  32 30100100  0  8617       0      0 0.00e+00    0 0.00e+00  0
MatSolve             951 1.0 1.0720e+00 1.0 4.24e+08 1.0 0.0e+00 0.0e+00 0.0e+00 34 29  0  0  0  34 29  0  0  0  7910       0      0 0.00e+00    0 0.00e+00  0
MatLUFactorNum         1 1.0 7.5117e-03 1.0 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1445       0      0 0.00e+00    0 0.00e+00  0
MatILUFactorSym        1 1.0 3.4130e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 4.2504e-0338.2 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7657e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetRowIJ            1 1.0 2.8730e-06 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetOrdering         1 1.0 6.9649e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot               950 1.0 1.2622e-01 1.2 9.50e+07 1.0 0.0e+00 0.0e+00 9.5e+02  4  7  0  0 66   4  7  0  0 66 15052       0      0 0.00e+00    0 0.00e+00  0
VecNorm              477 1.0 7.3489e-02 1.6 4.77e+07 1.0 0.0e+00 0.0e+00 4.8e+02  2  3  0  0 33   2  3  0  0 33 12981       0      0 0.00e+00    0 0.00e+00  0
VecCopy                5 1.0 3.6796e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet               955 1.0 2.0471e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             1899 1.0 1.1008e-01 1.1 1.90e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3 13  0  0  0   3 13  0  0  0 34502       0      0 0.00e+00    0 0.00e+00  0
VecAYPX              949 1.0 1.2705e-01 1.0 9.48e+07 1.0 0.0e+00 0.0e+00 0.0e+00  4  7  0  0  0   4  7  0  0  0 14923       0      0 0.00e+00    0 0.00e+00  0
VecWAXPY            1898 1.0 2.5208e-01 1.0 1.66e+08 1.0 0.0e+00 0.0e+00 0.0e+00  8 11  0  0  0   8 11  0  0  0 13175       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin      951 1.0 1.3620e-02 1.4 0.00e+00 0.0 3.6e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd        951 1.0 4.7558e-02 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 1.4116e-05 2.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 2.1986e-03 5.5 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin       951 1.0 1.2370e-02 1.4 0.00e+00 0.0 3.6e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd         951 1.0 4.6549e-02 2.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack               951 1.0 2.8459e-03 7.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack             951 1.0 6.1161e-04 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.8705e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.7459e+00 1.0 1.45e+09 1.0 3.6e+04 8.0e+03 1.4e+03 88100100100 99  88100100100 99 10524       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                2 1.0 2.5278e-02 1.3 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0   429       0      0 0.00e+00    0 0.00e+00  0
PCSetUpOnBlocks        1 1.0 1.1659e-02 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   931       0      0 0.00e+00    0 0.00e+00  0
PCApply              951 1.0 1.0990e+00 1.0 4.24e+08 1.0 0.0e+00 0.0e+00 0.0e+00 35 29  0  0  0  35 29  0  0  0  7715       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              4     11791508     0.
         Vec Scatter     1              1          816     0.
              Vector    16             16      4835264     0.
           Index Set     5              5       608512     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     2              2         2832     0.
      Preconditioner     2              2         1944     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.96e-08
Average time for MPI_Barrier(): 1.21768e-05
Average time for zero size MPI_Send(): 6.59505e-06
#PETSc Option Table entries:
-ksp_type tfqmr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type bjacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 3.10408e-07 iterations 1511
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:36:27 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           5.755e+00     1.000   5.755e+00
Objects:              2.500e+01     1.000   2.500e+01
Flop:                 3.399e+09     1.002   3.399e+09  6.797e+10
Flop/sec:             5.907e+08     1.002   5.906e+08  1.181e+10
MPI Messages:         6.050e+03     2.000   5.748e+03  1.150e+05
MPI Message Lengths:  4.838e+07     2.000   7.996e+03  9.191e+08
MPI Reductions:       4.555e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 5.7547e+00 100.0%  6.7975e+10 100.0%  1.150e+05 100.0%  7.996e+03      100.0%  4.548e+03  99.8%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 4.9772e-0316.5 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 4.9030e-0393.4 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             3023 1.0 3.0669e+00 1.0 1.36e+09 1.0 1.1e+05 8.0e+03 0.0e+00 53 40100100  0  53 40100100  0  8863       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 4.9543e-0346.2 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7950e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot              3022 1.0 3.0184e-01 1.1 3.02e+08 1.0 0.0e+00 0.0e+00 3.0e+03  5  9  0  0 66   5  9  0  0 66 20024       0      0 0.00e+00    0 0.00e+00  0
VecNorm             1513 1.0 1.5142e-01 1.2 1.51e+08 1.0 0.0e+00 0.0e+00 1.5e+03  2  4  0  0 33   2  4  0  0 33 19985       0      0 0.00e+00    0 0.00e+00  0
VecCopy                5 1.0 3.2789e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 4 1.0 2.8049e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             6043 1.0 3.1253e-01 1.1 6.04e+08 1.0 0.0e+00 0.0e+00 0.0e+00  5 18  0  0  0   5 18  0  0  0 38672       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             3021 1.0 3.8978e-01 1.0 3.02e+08 1.0 0.0e+00 0.0e+00 0.0e+00  7  9  0  0  0   7  9  0  0  0 15496       0      0 0.00e+00    0 0.00e+00  0
VecWAXPY            6042 1.0 7.7549e-01 1.0 5.29e+08 1.0 0.0e+00 0.0e+00 0.0e+00 13 16  0  0  0  13 16  0  0  0 13634       0      0 0.00e+00    0 0.00e+00  0
VecPointwiseMult    3023 1.0 4.2195e-01 1.0 1.51e+08 1.0 0.0e+00 0.0e+00 0.0e+00  7  4  0  0  0   7  4  0  0  0  7164       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     3023 1.0 3.5053e-02 1.5 0.00e+00 0.0 1.1e+05 8.0e+03 0.0e+00  1  0100100  0   1  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       3023 1.0 9.5250e-02 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.8570e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 2.6448e-0314.0 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      3023 1.0 3.1513e-02 1.6 0.00e+00 0.0 1.1e+05 8.0e+03 0.0e+00  1  0100100  0   1  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        3023 1.0 9.2355e-02 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              3023 1.0 8.2591e-03 8.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            3023 1.0 1.9210e-03 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 1.8744e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 5.3777e+00 1.0 3.40e+09 1.0 1.1e+05 8.0e+03 4.5e+03 93100100100100  93100100100100 12638       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 9.5000e-07 3.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply             3023 1.0 4.2642e-01 1.0 1.51e+08 1.0 0.0e+00 0.0e+00 2.0e+00  7  4  0  0  0   7  4  0  0  0  7089       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      8013484     0.
         Vec Scatter     1              1          816     0.
              Vector    15             15      5233792     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1416     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.99e-08
Average time for MPI_Barrier(): 1.19958e-05
Average time for zero size MPI_Send(): 6.5328e-06
#PETSc Option Table entries:
-ksp_type tfqmr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type jacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.000233762 iterations 533
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:36:33 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.859e+00     1.000   3.859e+00
Objects:              2.400e+01     1.000   2.400e+01
Flop:                 1.675e+09     1.001   1.675e+09  3.349e+10
Flop/sec:             4.341e+08     1.001   4.340e+08  8.681e+09
MPI Messages:         2.138e+03     2.000   2.031e+03  4.062e+04
MPI Message Lengths:  1.708e+07     2.000   7.989e+03  3.245e+08
MPI Reductions:       1.619e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.8586e+00 100.0%  3.3495e+10 100.0%  4.062e+04 100.0%  7.989e+03      100.0%  1.612e+03  99.6%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 4.0060e-0358.4 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 3.9606e-0377.3 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             1067 1.0 1.1018e+00 1.0 4.80e+08 1.0 4.1e+04 8.0e+03 0.0e+00 28 29100100  0  28 29100100  0  8708       0      0 0.00e+00    0 0.00e+00  0
MatSOR              1067 1.0 1.6035e+00 1.0 5.29e+08 1.0 0.0e+00 0.0e+00 0.0e+00 41 32  0  0  0  41 32  0  0  0  6599       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 4.0113e-0338.0 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7668e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot              1066 1.0 1.5849e-01 1.1 1.07e+08 1.0 0.0e+00 0.0e+00 1.1e+03  4  6  0  0 66   4  6  0  0 66 13451       0      0 0.00e+00    0 0.00e+00  0
VecNorm              535 1.0 7.8817e-02 1.3 5.35e+07 1.0 0.0e+00 0.0e+00 5.4e+02  2  3  0  0 33   2  3  0  0 33 13576       0      0 0.00e+00    0 0.00e+00  0
VecCopy                5 1.0 3.7791e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 4 1.0 2.8136e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             2131 1.0 1.3539e-01 1.1 2.13e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3 13  0  0  0   3 13  0  0  0 31479       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             1065 1.0 1.4603e-01 1.0 1.06e+08 1.0 0.0e+00 0.0e+00 0.0e+00  4  6  0  0  0   4  6  0  0  0 14573       0      0 0.00e+00    0 0.00e+00  0
VecWAXPY            2130 1.0 2.9686e-01 1.0 1.86e+08 1.0 0.0e+00 0.0e+00 0.0e+00  8 11  0  0  0   8 11  0  0  0 12555       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     1067 1.0 1.7776e-02 1.4 0.00e+00 0.0 4.1e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       1067 1.0 4.3130e-02 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.7870e-06 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.2305e-03 3.1 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      1067 1.0 1.5893e-02 1.5 0.00e+00 0.0 4.1e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        1067 1.0 4.1702e-02 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              1067 1.0 3.9575e-0313.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            1067 1.0 6.5379e-04 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 1.8580e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 3.4791e+00 1.0 1.67e+09 1.0 4.1e+04 8.0e+03 1.6e+03 90100100100 99  90100100100 99  9624       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 3.0900e-07 2.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply             1067 1.0 1.6056e+00 1.0 5.29e+08 1.0 0.0e+00 0.0e+00 0.0e+00 41 32  0  0  0  41 32  0  0  0  6590       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    14             14      4832064     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1416     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.97e-08
Average time for MPI_Barrier(): 1.35858e-05
Average time for zero size MPI_Send(): 6.25695e-06
#PETSc Option Table entries:
-ksp_type tfqmr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type sor
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.0017322 iterations 281
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:36:39 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           4.452e+00     1.000   4.452e+00
Objects:              4.700e+01     1.000   4.700e+01
Flop:                 1.665e+09     1.001   1.664e+09  3.329e+10
Flop/sec:             3.739e+08     1.001   3.738e+08  7.477e+09
MPI Messages:         2.276e+03     2.000   2.162e+03  4.324e+04
MPI Message Lengths:  1.818e+07     2.000   7.989e+03  3.455e+08
MPI Reductions:       8.860e+02     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 4.4522e+00 100.0%  3.3288e+10 100.0%  4.324e+04 100.0%  7.989e+03      100.0%  8.790e+02  99.2%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 4.1261e-0344.8 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 4.0181e-0386.6 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             1136 1.0 1.3593e+00 1.1 5.11e+08 1.0 4.3e+04 8.0e+03 0.0e+00 28 31100100  0  28 31100100  0  7515       0      0 0.00e+00    0 0.00e+00  0
MatSOR              1137 1.0 1.8560e+00 1.1 5.64e+08 1.0 0.0e+00 0.0e+00 0.0e+00 40 34  0  0  0  40 34  0  0  0  6075       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 4.0687e-0341.8 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7079e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  1   1  0  0  0  1     0       0      0 0.00e+00    0 0.00e+00  0
VecDot               562 1.0 1.7628e-01 2.0 5.62e+07 1.0 0.0e+00 0.0e+00 5.6e+02  3  3  0  0 63   3  3  0  0 64  6376       0      0 0.00e+00    0 0.00e+00  0
VecMDot               10 1.0 5.6126e-03 1.6 5.50e+06 1.0 0.0e+00 0.0e+00 1.0e+01  0  0  0  0  1   0  0  0  0  1 19599       0      0 0.00e+00    0 0.00e+00  0
VecNorm              294 1.0 1.3468e-01 2.6 2.94e+07 1.0 0.0e+00 0.0e+00 2.9e+02  2  2  0  0 33   2  2  0  0 33  4366       0      0 0.00e+00    0 0.00e+00  0
VecScale              11 1.0 2.5944e-04 1.1 5.50e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 42399       0      0 0.00e+00    0 0.00e+00  0
VecCopy             1132 1.0 5.4608e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet              1132 1.0 3.3376e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             1124 1.0 7.1943e-02 1.1 1.12e+08 1.0 0.0e+00 0.0e+00 0.0e+00  2  7  0  0  0   2  7  0  0  0 31247       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             1687 1.0 2.5392e-01 1.1 1.40e+08 1.0 0.0e+00 0.0e+00 0.0e+00  5  8  0  0  0   5  8  0  0  0 11063       0      0 0.00e+00    0 0.00e+00  0
VecAXPBYCZ           563 1.0 1.0398e-01 1.1 1.41e+08 1.0 0.0e+00 0.0e+00 0.0e+00  2  8  0  0  0   2  8  0  0  0 27072       0      0 0.00e+00    0 0.00e+00  0
VecWAXPY            1122 1.0 1.7529e-01 1.1 9.82e+07 1.0 0.0e+00 0.0e+00 0.0e+00  4  6  0  0  0   4  6  0  0  0 11199       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY              11 1.0 4.7669e-03 1.0 6.50e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 27272       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     1136 1.0 2.1281e-02 1.4 0.00e+00 0.0 4.3e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       1136 1.0 2.2208e-01 6.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecNormalize          11 1.0 7.6788e-03 1.2 1.65e+06 1.0 0.0e+00 0.0e+00 1.1e+01  0  0  0  0  1   0  0  0  0  1  4298       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.5940e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.9509e-03 4.9 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      1136 1.0 1.9199e-02 1.5 0.00e+00 0.0 4.3e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        1136 1.0 2.2059e-01 7.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              1136 1.0 3.9084e-03 8.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            1136 1.0 7.0982e-04 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.4533e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  3  1  1  1  3   3  1  1  1  3  3268       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 4.0634e+00 1.0 1.66e+09 1.0 4.3e+04 8.0e+03 8.7e+02 91100100100 98  91100100100 99  8189       0      0 0.00e+00    0 0.00e+00  0
KSPGMRESOrthog        10 1.0 1.3625e-02 1.2 1.10e+07 1.0 0.0e+00 0.0e+00 1.0e+01  0  1  0  0  1   0  1  0  0  1 16147       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 1.4383e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  3  1  1  1  3   3  1  1  1  3  3303       0      0 0.00e+00    0 0.00e+00  0
PCApply              574 1.0 2.8431e+00 1.0 1.04e+09 1.0 2.1e+04 8.0e+03 0.0e+00 62 63 49 50  0  62 63 49 50  0  7331       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    33             33     12464896     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     3              3        33432     0.
      Preconditioner     3              3         2936     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.96e-08
Average time for MPI_Barrier(): 1.18298e-05
Average time for zero size MPI_Send(): 6.46235e-06
#PETSc Option Table entries:
-ksp_type tfqmr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.00115167 iterations 573
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:36:43 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           2.087e+00     1.000   2.087e+00
Objects:              2.900e+01     1.000   2.900e+01
Flop:                 9.164e+08     1.001   9.163e+08  1.833e+10
Flop/sec:             4.390e+08     1.001   4.389e+08  8.779e+09
MPI Messages:         1.154e+03     2.000   1.096e+03  2.193e+04
MPI Message Lengths:  9.208e+06     2.000   7.979e+03  1.750e+08
MPI Reductions:       1.166e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 2.0874e+00 100.0%  1.8325e+10 100.0%  2.193e+04 100.0%  7.979e+03      100.0%  1.159e+03  99.4%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 3.2705e-0328.5 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 3.2482e-0362.6 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult              575 1.0 6.2859e-01 1.0 2.59e+08 1.0 2.2e+04 8.0e+03 0.0e+00 29 28100100  0  29 28100100  0  8225       0      0 0.00e+00    0 0.00e+00  0
MatSolve             574 1.0 6.7867e-01 1.1 2.56e+08 1.0 0.0e+00 0.0e+00 0.0e+00 31 28  0  0  0  31 28  0  0  0  7541       0      0 0.00e+00    0 0.00e+00  0
MatLUFactorNum         1 1.0 8.0207e-03 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1353       0      0 0.00e+00    0 0.00e+00  0
MatILUFactorSym        1 1.0 3.4154e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 3.3022e-0331.9 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7948e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetRowIJ            1 1.0 2.7690e-06 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetOrdering         1 1.0 6.9704e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot               573 1.0 1.1366e-01 2.2 5.73e+07 1.0 0.0e+00 0.0e+00 5.7e+02  4  6  0  0 49   4  6  0  0 49 10083       0      0 0.00e+00    0 0.00e+00  0
VecNorm                1 1.0 9.5998e-05 1.2 1.00e+05 1.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0 20834       0      0 0.00e+00    0 0.00e+00  0
VecCopy                3 1.0 2.3471e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet               577 1.0 1.0936e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             1147 1.0 6.0507e-02 1.1 1.15e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3 13  0  0  0   3 13  0  0  0 37913       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             1144 1.0 1.5042e-01 1.1 1.14e+08 1.0 0.0e+00 0.0e+00 0.0e+00  7 12  0  0  0   7 12  0  0  0 15211       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin      575 1.0 1.0616e-02 1.6 0.00e+00 0.0 2.2e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd        575 1.0 6.0857e-02 4.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecReduceArith      1148 1.0 8.5220e-02 1.2 1.15e+08 1.0 0.0e+00 0.0e+00 0.0e+00  4 13  0  0  0   4 13  0  0  0 26942       0      0 0.00e+00    0 0.00e+00  0
VecReduceComm        574 1.0 5.4527e-02 3.0 0.00e+00 0.0 0.0e+00 0.0e+00 5.7e+02  2  0  0  0 49   2  0  0  0 50     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.4570e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.3208e-03 3.4 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin       575 1.0 9.6162e-03 1.6 0.00e+00 0.0 2.2e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd         575 1.0 6.0091e-02 4.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack               575 1.0 1.6047e-03 7.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack             575 1.0 3.8628e-04 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.2564e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 1.7102e+00 1.0 9.16e+08 1.0 2.2e+04 8.0e+03 1.1e+03 82100 99100 98  82100 99100 99 10708       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                2 1.0 2.5774e-02 1.2 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0   421       0      0 0.00e+00    0 0.00e+00  0
PCSetUpOnBlocks        1 1.0 1.2147e-02 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0   894       0      0 0.00e+00    0 0.00e+00  0
PCApply              574 1.0 6.9379e-01 1.1 2.56e+08 1.0 0.0e+00 0.0e+00 0.0e+00 32 28  0  0  0  32 28  0  0  0  7377       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              4     11791508     0.
         Vec Scatter     1              1          816     0.
              Vector    13             13      3630080     0.
           Index Set     5              5       608512     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     2              2         2832     0.
      Preconditioner     2              2         1944     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.04e-08
Average time for MPI_Barrier(): 1.03282e-05
Average time for zero size MPI_Send(): 6.15855e-06
#PETSc Option Table entries:
-ksp_type cr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type bjacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.000525242 iterations 1635
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:36:48 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.243e+00     1.000   3.243e+00
Objects:              2.200e+01     1.000   2.200e+01
Flop:                 1.963e+09     1.002   1.962e+09  3.925e+10
Flop/sec:             6.053e+08     1.002   6.052e+08  1.210e+10
MPI Messages:         3.278e+03     2.000   3.114e+03  6.228e+04
MPI Message Lengths:  2.620e+07     2.000   7.993e+03  4.978e+08
MPI Reductions:       3.292e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.2428e+00 100.0%  3.9250e+10 100.0%  6.228e+04 100.0%  7.993e+03      100.0%  3.285e+03  99.8%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 3.4849e-0331.3 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 3.4309e-0352.6 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             1637 1.0 1.6966e+00 1.0 7.36e+08 1.0 6.2e+04 8.0e+03 0.0e+00 52 38100100  0  52 38100100  0  8676       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 3.4845e-0329.4 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7926e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot              1635 1.0 1.4899e-01 1.2 1.63e+08 1.0 0.0e+00 0.0e+00 1.6e+03  4  8  0  0 50   4  8  0  0 50 21948       0      0 0.00e+00    0 0.00e+00  0
VecNorm                1 1.0 8.7875e-05 1.2 1.00e+05 1.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0 22760       0      0 0.00e+00    0 0.00e+00  0
VecCopy                3 1.0 2.2174e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 3 1.0 2.1921e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             3271 1.0 1.6064e-01 1.1 3.27e+08 1.0 0.0e+00 0.0e+00 0.0e+00  5 17  0  0  0   5 17  0  0  0 40725       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             3268 1.0 4.1465e-01 1.0 3.27e+08 1.0 0.0e+00 0.0e+00 0.0e+00 13 17  0  0  0  13 17  0  0  0 15763       0      0 0.00e+00    0 0.00e+00  0
VecPointwiseMult    1636 1.0 2.3270e-01 1.1 8.18e+07 1.0 0.0e+00 0.0e+00 0.0e+00  7  4  0  0  0   7  4  0  0  0  7030       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     1637 1.0 1.9042e-02 1.6 0.00e+00 0.0 6.2e+04 8.0e+03 0.0e+00  1  0100100  0   1  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       1637 1.0 9.9180e-02 3.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecReduceArith      3272 1.0 2.1308e-01 1.0 3.27e+08 1.0 0.0e+00 0.0e+00 0.0e+00  6 17  0  0  0   6 17  0  0  0 30711       0      0 0.00e+00    0 0.00e+00  0
VecReduceComm       1636 1.0 6.0747e-02 2.7 0.00e+00 0.0 0.0e+00 0.0e+00 1.6e+03  1  0  0  0 50   1  0  0  0 50     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.5260e-06 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 2.5930e-03 7.0 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      1637 1.0 1.7387e-02 1.7 0.00e+00 0.0 6.2e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        1637 1.0 9.7546e-02 3.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              1637 1.0 3.9685e-03 7.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            1637 1.0 1.0963e-03 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 1.2538e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.8680e+00 1.0 1.96e+09 1.0 6.2e+04 8.0e+03 3.3e+03 88100100100 99  88100100100100 13681       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 6.5500e-07 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply             1636 1.0 2.3863e-01 1.1 8.18e+07 1.0 0.0e+00 0.0e+00 2.0e+00  7  4  0  0  0   7  4  0  0  0  6856       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      8013484     0.
         Vec Scatter     1              1          816     0.
              Vector    12             12      4028608     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1416     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.03e-08
Average time for MPI_Barrier(): 1.2737e-05
Average time for zero size MPI_Send(): 6.3475e-06
#PETSc Option Table entries:
-ksp_type cr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type jacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.000702134 iterations 672
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:36:53 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           2.690e+00     1.000   2.690e+00
Objects:              2.100e+01     1.000   2.100e+01
Flop:                 1.107e+09     1.001   1.107e+09  2.215e+10
Flop/sec:             4.117e+08     1.001   4.117e+08  8.233e+09
MPI Messages:         1.352e+03     2.000   1.284e+03  2.569e+04
MPI Message Lengths:  1.079e+07     2.000   7.982e+03  2.050e+08
MPI Reductions:       1.364e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 2.6899e+00 100.0%  2.2147e+10 100.0%  2.569e+04 100.0%  7.982e+03      100.0%  1.357e+03  99.5%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 2.7945e-0329.4 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 1.8830e-0339.8 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult              674 1.0 7.2355e-01 1.0 3.03e+08 1.0 2.6e+04 8.0e+03 0.0e+00 26 27100100  0  26 27100100  0  8376       0      0 0.00e+00    0 0.00e+00  0
MatSOR               673 1.0 1.1103e+00 1.1 3.34e+08 1.0 0.0e+00 0.0e+00 0.0e+00 38 30  0  0  0  38 30  0  0  0  6011       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 1.9359e-0319.3 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.8303e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot               672 1.0 1.7207e-01 2.8 6.72e+07 1.0 0.0e+00 0.0e+00 6.7e+02  6  6  0  0 49   6  6  0  0 50  7811       0      0 0.00e+00    0 0.00e+00  0
VecNorm                1 1.0 1.0174e-04 1.3 1.00e+05 1.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0 19657       0      0 0.00e+00    0 0.00e+00  0
VecCopy                3 1.0 2.4952e-04 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 3 1.0 2.1730e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             1345 1.0 1.1575e-01 1.1 1.34e+08 1.0 0.0e+00 0.0e+00 0.0e+00  4 12  0  0  0   4 12  0  0  0 23239       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             1342 1.0 1.7883e-01 1.0 1.34e+08 1.0 0.0e+00 0.0e+00 0.0e+00  7 12  0  0  0   7 12  0  0  0 15009       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin      674 1.0 1.5335e-02 1.6 0.00e+00 0.0 2.6e+04 8.0e+03 0.0e+00  1  0100100  0   1  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd        674 1.0 4.5672e-02 2.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecReduceArith      1346 1.0 1.0532e-01 1.2 1.35e+08 1.0 0.0e+00 0.0e+00 0.0e+00  4 12  0  0  0   4 12  0  0  0 25560       0      0 0.00e+00    0 0.00e+00  0
VecReduceComm        673 1.0 5.1655e-02 2.6 0.00e+00 0.0 0.0e+00 0.0e+00 6.7e+02  1  0  0  0 49   1  0  0  0 50     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.6160e-06 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.1849e-03 2.9 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin       674 1.0 1.3725e-02 1.7 0.00e+00 0.0 2.6e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd         674 1.0 4.4675e-02 3.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack               674 1.0 2.8206e-03 5.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack             674 1.0 4.6130e-04 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 1.2337e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.3111e+00 1.0 1.11e+09 1.0 2.6e+04 8.0e+03 1.3e+03 86100100100 99  86100100100 99  9577       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 2.8400e-07 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply              673 1.0 1.1117e+00 1.1 3.34e+08 1.0 0.0e+00 0.0e+00 0.0e+00 38 30  0  0  0  38 30  0  0  0  6004       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    11             11      3626880     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1416     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.26e-08
Average time for MPI_Barrier(): 1.27612e-05
Average time for zero size MPI_Send(): 6.11815e-06
#PETSc Option Table entries:
-ksp_type cr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type sor
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.000656407 iterations 374
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:36:58 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.096e+00     1.000   3.095e+00
Objects:              4.400e+01     1.000   4.400e+01
Flop:                 1.145e+09     1.001   1.145e+09  2.290e+10
Flop/sec:             3.701e+08     1.001   3.700e+08  7.400e+09
MPI Messages:         1.526e+03     2.000   1.450e+03  2.899e+04
MPI Message Lengths:  1.218e+07     2.000   7.984e+03  2.315e+08
MPI Reductions:       7.910e+02     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.0954e+00 100.0%  2.2905e+10 100.0%  2.899e+04 100.0%  7.984e+03      100.0%  7.840e+02  99.1%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 6.7266e-0364.6 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 6.7012e-03119.7 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult              761 1.0 8.8201e-01 1.1 3.42e+08 1.0 2.9e+04 8.0e+03 0.0e+00 27 30100100  0  27 30100100  0  7758       0      0 0.00e+00    0 0.00e+00  0
MatSOR               761 1.0 1.2170e+00 1.1 3.77e+08 1.0 0.0e+00 0.0e+00 0.0e+00 38 33  0  0  0  38 33  0  0  0  6201       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 6.7484e-0361.9 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.8010e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  1   1  0  0  0  1     0       0      0 0.00e+00    0 0.00e+00  0
VecDot               374 1.0 1.1726e-01 2.6 3.74e+07 1.0 0.0e+00 0.0e+00 3.7e+02  3  3  0  0 47   3  3  0  0 48  6379       0      0 0.00e+00    0 0.00e+00  0
VecMDot               10 1.0 4.2835e-03 1.2 5.50e+06 1.0 0.0e+00 0.0e+00 1.0e+01  0  0  0  0  1   0  0  0  0  1 25680       0      0 0.00e+00    0 0.00e+00  0
VecNorm               12 1.0 7.8664e-03 1.1 1.20e+06 1.0 0.0e+00 0.0e+00 1.2e+01  0  0  0  0  2   0  0  0  0  2  3051       0      0 0.00e+00    0 0.00e+00  0
VecScale              11 1.0 2.6012e-04 1.1 5.50e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 42288       0      0 0.00e+00    0 0.00e+00  0
VecCopy              754 1.0 3.2729e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet               755 1.0 2.0034e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY              750 1.0 8.1925e-02 1.1 7.50e+07 1.0 0.0e+00 0.0e+00 0.0e+00  3  7  0  0  0   3  7  0  0  0 18309       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             1496 1.0 2.1107e-01 1.1 1.31e+08 1.0 0.0e+00 0.0e+00 0.0e+00  7 11  0  0  0   7 11  0  0  0 12399       0      0 0.00e+00    0 0.00e+00  0
VecAXPBYCZ           375 1.0 6.8135e-02 1.1 9.38e+07 1.0 0.0e+00 0.0e+00 0.0e+00  2  8  0  0  0   2  8  0  0  0 27519       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY              11 1.0 5.0567e-03 1.1 6.50e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 25709       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin      761 1.0 1.5622e-02 1.3 0.00e+00 0.0 2.9e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd        761 1.0 1.1698e-01 7.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecReduceArith       750 1.0 5.7088e-02 1.2 7.50e+07 1.0 0.0e+00 0.0e+00 0.0e+00  2  7  0  0  0   2  7  0  0  0 26275       0      0 0.00e+00    0 0.00e+00  0
VecReduceComm        375 1.0 3.0057e-02 2.3 0.00e+00 0.0 0.0e+00 0.0e+00 3.8e+02  1  0  0  0 47   1  0  0  0 48     0       0      0 0.00e+00    0 0.00e+00  0
VecNormalize          11 1.0 8.0844e-03 1.1 1.65e+06 1.0 0.0e+00 0.0e+00 1.1e+01  0  0  0  0  1   0  0  0  0  1  4082       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.5890e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.7679e-03 4.4 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin       761 1.0 1.4018e-02 1.4 0.00e+00 0.0 2.9e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd         761 1.0 1.1595e-01 7.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack               761 1.0 2.9105e-03 9.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack             761 1.0 4.5973e-04 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.4702e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  5  2  1  1  3   5  2  1  1  3  3231       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.7041e+00 1.0 1.14e+09 1.0 2.9e+04 8.0e+03 7.7e+02 87100100100 98  87100100100 98  8466       0      0 0.00e+00    0 0.00e+00  0
KSPGMRESOrthog        10 1.0 1.2756e-02 1.1 1.10e+07 1.0 0.0e+00 0.0e+00 1.0e+01  0  1  0  0  1   0  1  0  0  1 17247       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 1.4605e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  5  2  1  1  3   5  2  1  1  3  3252       0      0 0.00e+00    0 0.00e+00  0
PCApply              386 1.0 1.8624e+00 1.0 6.96e+08 1.0 1.4e+04 8.0e+03 0.0e+00 59 61 49 49  0  59 61 49 49  0  7474       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    30             30     11259712     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     3              3        33432     0.
      Preconditioner     3              3         2936     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.1e-08
Average time for MPI_Barrier(): 1.1435e-05
Average time for zero size MPI_Send(): 6.31615e-06
#PETSc Option Table entries:
-ksp_type cr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.0311837 iterations 7307
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:37:39 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.877e+01     1.000   3.877e+01
Objects:              8.400e+01     1.000   8.400e+01
Flop:                 4.268e+10     1.000   4.268e+10  8.536e+11
Flop/sec:             1.101e+09     1.000   1.101e+09  2.202e+10
MPI Messages:         1.462e+04     2.000   1.389e+04  2.778e+05
MPI Message Lengths:  1.170e+08     2.000   7.998e+03  2.222e+09
MPI Reductions:       2.170e+04     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.8772e+01 100.0%  8.5362e+11 100.0%  2.778e+05 100.0%  7.998e+03      100.0%  2.169e+04 100.0%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 4.7264e-03 5.0 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 4.6854e-0366.7 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             7309 1.0 7.5279e+00 1.0 3.29e+09 1.0 2.8e+05 8.0e+03 0.0e+00 19  8100100  0  19  8100100  0  8731       0      0 0.00e+00    0 0.00e+00  0
MatSolve            7307 1.0 8.0251e+00 1.0 3.26e+09 1.0 0.0e+00 0.0e+00 0.0e+00 21  8  0  0  0  21  8  0  0  0  8118       0      0 0.00e+00    0 0.00e+00  0
MatLUFactorNum         1 1.0 7.5381e-03 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1440       0      0 0.00e+00    0 0.00e+00  0
MatILUFactorSym        1 1.0 3.4308e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 4.7389e-0338.4 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.8373e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetRowIJ            1 1.0 3.0150e-06 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetOrdering         1 1.0 6.9471e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDotNorm2         7307 1.0 1.5454e+00 1.1 1.46e+09 1.0 0.0e+00 0.0e+00 7.3e+03  4  3  0  0 34   4  3  0  0 34 18913       0      0 0.00e+00    0 0.00e+00  0
VecMDot             7063 1.0 5.8022e+00 1.0 1.06e+10 1.0 0.0e+00 0.0e+00 7.1e+03 15 25  0  0 33  15 25  0  0 33 36483       0      0 0.00e+00    0 0.00e+00  0
VecNorm             7309 1.0 5.9941e-01 1.1 7.31e+08 1.0 0.0e+00 0.0e+00 7.3e+03  1  2  0  0 34   1  2  0  0 34 24387       0      0 0.00e+00    0 0.00e+00  0
VecScale           14614 1.0 3.1896e-01 1.0 7.31e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1  2  0  0  0   1  2  0  0  0 45818       0      0 0.00e+00    0 0.00e+00  0
VecSet              7310 1.0 4.5891e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY            14615 1.0 7.2331e-01 1.1 1.46e+09 1.0 0.0e+00 0.0e+00 0.0e+00  2  3  0  0  0   2  3  0  0  0 40411       0      0 0.00e+00    0 0.00e+00  0
VecAYPX                1 1.0 1.7308e-04 1.1 5.00e+04 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  5778       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY           14126 1.0 1.3605e+01 1.0 2.12e+10 1.0 0.0e+00 0.0e+00 0.0e+00 35 50  0  0  0  35 50  0  0  0 31118       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     7309 1.0 1.0910e-01 1.5 0.00e+00 0.0 2.8e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       7309 1.0 2.9851e-01 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.4900e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.0141e-03 2.6 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      7309 1.0 9.7181e-02 1.6 0.00e+00 0.0 2.8e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        7309 1.0 2.8815e-01 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              7309 1.0 2.5901e-02 8.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            7309 1.0 4.1440e-03 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.6987e-02 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 3.8390e+01 1.0 4.27e+10 1.0 2.8e+05 8.0e+03 2.2e+04 99100100100100  99100100100100 22235       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                2 1.0 2.7888e-02 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   389       0      0 0.00e+00    0 0.00e+00  0
PCSetUpOnBlocks        1 1.0 1.1726e-02 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   926       0      0 0.00e+00    0 0.00e+00  0
PCApply             7307 1.0 8.5486e+00 1.0 3.26e+09 1.0 0.0e+00 0.0e+00 0.0e+00 22  8  0  0  0  22  8  0  0  0  7621       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              4     11791508     0.
         Vec Scatter     1              1          816     0.
              Vector    68             68     25725120     0.
           Index Set     5              5       608512     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     2              2         2896     0.
      Preconditioner     2              2         1944     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.05e-08
Average time for MPI_Barrier(): 1.1939e-05
Average time for zero size MPI_Send(): 6.54735e-06
#PETSc Option Table entries:
-ksp_type gcr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type bjacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 186.34 iterations 10000
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:38:23 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           4.258e+01     1.000   4.258e+01
Objects:              7.700e+01     1.000   7.700e+01
Flop:                 5.447e+10     1.000   5.447e+10  1.089e+12
Flop/sec:             1.279e+09     1.000   1.279e+09  2.558e+10
MPI Messages:         2.001e+04     2.000   1.901e+04  3.802e+05
MPI Message Lengths:  1.600e+08     2.000   7.999e+03  3.041e+09
MPI Reductions:       2.969e+04     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 4.2582e+01 100.0%  1.0893e+12 100.0%  3.802e+05 100.0%  7.999e+03      100.0%  2.968e+04 100.0%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 3.8522e-0341.4 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 2.9515e-0367.5 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult            10002 1.0 1.0292e+01 1.0 4.50e+09 1.0 3.8e+05 8.0e+03 0.0e+00 24  8100100  0  24  8100100  0  8738       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 3.0041e-0332.3 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7816e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDotNorm2        10000 1.0 2.1587e+00 1.1 2.00e+09 1.0 0.0e+00 0.0e+00 1.0e+04  5  4  0  0 34   5  4  0  0 34 18530       0      0 0.00e+00    0 0.00e+00  0
VecMDot             9666 1.0 7.5932e+00 1.0 1.45e+10 1.0 0.0e+00 0.0e+00 9.7e+03 17 27  0  0 33  17 27  0  0 33 38165       0      0 0.00e+00    0 0.00e+00  0
VecNorm            10002 1.0 8.3025e-01 1.1 1.00e+09 1.0 0.0e+00 0.0e+00 1.0e+04  2  2  0  0 34   2  2  0  0 34 24094       0      0 0.00e+00    0 0.00e+00  0
VecScale           20000 1.0 4.5956e-01 1.0 1.00e+09 1.0 0.0e+00 0.0e+00 0.0e+00  1  2  0  0  0   1  2  0  0  0 43519       0      0 0.00e+00    0 0.00e+00  0
VecSet                 3 1.0 2.2137e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY            20001 1.0 9.8820e-01 1.1 2.00e+09 1.0 0.0e+00 0.0e+00 0.0e+00  2  4  0  0  0   2  4  0  0  0 40480       0      0 0.00e+00    0 0.00e+00  0
VecAYPX                1 1.0 1.7116e-04 1.1 5.00e+04 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  5842       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY           19332 1.0 1.8657e+01 1.0 2.90e+10 1.0 0.0e+00 0.0e+00 0.0e+00 44 53  0  0  0  44 53  0  0  0 31067       0      0 0.00e+00    0 0.00e+00  0
VecPointwiseMult   10000 1.0 1.5153e+00 1.0 5.00e+08 1.0 0.0e+00 0.0e+00 0.0e+00  4  1  0  0  0   4  1  0  0  0  6599       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin    10002 1.0 1.7271e-01 1.6 0.00e+00 0.0 3.8e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd      10002 1.0 3.7504e-01 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.8860e-06 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.1011e-03 2.7 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin     10002 1.0 1.4828e-01 1.7 0.00e+00 0.0 3.8e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd       10002 1.0 3.6275e-01 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack             10002 1.0 4.2946e-02 7.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack           10002 1.0 6.6916e-03 2.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 1.3284e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 4.2199e+01 1.0 5.45e+10 1.0 3.8e+05 8.0e+03 3.0e+04 99100100100100  99100100100100 25814       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 1.8330e-06 3.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply            10000 1.0 1.5323e+00 1.0 5.00e+08 1.0 0.0e+00 0.0e+00 2.0e+00  4  1  0  0  0   4  1  0  0  0  6526       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      8013484     0.
         Vec Scatter     1              1          816     0.
              Vector    67             67     26123648     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1480     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.24e-08
Average time for MPI_Barrier(): 1.07638e-05
Average time for zero size MPI_Send(): 6.68585e-06
#PETSc Option Table entries:
-ksp_type gcr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type jacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.0316741 iterations 6946
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:39:05 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           4.009e+01     1.000   4.009e+01
Objects:              7.600e+01     1.000   7.600e+01
Flop:                 4.092e+10     1.000   4.092e+10  8.183e+11
Flop/sec:             1.021e+09     1.000   1.021e+09  2.041e+10
MPI Messages:         1.390e+04     2.000   1.320e+04  2.641e+05
MPI Message Lengths:  1.112e+08     2.000   7.998e+03  2.112e+09
MPI Reductions:       2.063e+04     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 4.0093e+01 100.0%  8.1834e+11 100.0%  2.641e+05 100.0%  7.998e+03      100.0%  2.062e+04 100.0%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 6.4562e-0355.5 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 6.4134e-0384.8 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             6948 1.0 7.1564e+00 1.0 3.13e+09 1.0 2.6e+05 8.0e+03 0.0e+00 18  8100100  0  18  8100100  0  8730       0      0 0.00e+00    0 0.00e+00  0
MatSOR              6946 1.0 1.0868e+01 1.0 3.44e+09 1.0 0.0e+00 0.0e+00 0.0e+00 27  8  0  0  0  27  8  0  0  0  6338       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 6.4640e-0347.1 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7880e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDotNorm2         6946 1.0 1.5511e+00 1.1 1.39e+09 1.0 0.0e+00 0.0e+00 6.9e+03  4  3  0  0 34   4  3  0  0 34 17913       0      0 0.00e+00    0 0.00e+00  0
VecMDot             6714 1.0 5.8671e+00 1.0 1.01e+10 1.0 0.0e+00 0.0e+00 6.7e+03 14 25  0  0 33  14 25  0  0 33 34294       0      0 0.00e+00    0 0.00e+00  0
VecNorm             6948 1.0 6.0286e-01 1.1 6.95e+08 1.0 0.0e+00 0.0e+00 6.9e+03  1  2  0  0 34   1  2  0  0 34 23050       0      0 0.00e+00    0 0.00e+00  0
VecScale           13892 1.0 3.2549e-01 1.0 6.95e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1  2  0  0  0   1  2  0  0  0 42680       0      0 0.00e+00    0 0.00e+00  0
VecSet                 3 1.0 2.2612e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY            13893 1.0 7.1967e-01 1.1 1.39e+09 1.0 0.0e+00 0.0e+00 0.0e+00  2  3  0  0  0   2  3  0  0  0 38609       0      0 0.00e+00    0 0.00e+00  0
VecAYPX                1 1.0 1.6816e-04 1.1 5.00e+04 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  5947       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY           13428 1.0 1.3040e+01 1.0 2.01e+10 1.0 0.0e+00 0.0e+00 0.0e+00 32 49  0  0  0  32 49  0  0  0 30861       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     6948 1.0 1.2780e-01 1.8 0.00e+00 0.0 2.6e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       6948 1.0 3.3189e-01 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.7790e-06 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 9.5555e-04 2.5 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      6948 1.0 1.1257e-01 1.9 0.00e+00 0.0 2.6e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        6948 1.0 3.2297e-01 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              6948 1.0 3.0465e-02 9.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            6948 1.0 3.9674e-03 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 1.4590e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 3.9712e+01 1.0 4.09e+10 1.0 2.6e+05 8.0e+03 2.1e+04 99100100100100  99100100100100 20606       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 6.3100e-07 2.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply             6946 1.0 1.0881e+01 1.0 3.44e+09 1.0 0.0e+00 0.0e+00 0.0e+00 27  8  0  0  0  27  8  0  0  0  6330       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    66             66     25721920     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1480     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.11e-08
Average time for MPI_Barrier(): 1.08382e-05
Average time for zero size MPI_Send(): 6.32745e-06
#PETSc Option Table entries:
-ksp_type gcr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type sor
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.0315697 iterations 3254
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:39:37 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.014e+01     1.000   3.014e+01
Objects:              9.900e+01     1.000   9.900e+01
Flop:                 2.355e+10     1.001   2.355e+10  4.711e+11
Flop/sec:             7.814e+08     1.001   7.814e+08  1.563e+10
MPI Messages:         1.304e+04     2.000   1.239e+04  2.478e+05
MPI Message Lengths:  1.043e+08     2.000   7.998e+03  1.982e+09
MPI Reductions:       9.698e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.0144e+01 100.0%  4.7106e+11 100.0%  2.478e+05 100.0%  7.998e+03      100.0%  9.691e+03  99.9%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 6.2406e-0312.3 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 5.0591e-03105.7 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             6520 1.0 7.5753e+00 1.1 2.93e+09 1.0 2.5e+05 8.0e+03 0.0e+00 23 12100100  0  23 12100100  0  7739       0      0 0.00e+00    0 0.00e+00  0
MatSOR              6519 1.0 1.0068e+01 1.0 3.23e+09 1.0 0.0e+00 0.0e+00 0.0e+00 33 14  0  0  0  33 14  0  0  0  6421       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 5.1097e-0351.1 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7643e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDotNorm2         3254 1.0 7.5199e-01 1.1 6.51e+08 1.0 0.0e+00 0.0e+00 3.3e+03  2  3  0  0 34   2  3  0  0 34 17309       0      0 0.00e+00    0 0.00e+00  0
VecMDot             3155 1.0 3.4688e+00 1.4 4.71e+09 1.0 0.0e+00 0.0e+00 3.2e+03 11 20  0  0 33  11 20  0  0 33 27171       0      0 0.00e+00    0 0.00e+00  0
VecNorm             3267 1.0 2.8875e-01 1.1 3.27e+08 1.0 0.0e+00 0.0e+00 3.3e+03  1  1  0  0 34   1  1  0  0 34 22629       0      0 0.00e+00    0 0.00e+00  0
VecScale            6519 1.0 1.5404e-01 1.0 3.26e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1  1  0  0  0   1  1  0  0  0 42319       0      0 0.00e+00    0 0.00e+00  0
VecCopy             6509 1.0 2.7956e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet              6513 1.0 2.5486e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             6510 1.0 3.4143e-01 1.1 6.51e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1  3  0  0  0   1  3  0  0  0 38134       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             6509 1.0 8.7420e-01 1.0 4.88e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3  2  0  0  0   3  2  0  0  0 11168       0      0 0.00e+00    0 0.00e+00  0
VecAXPBYCZ          3254 1.0 5.5137e-01 1.0 8.14e+08 1.0 0.0e+00 0.0e+00 0.0e+00  2  3  0  0  0   2  3  0  0  0 29508       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY            6301 1.0 6.1280e+00 1.0 9.42e+09 1.0 0.0e+00 0.0e+00 0.0e+00 20 40  0  0  0  20 40  0  0  0 30746       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     6520 1.0 1.2201e-01 1.8 0.00e+00 0.0 2.5e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       6520 1.0 1.0398e+00 4.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecNormalize          11 1.0 8.4473e-03 1.1 1.65e+06 1.0 0.0e+00 0.0e+00 1.1e+01  0  0  0  0  0   0  0  0  0  0  3907       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.6210e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.3015e-03 3.2 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      6520 1.0 1.0783e-01 1.9 0.00e+00 0.0 2.5e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        6520 1.0 1.0296e+00 4.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              6520 1.0 2.3902e-02 9.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            6520 1.0 4.4899e-03 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.5902e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.5e+01  1  0  0  0  0   1  0  0  0  0  2987       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.9759e+01 1.0 2.36e+10 1.0 2.5e+05 8.0e+03 9.7e+03 99100100100100  99100100100100 15829       0      0 0.00e+00    0 0.00e+00  0
KSPGMRESOrthog        10 1.0 1.4099e-02 1.2 1.10e+07 1.0 0.0e+00 0.0e+00 1.0e+01  0  0  0  0  0   0  0  0  0  0 15604       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 1.4553e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  0  0  0  0  0   0  0  0  0  0  3264       0      0 0.00e+00    0 0.00e+00  0
PCApply             3265 1.0 1.6081e+01 1.1 6.00e+09 1.0 1.2e+05 8.0e+03 0.0e+00 51 25 50 50  0  51 25 50 50  0  7458       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    85             85     33354752     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     3              3        33496     0.
      Preconditioner     3              3         2936     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.95e-08
Average time for MPI_Barrier(): 9.6294e-06
Average time for zero size MPI_Send(): 6.14475e-06
#PETSc Option Table entries:
-ksp_type gcr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

