sdumont8072
Norm of error 0.000101716 iterations 641
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:04:33 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           2.147e+00     1.000   2.147e+00
Objects:              2.600e+01     1.000   2.600e+01
Flop:                 9.603e+08     1.001   9.602e+08  1.920e+10
Flop/sec:             4.474e+08     1.001   4.473e+08  8.946e+09
MPI Messages:         1.288e+03     2.000   1.224e+03  2.447e+04
MPI Message Lengths:  1.028e+07     2.000   7.981e+03  1.953e+08
MPI Reductions:       1.943e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 2.1467e+00 100.0%  1.9204e+10 100.0%  2.447e+04 100.0%  7.981e+03      100.0%  1.936e+03  99.6%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 4.7664e-0325.3 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 4.7178e-0384.3 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult              642 1.0 6.8547e-01 1.0 2.89e+08 1.0 2.4e+04 8.0e+03 0.0e+00 32 30100100  0  32 30100100  0  8422       0      0 0.00e+00    0 0.00e+00  0
MatSolve             642 1.0 7.4176e-01 1.0 2.86e+08 1.0 0.0e+00 0.0e+00 0.0e+00 34 30  0  0  0  34 30  0  0  0  7717       0      0 0.00e+00    0 0.00e+00  0
MatLUFactorNum         1 1.0 7.5551e-03 1.0 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1437       0      0 0.00e+00    0 0.00e+00  0
MatILUFactorSym        1 1.0 3.4086e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 4.7680e-0344.2 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7398e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetRowIJ            1 1.0 6.9560e-06 3.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetOrdering         1 1.0 6.9212e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecTDot             1282 1.0 1.1971e-01 1.2 1.28e+08 1.0 0.0e+00 0.0e+00 1.3e+03  5 13  0  0 66   5 13  0  0 66 21417       0      0 0.00e+00    0 0.00e+00  0
VecNorm              643 1.0 8.9725e-02 1.5 6.43e+07 1.0 0.0e+00 0.0e+00 6.4e+02  3  7  0  0 33   3  7  0  0 33 14333       0      0 0.00e+00    0 0.00e+00  0
VecCopy                2 1.0 1.6113e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet               645 1.0 4.8270e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             1283 1.0 6.1517e-02 1.1 1.28e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3 13  0  0  0   3 13  0  0  0 41712       0      0 0.00e+00    0 0.00e+00  0
VecAYPX              640 1.0 8.2184e-02 1.0 6.40e+07 1.0 0.0e+00 0.0e+00 0.0e+00  4  7  0  0  0   4  7  0  0  0 15575       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin      642 1.0 1.0541e-02 1.7 0.00e+00 0.0 2.4e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd        642 1.0 4.4886e-02 3.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.8600e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.4088e-03 3.5 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin       642 1.0 9.4204e-03 1.7 0.00e+00 0.0 2.4e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd         642 1.0 4.4255e-02 3.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack               642 1.0 1.4813e-03 8.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack             642 1.0 4.0371e-04 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 6.3888e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 1.7703e+00 1.0 9.60e+08 1.0 2.4e+04 8.0e+03 1.9e+03 82100100100 99  82100100100 99 10841       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                2 1.0 2.6514e-02 1.0 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0   409       0      0 0.00e+00    0 0.00e+00  0
PCSetUpOnBlocks        1 1.0 1.1716e-02 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0   926       0      0 0.00e+00    0 0.00e+00  0
PCApply              642 1.0 7.5190e-01 1.0 2.86e+08 1.0 0.0e+00 0.0e+00 0.0e+00 34 30  0  0  0  34 30  0  0  0  7613       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              4     11791508     0.
         Vec Scatter     1              1          816     0.
              Vector    10             10      2424896     0.
           Index Set     5              5       608512     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     2              2         2904     0.
      Preconditioner     2              2         1944     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.3e-08
Average time for MPI_Barrier(): 1.2805e-05
Average time for zero size MPI_Send(): 6.36435e-06
#PETSc Option Table entries:
-ksp_type cg
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type bjacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 4.69193e-05 iterations 1715
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:04:38 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.173e+00     1.000   3.173e+00
Objects:              1.900e+01     1.000   1.900e+01
Flop:                 1.887e+09     1.002   1.887e+09  3.773e+10
Flop/sec:             5.947e+08     1.002   5.946e+08  1.189e+10
MPI Messages:         3.436e+03     2.000   3.264e+03  6.528e+04
MPI Message Lengths:  2.746e+07     2.000   7.993e+03  5.218e+08
MPI Reductions:       5.167e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.1726e+00 100.0%  3.7730e+10 100.0%  6.528e+04 100.0%  7.993e+03      100.0%  5.160e+03  99.9%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 4.3232e-0359.6 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 4.2782e-0392.0 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             1716 1.0 1.7739e+00 1.0 7.72e+08 1.0 6.5e+04 8.0e+03 0.0e+00 56 41100100  0  56 41100100  0  8698       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 4.3301e-0343.3 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7567e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecTDot             3430 1.0 2.9853e-01 1.1 3.43e+08 1.0 0.0e+00 0.0e+00 3.4e+03  9 18  0  0 66   9 18  0  0 66 22979       0      0 0.00e+00    0 0.00e+00  0
VecNorm             1717 1.0 1.5488e-01 1.1 1.72e+08 1.0 0.0e+00 0.0e+00 1.7e+03  5  9  0  0 33   5  9  0  0 33 22172       0      0 0.00e+00    0 0.00e+00  0
VecCopy                2 1.0 1.0739e-04 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 3 1.0 2.2063e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             3431 1.0 1.5910e-01 1.1 3.43e+08 1.0 0.0e+00 0.0e+00 0.0e+00  5 18  0  0  0   5 18  0  0  0 43131       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             1714 1.0 2.1397e-01 1.0 1.71e+08 1.0 0.0e+00 0.0e+00 0.0e+00  7  9  0  0  0   7  9  0  0  0 16021       0      0 0.00e+00    0 0.00e+00  0
VecPointwiseMult    1716 1.0 2.3696e-01 1.0 8.58e+07 1.0 0.0e+00 0.0e+00 0.0e+00  7  5  0  0  0   7  5  0  0  0  7242       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     1716 1.0 1.8751e-02 1.6 0.00e+00 0.0 6.5e+04 8.0e+03 0.0e+00  1  0100100  0   1  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       1716 1.0 8.2559e-02 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.6960e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.1238e-03 2.8 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      1716 1.0 1.6873e-02 1.7 0.00e+00 0.0 6.5e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        1716 1.0 8.1097e-02 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              1716 1.0 3.6084e-03 7.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            1716 1.0 8.5892e-04 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 6.3918e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.7896e+00 1.0 1.89e+09 1.0 6.5e+04 8.0e+03 5.1e+03 88100100100100  88100100100100 13521       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 8.2700e-07 3.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply             1716 1.0 2.4001e-01 1.0 8.58e+07 1.0 0.0e+00 0.0e+00 2.0e+00  7  5  0  0  0   7  5  0  0  0  7150       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      8013484     0.
         Vec Scatter     1              1          816     0.
              Vector     9              9      2823424     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1488     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.97e-08
Average time for MPI_Barrier(): 1.07388e-05
Average time for zero size MPI_Send(): 6.4969e-06
#PETSc Option Table entries:
-ksp_type cg
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type jacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.00061416 iterations 692
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:04:43 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           2.634e+00     1.000   2.634e+00
Objects:              1.800e+01     1.000   1.800e+01
Flop:                 1.071e+09     1.001   1.071e+09  2.141e+10
Flop/sec:             4.065e+08     1.001   4.064e+08  8.129e+09
MPI Messages:         1.390e+03     2.000   1.320e+03  2.641e+04
MPI Message Lengths:  1.110e+07     2.000   7.983e+03  2.108e+08
MPI Reductions:       2.096e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 2.6341e+00 100.0%  2.1412e+10 100.0%  2.641e+04 100.0%  7.983e+03      100.0%  2.089e+03  99.7%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 4.7150e-0352.8 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 3.5426e-0367.3 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult              693 1.0 7.5610e-01 1.0 3.12e+08 1.0 2.6e+04 8.0e+03 0.0e+00 28 29100100  0  28 29100100  0  8242       0      0 0.00e+00    0 0.00e+00  0
MatSOR               693 1.0 1.1002e+00 1.1 3.44e+08 1.0 0.0e+00 0.0e+00 0.0e+00 40 32  0  0  0  40 32  0  0  0  6247       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 3.5969e-0334.0 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7947e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecTDot             1384 1.0 1.6743e-01 1.2 1.38e+08 1.0 0.0e+00 0.0e+00 1.4e+03  6 13  0  0 66   6 13  0  0 66 16532       0      0 0.00e+00    0 0.00e+00  0
VecNorm              694 1.0 1.6540e-01 2.0 6.94e+07 1.0 0.0e+00 0.0e+00 6.9e+02  5  6  0  0 33   5  6  0  0 33  8392       0      0 0.00e+00    0 0.00e+00  0
VecCopy                2 1.0 1.3316e-04 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 3 1.0 2.3569e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             1385 1.0 7.7071e-02 1.2 1.38e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3 13  0  0  0   3 13  0  0  0 35941       0      0 0.00e+00    0 0.00e+00  0
VecAYPX              691 1.0 9.4849e-02 1.1 6.91e+07 1.0 0.0e+00 0.0e+00 0.0e+00  3  6  0  0  0   3  6  0  0  0 14571       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin      693 1.0 1.1681e-02 1.5 0.00e+00 0.0 2.6e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd        693 1.0 7.2566e-02 3.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.5310e-06 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.3090e-03 3.2 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin       693 1.0 1.0542e-02 1.6 0.00e+00 0.0 2.6e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd         693 1.0 7.1729e-02 4.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack               693 1.0 2.3686e-03 9.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack             693 1.0 4.9132e-04 2.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 6.4231e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.2479e+00 1.0 1.07e+09 1.0 2.6e+04 8.0e+03 2.1e+03 85100100100 99  85100100100 99  9520       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 5.2700e-07 4.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply              693 1.0 1.1015e+00 1.1 3.44e+08 1.0 0.0e+00 0.0e+00 0.0e+00 40 32  0  0  0  40 32  0  0  0  6239       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector     8              8      2421696     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1488     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.97e-08
Average time for MPI_Barrier(): 1.18502e-05
Average time for zero size MPI_Send(): 6.42255e-06
#PETSc Option Table entries:
-ksp_type cg
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type sor
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.000603072 iterations 383
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:04:48 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.085e+00     1.000   3.085e+00
Objects:              4.100e+01     1.000   4.100e+01
Flop:                 1.134e+09     1.001   1.133e+09  2.267e+10
Flop/sec:             3.674e+08     1.001   3.674e+08  7.347e+09
MPI Messages:         1.560e+03     2.000   1.482e+03  2.964e+04
MPI Message Lengths:  1.246e+07     2.000   7.985e+03  2.367e+08
MPI Reductions:       1.192e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.0853e+00 100.0%  2.2668e+10 100.0%  2.964e+04 100.0%  7.985e+03      100.0%  1.185e+03  99.4%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 3.6830e-0344.0 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 3.6176e-0373.9 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult              778 1.0 9.1443e-01 1.1 3.50e+08 1.0 3.0e+04 8.0e+03 0.0e+00 29 31100100  0  29 31100100  0  7650       0      0 0.00e+00    0 0.00e+00  0
MatSOR               779 1.0 1.2631e+00 1.1 3.86e+08 1.0 0.0e+00 0.0e+00 0.0e+00 39 34  0  0  0  39 34  0  0  0  6116       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 3.6748e-0337.6 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7053e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecMDot               10 1.0 4.1340e-03 1.2 5.50e+06 1.0 0.0e+00 0.0e+00 1.0e+01  0  0  0  0  1   0  0  0  0  1 26609       0      0 0.00e+00    0 0.00e+00  0
VecTDot              766 1.0 9.1336e-02 1.2 7.66e+07 1.0 0.0e+00 0.0e+00 7.7e+02  3  7  0  0 64   3  7  0  0 65 16773       0      0 0.00e+00    0 0.00e+00  0
VecNorm              396 1.0 1.1067e-01 2.3 3.96e+07 1.0 0.0e+00 0.0e+00 4.0e+02  3  3  0  0 33   3  3  0  0 33  7157       0      0 0.00e+00    0 0.00e+00  0
VecScale              11 1.0 2.5182e-04 1.0 5.50e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 43682       0      0 0.00e+00    0 0.00e+00  0
VecCopy              771 1.0 3.4354e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet               773 1.0 5.8419e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY              768 1.0 5.1981e-02 1.1 7.68e+07 1.0 0.0e+00 0.0e+00 0.0e+00  2  7  0  0  0   2  7  0  0  0 29550       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             1150 1.0 1.6774e-01 1.1 9.58e+07 1.0 0.0e+00 0.0e+00 0.0e+00  5  8  0  0  0   5  8  0  0  0 11423       0      0 0.00e+00    0 0.00e+00  0
VecAXPBYCZ           384 1.0 7.3432e-02 1.1 9.60e+07 1.0 0.0e+00 0.0e+00 0.0e+00  2  8  0  0  0   2  8  0  0  0 26147       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY              11 1.0 4.8730e-03 1.0 6.50e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 26678       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin      778 1.0 1.4465e-02 1.6 0.00e+00 0.0 3.0e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd        778 1.0 1.3763e-01 8.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecNormalize          11 1.0 7.8951e-03 1.1 1.65e+06 1.0 0.0e+00 0.0e+00 1.1e+01  0  0  0  0  1   0  0  0  0  1  4180       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.0290e-06 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 2.4827e-03 6.7 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin       778 1.0 1.3032e-02 1.6 0.00e+00 0.0 3.0e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd         778 1.0 1.3656e-01 9.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack               778 1.0 2.7607e-03 8.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack             778 1.0 4.1239e-04 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.4193e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  5  2  1  1  2   5  2  1  1  2  3347       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.6899e+00 1.0 1.13e+09 1.0 3.0e+04 8.0e+03 1.2e+03 87100100100 98  87100100100 99  8422       0      0 0.00e+00    0 0.00e+00  0
KSPGMRESOrthog        10 1.0 1.2544e-02 1.1 1.10e+07 1.0 0.0e+00 0.0e+00 1.0e+01  0  1  0  0  1   0  1  0  0  1 17538       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 1.4154e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  5  2  1  1  2   5  2  1  1  2  3356       0      0 0.00e+00    0 0.00e+00  0
PCApply              395 1.0 1.9178e+00 1.0 7.13e+08 1.0 1.5e+04 8.0e+03 0.0e+00 61 63 49 49  0  61 63 49 49  0  7431       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    27             27     10054528     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     3              3        33504     0.
      Preconditioner     3              3         2936     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.78e-08
Average time for MPI_Barrier(): 1.02268e-05
Average time for zero size MPI_Send(): 6.71025e-06
#PETSc Option Table entries:
-ksp_type cg
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.0125906 iterations 6472
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:05:17 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           2.702e+01     1.000   2.702e+01
Objects:              5.700e+01     1.000   5.700e+01
Flop:                 2.773e+10     1.000   2.773e+10  5.546e+11
Flop/sec:             1.026e+09     1.000   1.026e+09  2.052e+10
MPI Messages:         1.338e+04     2.000   1.271e+04  2.542e+05
MPI Message Lengths:  1.070e+08     2.000   7.998e+03  2.033e+09
MPI Reductions:       1.318e+04     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 2.7023e+01 100.0%  5.5456e+11 100.0%  2.542e+05 100.0%  7.998e+03      100.0%  1.317e+04  99.9%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 6.3987e-0347.7 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 5.4880e-0395.6 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             6688 1.0 6.7992e+00 1.0 3.01e+09 1.0 2.5e+05 8.0e+03 0.0e+00 25 11100100  0  25 11100100  0  8845       0      0 0.00e+00    0 0.00e+00  0
MatSolve            6688 1.0 7.3210e+00 1.0 2.98e+09 1.0 0.0e+00 0.0e+00 0.0e+00 27 11  0  0  0  27 11  0  0  0  8145       0      0 0.00e+00    0 0.00e+00  0
MatLUFactorNum         1 1.0 7.5343e-03 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1440       0      0 0.00e+00    0 0.00e+00  0
MatILUFactorSym        1 1.0 3.3756e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 5.5394e-0351.4 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.8010e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetRowIJ            1 1.0 2.8370e-06 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetOrdering         1 1.0 6.9154e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecMDot             6472 1.0 4.9877e+00 1.0 1.00e+10 1.0 0.0e+00 0.0e+00 6.5e+03 18 36  0  0 49  18 36  0  0 49 40189       0      0 0.00e+00    0 0.00e+00  0
VecNorm             6689 1.0 6.3947e-01 1.1 6.69e+08 1.0 0.0e+00 0.0e+00 6.7e+03  2  2  0  0 51   2  2  0  0 51 20920       0      0 0.00e+00    0 0.00e+00  0
VecScale            6688 1.0 1.3438e-01 1.0 3.34e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 49770       0      0 0.00e+00    0 0.00e+00  0
VecCopy              216 1.0 1.9171e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet              6907 1.0 4.2609e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY              432 1.0 2.1597e-02 1.1 4.32e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 40006       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY            6688 1.0 6.3637e+00 1.0 1.07e+10 1.0 0.0e+00 0.0e+00 0.0e+00 23 38  0  0  0  23 38  0  0  0 33534       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     6688 1.0 9.2037e-02 1.4 0.00e+00 0.0 2.5e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       6688 1.0 2.1579e-01 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecNormalize        6688 1.0 7.8191e-01 1.1 1.00e+09 1.0 0.0e+00 0.0e+00 6.7e+03  3  4  0  0 51   3  4  0  0 51 25660       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.6240e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.1434e-03 3.1 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      6688 1.0 8.0769e-02 1.5 0.00e+00 0.0 2.5e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        6688 1.0 2.0757e-01 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              6688 1.0 2.1155e-02 5.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            6688 1.0 3.6392e-03 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.0470e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.6640e+01 1.0 2.77e+10 1.0 2.5e+05 8.0e+03 1.3e+04 99100100100100  99100100100100 20816       0      0 0.00e+00    0 0.00e+00  0
KSPGMRESOrthog      6472 1.0 1.0959e+01 1.0 2.00e+10 1.0 0.0e+00 0.0e+00 6.5e+03 40 72  0  0 49  40 72  0  0 49 36583       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                2 1.0 1.8247e-02 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   595       0      0 0.00e+00    0 0.00e+00  0
PCSetUpOnBlocks        1 1.0 1.1608e-02 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   935       0      0 0.00e+00    0 0.00e+00  0
PCApply             6688 1.0 7.7855e+00 1.0 2.98e+09 1.0 0.0e+00 0.0e+00 0.0e+00 29 11  0  0  0  29 11  0  0  0  7659       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              4     11791508     0.
         Vec Scatter     1              1          816     0.
              Vector    41             41     14878464     0.
           Index Set     5              5       608512     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     2              2        20072     0.
      Preconditioner     2              2         1944     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.05e-08
Average time for MPI_Barrier(): 1.34112e-05
Average time for zero size MPI_Send(): 6.32455e-06
#PETSc Option Table entries:
-ksp_type gmres
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type bjacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 186.34 iterations 10000
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:05:51 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.189e+01     1.000   3.189e+01
Objects:              5.000e+01     1.000   5.000e+01
Flop:                 3.876e+10     1.001   3.876e+10  7.752e+11
Flop/sec:             1.216e+09     1.001   1.216e+09  2.431e+10
MPI Messages:         2.067e+04     2.000   1.964e+04  3.928e+05
MPI Message Lengths:  1.654e+08     2.000   7.999e+03  3.142e+09
MPI Reductions:       2.036e+04     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.1886e+01 100.0%  7.7519e+11 100.0%  3.928e+05 100.0%  7.999e+03      100.0%  2.035e+04 100.0%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 8.5460e-0339.0 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 8.3042e-03175.5 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult            10334 1.0 1.0531e+01 1.0 4.65e+09 1.0 3.9e+05 8.0e+03 0.0e+00 33 12100100  0  33 12100100  0  8824       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 8.3548e-0381.9 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6920e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecMDot            10000 1.0 7.9658e+00 1.0 1.55e+10 1.0 0.0e+00 0.0e+00 1.0e+04 25 40  0  0 49  25 40  0  0 49 38891       0      0 0.00e+00    0 0.00e+00  0
VecNorm            10335 1.0 9.6790e-01 1.2 1.03e+09 1.0 0.0e+00 0.0e+00 1.0e+04  3  3  0  0 51   3  3  0  0 51 21355       0      0 0.00e+00    0 0.00e+00  0
VecScale           10334 1.0 2.0747e-01 1.0 5.17e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1  1  0  0  0   1  1  0  0  0 49810       0      0 0.00e+00    0 0.00e+00  0
VecCopy              334 1.0 2.9549e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet               337 1.0 2.1092e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY              668 1.0 3.2808e-02 1.1 6.68e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 40722       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY           10334 1.0 1.0421e+01 1.0 1.65e+10 1.0 0.0e+00 0.0e+00 0.0e+00 32 43  0  0  0  32 43  0  0  0 31647       0      0 0.00e+00    0 0.00e+00  0
VecPointwiseMult   10334 1.0 1.5929e+00 1.0 5.17e+08 1.0 0.0e+00 0.0e+00 0.0e+00  5  1  0  0  0   5  1  0  0  0  6487       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin    10334 1.0 1.5475e-01 1.8 0.00e+00 0.0 3.9e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd      10334 1.0 3.5708e-01 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecNormalize       10334 1.0 1.1841e+00 1.1 1.55e+09 1.0 0.0e+00 0.0e+00 1.0e+04  4  4  0  0 51   4  4  0  0 51 26183       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.8310e-06 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 2.5465e-0310.1 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin     10334 1.0 1.3618e-01 1.9 0.00e+00 0.0 3.9e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd       10334 1.0 3.4449e-01 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack             10334 1.0 3.7341e-0210.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack           10334 1.0 5.3872e-03 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 1.0391e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 3.1504e+01 1.0 3.88e+10 1.0 3.9e+05 8.0e+03 2.0e+04 99100100100100  99100100100100 24606       0      0 0.00e+00    0 0.00e+00  0
KSPGMRESOrthog     10000 1.0 1.7734e+01 1.0 3.10e+10 1.0 0.0e+00 0.0e+00 1.0e+04 55 80  0  0 49  55 80  0  0 49 34938       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 5.2900e-07 3.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply            10334 1.0 1.6086e+00 1.0 5.17e+08 1.0 0.0e+00 0.0e+00 2.0e+00  5  1  0  0  0   5  1  0  0  0  6424       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      8013484     0.
         Vec Scatter     1              1          816     0.
              Vector    40             40     15276992     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1        18656     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.95e-08
Average time for MPI_Barrier(): 1.105e-05
Average time for zero size MPI_Send(): 6.6024e-06
#PETSc Option Table entries:
-ksp_type gmres
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type jacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.0487126 iterations 10000
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:06:40 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           4.720e+01     1.000   4.720e+01
Objects:              4.900e+01     1.000   4.900e+01
Flop:                 4.337e+10     1.000   4.337e+10  8.673e+11
Flop/sec:             9.189e+08     1.000   9.189e+08  1.838e+10
MPI Messages:         2.067e+04     2.000   1.964e+04  3.928e+05
MPI Message Lengths:  1.654e+08     2.000   7.999e+03  3.142e+09
MPI Reductions:       2.035e+04     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 4.7196e+01 100.0%  8.6733e+11 100.0%  3.928e+05 100.0%  7.999e+03      100.0%  2.035e+04 100.0%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 6.2221e-0312.4 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 6.1998e-03125.5 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult            10334 1.0 1.0462e+01 1.0 4.65e+09 1.0 3.9e+05 8.0e+03 0.0e+00 22 11100100  0  22 11100100  0  8882       0      0 0.00e+00    0 0.00e+00  0
MatSOR             10334 1.0 1.5840e+01 1.0 5.12e+09 1.0 0.0e+00 0.0e+00 0.0e+00 33 12  0  0  0  33 12  0  0  0  6469       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 6.2502e-0362.0 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6812e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecMDot            10000 1.0 9.0287e+00 1.1 1.55e+10 1.0 0.0e+00 0.0e+00 1.0e+04 19 36  0  0 49  19 36  0  0 49 34312       0      0 0.00e+00    0 0.00e+00  0
VecNorm            10335 1.0 1.0199e+00 1.1 1.03e+09 1.0 0.0e+00 0.0e+00 1.0e+04  2  2  0  0 51   2  2  0  0 51 20266       0      0 0.00e+00    0 0.00e+00  0
VecScale           10334 1.0 2.1062e-01 1.1 5.17e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 49065       0      0 0.00e+00    0 0.00e+00  0
VecCopy              334 1.0 2.9415e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet               337 1.0 2.1222e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY              668 1.0 3.2669e-02 1.1 6.68e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 40894       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY           10334 1.0 1.0622e+01 1.0 1.65e+10 1.0 0.0e+00 0.0e+00 0.0e+00 22 38  0  0  0  22 38  0  0  0 31050       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin    10334 1.0 1.6505e-01 1.7 0.00e+00 0.0 3.9e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd      10334 1.0 3.3573e-01 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecNormalize       10334 1.0 1.2393e+00 1.1 1.55e+09 1.0 0.0e+00 0.0e+00 1.0e+04  3  4  0  0 51   3  4  0  0 51 25015       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.6840e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.5065e-03 3.9 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin     10334 1.0 1.4260e-01 1.7 0.00e+00 0.0 3.9e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd       10334 1.0 3.2143e-01 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack             10334 1.0 3.9983e-02 9.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack           10334 1.0 6.1892e-03 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 1.0497e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 4.6817e+01 1.0 4.34e+10 1.0 3.9e+05 8.0e+03 2.0e+04 99100100100100  99100100100100 18526       0      0 0.00e+00    0 0.00e+00  0
KSPGMRESOrthog     10000 1.0 1.9049e+01 1.0 3.10e+10 1.0 0.0e+00 0.0e+00 1.0e+04 40 71  0  0 49  40 71  0  0 49 32527       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 6.9500e-07 4.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply            10334 1.0 1.5862e+01 1.0 5.12e+09 1.0 0.0e+00 0.0e+00 0.0e+00 33 12  0  0  0  33 12  0  0  0  6460       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    39             39     14875264     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1        18656     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.26e-08
Average time for MPI_Barrier(): 1.12474e-05
Average time for zero size MPI_Send(): 6.197e-06
#PETSc Option Table entries:
-ksp_type gmres
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type sor
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.00984153 iterations 3132
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:07:07 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           2.517e+01     1.000   2.517e+01
Objects:              7.200e+01     1.000   7.200e+01
Flop:                 1.795e+10     1.001   1.795e+10  3.589e+11
Flop/sec:             7.130e+08     1.001   7.129e+08  1.426e+10
MPI Messages:         1.297e+04     2.000   1.232e+04  2.465e+05
MPI Message Lengths:  1.038e+08     2.000   7.998e+03  1.971e+09
MPI Reductions:       6.411e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 2.5172e+01 100.0%  3.5893e+11 100.0%  2.465e+05 100.0%  7.998e+03      100.0%  6.404e+03  99.9%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 5.6916e-0317.1 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 4.5722e-03104.4 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             6484 1.0 6.6531e+00 1.0 2.92e+09 1.0 2.5e+05 8.0e+03 0.0e+00 26 16100100  0  26 16100100  0  8764       0      0 0.00e+00    0 0.00e+00  0
MatSOR              6485 1.0 9.7690e+00 1.0 3.22e+09 1.0 0.0e+00 0.0e+00 0.0e+00 38 18  0  0  0  38 18  0  0  0  6583       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 4.6215e-0347.9 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7722e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecMDot             3142 1.0 2.8701e+00 1.1 4.85e+09 1.0 0.0e+00 0.0e+00 3.1e+03 11 27  0  0 49  11 27  0  0 49 33792       0      0 0.00e+00    0 0.00e+00  0
VecNorm             3249 1.0 3.4761e-01 1.2 3.25e+08 1.0 0.0e+00 0.0e+00 3.2e+03  1  2  0  0 51   1  2  0  0 51 18694       0      0 0.00e+00    0 0.00e+00  0
VecScale            3248 1.0 6.5786e-02 1.0 1.62e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 49372       0      0 0.00e+00    0 0.00e+00  0
VecCopy             6580 1.0 3.4170e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet              6584 1.0 2.5984e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY              211 1.0 1.0406e-02 1.1 2.11e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 40555       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             6474 1.0 8.8602e-01 1.0 4.86e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3  3  0  0  0   3  3  0  0  0 10960       0      0 0.00e+00    0 0.00e+00  0
VecAXPBYCZ          3237 1.0 5.5921e-01 1.0 8.09e+08 1.0 0.0e+00 0.0e+00 0.0e+00  2  5  0  0  0   2  5  0  0  0 28943       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY            3248 1.0 3.1450e+00 1.0 5.16e+09 1.0 0.0e+00 0.0e+00 0.0e+00 12 29  0  0  0  12 29  0  0  0 32836       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     6484 1.0 1.1084e-01 1.8 0.00e+00 0.0 2.5e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       6484 1.0 3.0933e-01 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecNormalize        3248 1.0 4.1541e-01 1.2 4.87e+08 1.0 0.0e+00 0.0e+00 3.2e+03  2  3  0  0 51   2  3  0  0 51 23456       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.7090e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.3946e-03 3.5 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      6484 1.0 9.8776e-02 1.9 0.00e+00 0.0 2.5e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        6484 1.0 3.0110e-01 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              6484 1.0 2.3363e-02 7.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            6484 1.0 3.4588e-03 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.4584e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  1  0  0  0  0   1  0  0  0  0  3257       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.4786e+01 1.0 1.79e+10 1.0 2.5e+05 8.0e+03 6.4e+03 98100100100100  98100100100100 14480       0      0 0.00e+00    0 0.00e+00  0
KSPGMRESOrthog      3142 1.0 5.8146e+00 1.0 9.70e+09 1.0 0.0e+00 0.0e+00 3.1e+03 23 54  0  0 49  23 54  0  0 49 33359       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 1.4506e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  1  0  0  0  0   1  0  0  0  0  3274       0      0 0.00e+00    0 0.00e+00  0
PCApply             3248 1.0 1.5143e+01 1.0 5.97e+09 1.0 1.2e+05 8.0e+03 0.0e+00 60 33 50 50  0  60 33 50 50  0  7879       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    58             58     22508096     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     3              3        50672     0.
      Preconditioner     3              3         2936     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.31e-08
Average time for MPI_Barrier(): 1.14668e-05
Average time for zero size MPI_Send(): 6.6768e-06
#PETSc Option Table entries:
-ksp_type gmres
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.000101716 iterations 641
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:07:12 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.305e+00     1.000   3.305e+00
Objects:              8.700e+01     1.000   8.700e+01
Flop:                 2.892e+09     1.000   2.892e+09  5.784e+10
Flop/sec:             8.750e+08     1.000   8.750e+08  1.750e+10
MPI Messages:         1.288e+03     2.000   1.224e+03  2.447e+04
MPI Message Lengths:  1.028e+07     2.000   7.981e+03  1.953e+08
MPI Reductions:       2.583e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.3054e+00 100.0%  5.7844e+10 100.0%  2.447e+04 100.0%  7.981e+03      100.0%  2.576e+03  99.7%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 2.0458e-0331.0 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 2.0118e-0346.3 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult              642 1.0 6.8779e-01 1.1 2.89e+08 1.0 2.4e+04 8.0e+03 0.0e+00 20 10100100  0  20 10100100  0  8393       0      0 0.00e+00    0 0.00e+00  0
MatSolve             642 1.0 7.0888e-01 1.0 2.86e+08 1.0 0.0e+00 0.0e+00 0.0e+00 21 10  0  0  0  21 10  0  0  0  8075       0      0 0.00e+00    0 0.00e+00  0
MatLUFactorNum         1 1.0 7.5035e-03 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1446       0      0 0.00e+00    0 0.00e+00  0
MatILUFactorSym        1 1.0 3.3644e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 2.0608e-0321.6 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7900e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetRowIJ            1 1.0 3.0110e-06 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetOrdering         1 1.0 6.8737e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecTDot             1282 1.0 1.5063e-01 1.3 1.28e+08 1.0 0.0e+00 0.0e+00 1.3e+03  4  4  0  0 50   4  4  0  0 50 17022       0      0 0.00e+00    0 0.00e+00  0
VecMTDot             640 1.0 5.4854e-01 1.0 9.82e+08 1.0 0.0e+00 0.0e+00 6.4e+02 17 34  0  0 25  17 34  0  0 25 35804       0      0 0.00e+00    0 0.00e+00  0
VecNorm              643 1.0 7.3636e-02 1.3 6.43e+07 1.0 0.0e+00 0.0e+00 6.4e+02  2  2  0  0 25   2  2  0  0 25 17464       0      0 0.00e+00    0 0.00e+00  0
VecScale             640 1.0 2.2077e-02 1.0 3.20e+07 1.0 0.0e+00 0.0e+00 0.0e+00  1  1  0  0  0   1  1  0  0  0 28990       0      0 0.00e+00    0 0.00e+00  0
VecCopy              642 1.0 3.9268e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet               645 1.0 1.3917e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             1283 1.0 7.4879e-02 1.1 1.28e+08 1.0 0.0e+00 0.0e+00 0.0e+00  2  4  0  0  0   2  4  0  0  0 34269       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY             640 1.0 6.1528e-01 1.0 9.82e+08 1.0 0.0e+00 0.0e+00 0.0e+00 18 34  0  0  0  18 34  0  0  0 31921       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin      642 1.0 1.0907e-02 1.4 0.00e+00 0.0 2.4e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd        642 1.0 4.8547e-02 3.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.5330e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.6481e-03 4.2 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin       642 1.0 9.8242e-03 1.4 0.00e+00 0.0 2.4e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd         642 1.0 4.7697e-02 3.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack               642 1.0 2.0153e-03 7.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack             642 1.0 3.5828e-04 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 4.3795e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.9246e+00 1.0 2.89e+09 1.0 2.4e+04 8.0e+03 2.6e+03 88100100100 99  88100100100100 19774       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                2 1.0 2.5694e-02 1.2 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0   422       0      0 0.00e+00    0 0.00e+00  0
PCSetUpOnBlocks        1 1.0 1.1572e-02 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   938       0      0 0.00e+00    0 0.00e+00  0
PCApply              642 1.0 7.2890e-01 1.0 2.86e+08 1.0 0.0e+00 0.0e+00 0.0e+00 22 10  0  0  0  22 10  0  0  0  7853       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              4     11791508     0.
         Vec Scatter     1              1          816     0.
              Vector    71             71     26930304     0.
           Index Set     5              5       608512     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     2              2         4080     0.
      Preconditioner     2              2         1944     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.02e-08
Average time for MPI_Barrier(): 1.17146e-05
Average time for zero size MPI_Send(): 6.61885e-06
#PETSc Option Table entries:
-ksp_type fcg
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type bjacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 4.69193e-05 iterations 1715
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:07:20 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           6.520e+00     1.000   6.520e+00
Objects:              8.000e+01     1.000   8.000e+01
Flop:                 7.104e+09     1.000   7.104e+09  1.421e+11
Flop/sec:             1.090e+09     1.000   1.090e+09  2.179e+10
MPI Messages:         3.436e+03     2.000   3.264e+03  6.528e+04
MPI Message Lengths:  2.746e+07     2.000   7.993e+03  5.218e+08
MPI Reductions:       6.881e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 6.5198e+00 100.0%  1.4208e+11 100.0%  6.528e+04 100.0%  7.993e+03      100.0%  6.874e+03  99.9%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 3.6979e-0339.8 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 3.6337e-0373.9 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             1716 1.0 1.8113e+00 1.0 7.72e+08 1.0 6.5e+04 8.0e+03 0.0e+00 27 11100100  0  27 11100100  0  8519       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 3.6935e-0336.8 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7852e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecTDot             3430 1.0 4.1752e-01 1.4 3.43e+08 1.0 0.0e+00 0.0e+00 3.4e+03  6  5  0  0 50   6  5  0  0 50 16430       0      0 0.00e+00    0 0.00e+00  0
VecMTDot            1714 1.0 1.4430e+00 1.0 2.65e+09 1.0 0.0e+00 0.0e+00 1.7e+03 22 37  0  0 25  22 37  0  0 25 36751       0      0 0.00e+00    0 0.00e+00  0
VecNorm             1717 1.0 1.7009e-01 1.2 1.72e+08 1.0 0.0e+00 0.0e+00 1.7e+03  2  2  0  0 25   2  2  0  0 25 20189       0      0 0.00e+00    0 0.00e+00  0
VecScale            1714 1.0 5.0423e-02 1.1 8.57e+07 1.0 0.0e+00 0.0e+00 0.0e+00  1  1  0  0  0   1  1  0  0  0 33993       0      0 0.00e+00    0 0.00e+00  0
VecCopy             1716 1.0 1.1121e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 3 1.0 2.1845e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             3431 1.0 2.0901e-01 1.1 3.43e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3  5  0  0  0   3  5  0  0  0 32831       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY            1714 1.0 1.7945e+00 1.0 2.65e+09 1.0 0.0e+00 0.0e+00 0.0e+00 27 37  0  0  0  27 37  0  0  0 29551       0      0 0.00e+00    0 0.00e+00  0
VecPointwiseMult    1716 1.0 2.4876e-01 1.0 8.58e+07 1.0 0.0e+00 0.0e+00 0.0e+00  4  1  0  0  0   4  1  0  0  0  6898       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     1716 1.0 2.7935e-02 1.6 0.00e+00 0.0 6.5e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       1716 1.0 1.0960e-01 2.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.6440e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.2204e-03 3.0 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      1716 1.0 2.4516e-02 1.7 0.00e+00 0.0 6.5e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        1716 1.0 1.0738e-01 2.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              1716 1.0 5.7884e-03 7.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            1716 1.0 9.9488e-04 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 4.5290e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 6.1375e+00 1.0 7.10e+09 1.0 6.5e+04 8.0e+03 6.9e+03 94100100100100  94100100100100 23147       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 9.9900e-07 2.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply             1716 1.0 2.5369e-01 1.0 8.58e+07 1.0 0.0e+00 0.0e+00 2.0e+00  4  1  0  0  0   4  1  0  0  0  6764       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      8013484     0.
         Vec Scatter     1              1          816     0.
              Vector    70             70     27328832     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         2664     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.04e-08
Average time for MPI_Barrier(): 1.23418e-05
Average time for zero size MPI_Send(): 6.37805e-06
#PETSc Option Table entries:
-ksp_type fcg
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type jacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.000614161 iterations 692
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:07:26 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.967e+00     1.000   3.967e+00
Objects:              7.900e+01     1.000   7.900e+01
Flop:                 3.175e+09     1.000   3.175e+09  6.351e+10
Flop/sec:             8.004e+08     1.000   8.004e+08  1.601e+10
MPI Messages:         1.390e+03     2.000   1.320e+03  2.641e+04
MPI Message Lengths:  1.110e+07     2.000   7.983e+03  2.108e+08
MPI Reductions:       2.787e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.9673e+00 100.0%  6.3505e+10 100.0%  2.641e+04 100.0%  7.983e+03      100.0%  2.780e+03  99.7%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 4.7603e-0367.4 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 4.6176e-0383.1 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult              693 1.0 7.1797e-01 1.0 3.12e+08 1.0 2.6e+04 8.0e+03 0.0e+00 18 10100100  0  18 10100100  0  8679       0      0 0.00e+00    0 0.00e+00  0
MatSOR               693 1.0 1.0666e+00 1.0 3.44e+08 1.0 0.0e+00 0.0e+00 0.0e+00 27 11  0  0  0  27 11  0  0  0  6444       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 4.6711e-0342.1 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7320e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecTDot             1384 1.0 1.5531e-01 1.1 1.38e+08 1.0 0.0e+00 0.0e+00 1.4e+03  4  4  0  0 50   4  4  0  0 50 17822       0      0 0.00e+00    0 0.00e+00  0
VecMTDot             691 1.0 6.9643e-01 1.0 1.07e+09 1.0 0.0e+00 0.0e+00 6.9e+02 18 34  0  0 25  18 34  0  0 25 30716       0      0 0.00e+00    0 0.00e+00  0
VecNorm              694 1.0 9.5728e-02 1.3 6.94e+07 1.0 0.0e+00 0.0e+00 6.9e+02  2  2  0  0 25   2  2  0  0 25 14499       0      0 0.00e+00    0 0.00e+00  0
VecScale             691 1.0 3.2785e-02 1.0 3.46e+07 1.0 0.0e+00 0.0e+00 0.0e+00  1  1  0  0  0   1  1  0  0  0 21077       0      0 0.00e+00    0 0.00e+00  0
VecCopy              693 1.0 4.3998e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 3 1.0 2.1969e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             1385 1.0 9.3113e-02 1.1 1.38e+08 1.0 0.0e+00 0.0e+00 0.0e+00  2  4  0  0  0   2  4  0  0  0 29749       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY             691 1.0 7.1199e-01 1.0 1.07e+09 1.0 0.0e+00 0.0e+00 0.0e+00 18 34  0  0  0  18 34  0  0  0 30046       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin      693 1.0 1.2489e-02 1.4 0.00e+00 0.0 2.6e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd        693 1.0 2.3472e-02 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.4960e-06 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.1743e-03 3.1 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin       693 1.0 1.1079e-02 1.5 0.00e+00 0.0 2.6e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd         693 1.0 2.2481e-02 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack               693 1.0 2.5108e-0310.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack             693 1.0 4.0785e-04 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 4.5184e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 3.5906e+00 1.0 3.17e+09 1.0 2.6e+04 8.0e+03 2.8e+03 90100100100 99  90100100100100 17683       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 6.3600e-07 3.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply              693 1.0 1.0681e+00 1.0 3.44e+08 1.0 0.0e+00 0.0e+00 0.0e+00 27 11  0  0  0  27 11  0  0  0  6434       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    69             69     26927104     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         2664     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.86e-08
Average time for MPI_Barrier(): 1.03594e-05
Average time for zero size MPI_Send(): 6.4912e-06
#PETSc Option Table entries:
-ksp_type fcg
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type sor
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.000603072 iterations 383
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:07:32 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.787e+00     1.000   3.787e+00
Objects:              1.020e+02     1.000   1.020e+02
Flop:                 2.281e+09     1.001   2.281e+09  4.562e+10
Flop/sec:             6.024e+08     1.001   6.023e+08  1.205e+10
MPI Messages:         1.560e+03     2.000   1.482e+03  2.964e+04
MPI Message Lengths:  1.246e+07     2.000   7.985e+03  2.367e+08
MPI Reductions:       1.574e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.7868e+00 100.0%  4.5618e+10 100.0%  2.964e+04 100.0%  7.985e+03      100.0%  1.567e+03  99.6%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 5.4639e-0344.0 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 4.4525e-0380.6 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult              778 1.0 8.3711e-01 1.0 3.50e+08 1.0 3.0e+04 8.0e+03 0.0e+00 22 15100100  0  22 15100100  0  8357       0      0 0.00e+00    0 0.00e+00  0
MatSOR               779 1.0 1.2322e+00 1.0 3.86e+08 1.0 0.0e+00 0.0e+00 0.0e+00 32 17  0  0  0  32 17  0  0  0  6270       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 4.5037e-0343.1 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7617e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecMDot               10 1.0 4.1724e-03 1.2 5.50e+06 1.0 0.0e+00 0.0e+00 1.0e+01  0  0  0  0  1   0  0  0  0  1 26363       0      0 0.00e+00    0 0.00e+00  0
VecTDot              766 1.0 9.3513e-02 1.2 7.66e+07 1.0 0.0e+00 0.0e+00 7.7e+02  2  3  0  0 49   2  3  0  0 49 16383       0      0 0.00e+00    0 0.00e+00  0
VecMTDot             382 1.0 3.9884e-01 1.0 5.83e+08 1.0 0.0e+00 0.0e+00 3.8e+02 11 26  0  0 24  11 26  0  0 24 29250       0      0 0.00e+00    0 0.00e+00  0
VecNorm              396 1.0 9.0562e-02 1.8 3.96e+07 1.0 0.0e+00 0.0e+00 4.0e+02  2  2  0  0 25   2  2  0  0 25  8745       0      0 0.00e+00    0 0.00e+00  0
VecScale             393 1.0 2.4239e-02 1.0 1.96e+07 1.0 0.0e+00 0.0e+00 0.0e+00  1  1  0  0  0   1  1  0  0  0 16213       0      0 0.00e+00    0 0.00e+00  0
VecCopy             1153 1.0 6.0191e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet               773 1.0 1.7557e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY              768 1.0 5.3685e-02 1.1 7.68e+07 1.0 0.0e+00 0.0e+00 0.0e+00  1  3  0  0  0   1  3  0  0  0 28611       0      0 0.00e+00    0 0.00e+00  0
VecAYPX              768 1.0 1.0796e-01 1.0 5.76e+07 1.0 0.0e+00 0.0e+00 0.0e+00  3  3  0  0  0   3  3  0  0  0 10671       0      0 0.00e+00    0 0.00e+00  0
VecAXPBYCZ           384 1.0 6.7918e-02 1.0 9.60e+07 1.0 0.0e+00 0.0e+00 0.0e+00  2  4  0  0  0   2  4  0  0  0 28269       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY             393 1.0 3.8188e-01 1.0 5.90e+08 1.0 0.0e+00 0.0e+00 0.0e+00 10 26  0  0  0  10 26  0  0  0 30889       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin      778 1.0 1.4541e-02 1.5 0.00e+00 0.0 3.0e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd        778 1.0 6.0121e-02 3.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecNormalize          11 1.0 7.9516e-03 1.1 1.65e+06 1.0 0.0e+00 0.0e+00 1.1e+01  0  0  0  0  1   0  0  0  0  1  4150       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.6370e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.2792e-03 3.3 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin       778 1.0 1.2995e-02 1.5 0.00e+00 0.0 3.0e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd         778 1.0 5.9096e-02 3.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack               778 1.0 2.7087e-0310.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack             778 1.0 5.0023e-04 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.5045e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  4  1  1  1  1   4  1  1  1  1  3157       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 3.4081e+00 1.0 2.28e+09 1.0 3.0e+04 8.0e+03 1.6e+03 90100100100 99  90100100100 99 13381       0      0 0.00e+00    0 0.00e+00  0
KSPGMRESOrthog        10 1.0 1.2794e-02 1.1 1.10e+07 1.0 0.0e+00 0.0e+00 1.0e+01  0  0  0  0  1   0  0  0  0  1 17196       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 1.4624e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  4  1  1  1  1   4  1  1  1  1  3248       0      0 0.00e+00    0 0.00e+00  0
PCApply              395 1.0 1.8647e+00 1.0 7.13e+08 1.0 1.5e+04 8.0e+03 0.0e+00 49 31 49 49  0  49 31 49 49  0  7642       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    88             88     34559936     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     3              3        34680     0.
      Preconditioner     3              3         2936     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.79e-08
Average time for MPI_Barrier(): 1.26632e-05
Average time for zero size MPI_Send(): 6.2869e-06
#PETSc Option Table entries:
-ksp_type fcg
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 8.47251e-05 iterations 1671
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:07:48 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           1.400e+01     1.000   1.400e+01
Objects:              4.000e+01     1.000   4.000e+01
Flop:                 7.332e+09     1.001   7.331e+09  1.466e+11
Flop/sec:             5.236e+08     1.001   5.235e+08  1.047e+10
MPI Messages:         1.003e+04     2.000   9.530e+03  1.906e+05
MPI Message Lengths:  8.023e+07     2.000   7.998e+03  1.524e+09
MPI Reductions:       8.375e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 1.4004e+01 100.0%  1.4662e+11 100.0%  1.906e+05 100.0%  7.998e+03      100.0%  8.368e+03  99.9%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 5.2111e-0314.0 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 5.1755e-03108.6 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             5014 1.0 5.1133e+00 1.0 2.26e+09 1.0 1.9e+05 8.0e+03 0.0e+00 36 31100100  0  36 31100100  0  8817       0      0 0.00e+00    0 0.00e+00  0
MatSolve            5014 1.0 5.4980e+00 1.0 2.24e+09 1.0 0.0e+00 0.0e+00 0.0e+00 39 30  0  0  0  39 30  0  0  0  8131       0      0 0.00e+00    0 0.00e+00  0
MatLUFactorNum         1 1.0 7.5275e-03 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1442       0      0 0.00e+00    0 0.00e+00  0
MatILUFactorSym        1 1.0 3.4135e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 5.2250e-0352.6 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7065e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetRowIJ            1 1.0 3.0940e-06 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetOrdering         1 1.0 6.9321e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot              5013 1.0 5.0078e-01 1.1 5.01e+08 1.0 0.0e+00 0.0e+00 5.0e+03  3  7  0  0 60   3  7  0  0 60 20020       0      0 0.00e+00    0 0.00e+00  0
VecNorm             3344 1.0 3.8715e-01 1.4 3.34e+08 1.0 0.0e+00 0.0e+00 3.3e+03  2  5  0  0 40   2  5  0  0 40 17275       0      0 0.00e+00    0 0.00e+00  0
VecScale            6685 1.0 1.3993e-01 1.1 3.34e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1  5  0  0  0   1  5  0  0  0 47774       0      0 0.00e+00    0 0.00e+00  0
VecCopy            15043 1.0 7.1055e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  5  0  0  0  0   5  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet              5022 1.0 3.0672e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY            15037 1.0 8.5791e-01 1.1 1.50e+09 1.0 0.0e+00 0.0e+00 0.0e+00  6 21  0  0  0   6 21  0  0  0 35055       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             1671 1.0 2.3285e-01 1.1 1.67e+08 1.0 0.0e+00 0.0e+00 0.0e+00  2  2  0  0  0   2  2  0  0  0 14352       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     5014 1.0 6.9052e-02 1.4 0.00e+00 0.0 1.9e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       5014 1.0 1.8906e-01 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.9250e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 2.6222e-0313.2 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      5014 1.0 6.1478e-02 1.5 0.00e+00 0.0 1.9e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        5014 1.0 1.8370e-01 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              5014 1.0 1.7405e-02 7.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            5014 1.0 2.6844e-03 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 3.5185e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 1.3623e+01 1.0 7.33e+09 1.0 1.9e+05 8.0e+03 8.4e+03 97100100100100  97100100100100 10762       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                2 1.0 2.6751e-02 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   406       0      0 0.00e+00    0 0.00e+00  0
PCSetUpOnBlocks        1 1.0 1.1697e-02 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   928       0      0 0.00e+00    0 0.00e+00  0
PCApply             5014 1.0 5.8444e+00 1.0 2.24e+09 1.0 0.0e+00 0.0e+00 0.0e+00 41 30  0  0  0  41 30  0  0  0  7649       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              4     11791508     0.
         Vec Scatter     1              1          816     0.
              Vector    24             24      8049088     0.
           Index Set     5              5       608512     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     2              2         2832     0.
      Preconditioner     2              2         1944     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.04e-08
Average time for MPI_Barrier(): 1.50668e-05
Average time for zero size MPI_Send(): 6.37e-06
#PETSc Option Table entries:
-ksp_type tcqmr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type bjacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 2.62209e-05 iterations 6622
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:08:24 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.410e+01     1.000   3.410e+01
Objects:              3.300e+01     1.000   3.300e+01
Flop:                 2.119e+10     1.002   2.118e+10  4.237e+11
Flop/sec:             6.213e+08     1.002   6.212e+08  1.242e+10
MPI Messages:         3.974e+04     2.000   3.775e+04  7.550e+05
MPI Message Lengths:  3.179e+08     2.000   7.999e+03  6.040e+09
MPI Reductions:       3.313e+04     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.4102e+01 100.0%  4.2366e+11 100.0%  7.550e+05 100.0%  7.999e+03      100.0%  3.312e+04 100.0%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 7.6550e-0383.9 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 5.2559e-03119.3 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult            19867 1.0 2.0238e+01 1.0 8.94e+09 1.0 7.5e+05 8.0e+03 0.0e+00 59 42100100  0  59 42100100  0  8827       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 5.3059e-0355.7 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.8035e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot             19866 1.0 1.9711e+00 1.1 1.99e+09 1.0 0.0e+00 0.0e+00 2.0e+04  5  9  0  0 60   5  9  0  0 60 20158       0      0 0.00e+00    0 0.00e+00  0
VecNorm            13246 1.0 1.5430e+00 1.4 1.32e+09 1.0 0.0e+00 0.0e+00 1.3e+04  4  6  0  0 40   4  6  0  0 40 17170       0      0 0.00e+00    0 0.00e+00  0
VecScale           26489 1.0 5.0545e-01 1.0 1.32e+09 1.0 0.0e+00 0.0e+00 0.0e+00  1  6  0  0  0   1  6  0  0  0 52407       0      0 0.00e+00    0 0.00e+00  0
VecCopy            59602 1.0 2.8998e+00 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  8  0  0  0  0   8  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 8 1.0 5.2038e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY            59596 1.0 3.5953e+00 1.1 5.96e+09 1.0 0.0e+00 0.0e+00 0.0e+00 10 28  0  0  0  10 28  0  0  0 33152       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             6622 1.0 8.8553e-01 1.0 6.62e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3  3  0  0  0   3  3  0  0  0 14956       0      0 0.00e+00    0 0.00e+00  0
VecPointwiseMult   19867 1.0 2.8959e+00 1.0 9.93e+08 1.0 0.0e+00 0.0e+00 0.0e+00  8  5  0  0  0   8  5  0  0  0  6860       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin    19867 1.0 2.6744e-01 1.5 0.00e+00 0.0 7.5e+05 8.0e+03 0.0e+00  1  0100100  0   1  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd      19867 1.0 8.3168e-01 2.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.5290e-06 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 2.5137e-0311.4 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin     19867 1.0 2.3998e-01 1.7 0.00e+00 0.0 7.5e+05 8.0e+03 0.0e+00  1  0100100  0   1  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd       19867 1.0 8.0989e-01 2.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack             19867 1.0 7.0225e-02 9.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack           19867 1.0 8.4573e-03 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 3.4511e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 3.3721e+01 1.0 2.12e+10 1.0 7.5e+05 8.0e+03 3.3e+04 99100100100100  99100100100100 12563       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 7.6300e-07 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply            19867 1.0 2.9175e+00 1.0 9.93e+08 1.0 0.0e+00 0.0e+00 2.0e+00  8  5  0  0  0   8  5  0  0  0  6810       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      8013484     0.
         Vec Scatter     1              1          816     0.
              Vector    23             23      8447616     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1416     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.87e-08
Average time for MPI_Barrier(): 1.05882e-05
Average time for zero size MPI_Send(): 6.2591e-06
#PETSc Option Table entries:
-ksp_type tcqmr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type jacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.000177372 iterations 1033
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:08:36 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           1.032e+01     1.000   1.032e+01
Objects:              3.200e+01     1.000   3.200e+01
Flop:                 4.688e+09     1.001   4.687e+09  9.374e+10
Flop/sec:             4.544e+08     1.001   4.543e+08  9.086e+09
MPI Messages:         6.204e+03     2.000   5.894e+03  1.179e+05
MPI Message Lengths:  4.961e+07     2.000   7.996e+03  9.426e+08
MPI Reductions:       5.185e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 1.0317e+01 100.0%  9.3739e+10 100.0%  1.179e+05 100.0%  7.996e+03      100.0%  5.178e+03  99.9%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 5.5433e-03 9.7 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 5.5133e-0395.2 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             3100 1.0 3.2009e+00 1.0 1.39e+09 1.0 1.2e+05 8.0e+03 0.0e+00 31 30100100  0  31 30100100  0  8708       0      0 0.00e+00    0 0.00e+00  0
MatSOR              3100 1.0 4.9821e+00 1.1 1.54e+09 1.0 0.0e+00 0.0e+00 0.0e+00 45 33  0  0  0  45 33  0  0  0  6170       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 5.5655e-0349.6 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7372e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot              3099 1.0 4.5170e-01 1.5 3.10e+08 1.0 0.0e+00 0.0e+00 3.1e+03  4  7  0  0 60   4  7  0  0 60 13721       0      0 0.00e+00    0 0.00e+00  0
VecNorm             2068 1.0 4.6328e-01 2.6 2.07e+08 1.0 0.0e+00 0.0e+00 2.1e+03  4  4  0  0 40   4  4  0  0 40  8928       0      0 0.00e+00    0 0.00e+00  0
VecScale            4133 1.0 8.9993e-02 1.0 2.07e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1  4  0  0  0   1  4  0  0  0 45926       0      0 0.00e+00    0 0.00e+00  0
VecCopy             9301 1.0 5.0276e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  5  0  0  0  0   5  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 8 1.0 5.3365e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             9295 1.0 5.6672e-01 1.1 9.30e+08 1.0 0.0e+00 0.0e+00 0.0e+00  5 20  0  0  0   5 20  0  0  0 32803       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             1033 1.0 1.4926e-01 1.1 1.03e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1  2  0  0  0   1  2  0  0  0 13842       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     3100 1.0 5.5803e-02 1.5 0.00e+00 0.0 1.2e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       3100 1.0 1.2041e-01 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.5410e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.6801e-03 4.2 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      3100 1.0 5.0521e-02 1.6 0.00e+00 0.0 1.2e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        3100 1.0 1.1678e-01 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              3100 1.0 1.4573e-0210.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            3100 1.0 1.5057e-03 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 3.4848e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 9.9414e+00 1.0 4.69e+09 1.0 1.2e+05 8.0e+03 5.2e+03 96100100100100  96100100100100  9428       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 5.9300e-07 4.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply             3100 1.0 4.9883e+00 1.1 1.54e+09 1.0 0.0e+00 0.0e+00 0.0e+00 45 33  0  0  0  45 33  0  0  0  6163       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    22             22      8045888     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1416     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.15e-08
Average time for MPI_Barrier(): 1.02214e-05
Average time for zero size MPI_Send(): 6.2013e-06
#PETSc Option Table entries:
-ksp_type tcqmr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type sor
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.000147819 iterations 474
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:08:48 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           9.541e+00     1.000   9.541e+00
Objects:              5.500e+01     1.000   5.500e+01
Flop:                 4.090e+09     1.001   4.089e+09  8.179e+10
Flop/sec:             4.287e+08     1.001   4.286e+08  8.572e+09
MPI Messages:         5.716e+03     2.000   5.430e+03  1.086e+05
MPI Message Lengths:  4.570e+07     2.000   7.996e+03  8.684e+08
MPI Reductions:       2.413e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 9.5409e+00 100.0%  8.1788e+10 100.0%  1.086e+05 100.0%  7.996e+03      100.0%  2.406e+03  99.7%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 5.1203e-0347.1 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 4.0204e-0375.9 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             2856 1.0 2.9750e+00 1.0 1.28e+09 1.0 1.1e+05 8.0e+03 0.0e+00 31 31100100  0  31 31100100  0  8632       0      0 0.00e+00    0 0.00e+00  0
MatSOR              2857 1.0 4.3078e+00 1.0 1.42e+09 1.0 0.0e+00 0.0e+00 0.0e+00 45 35  0  0  0  45 35  0  0  0  6577       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 4.0742e-0337.8 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7560e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot              1422 1.0 1.7611e-01 1.2 1.42e+08 1.0 0.0e+00 0.0e+00 1.4e+03  2  3  0  0 59   2  3  0  0 59 16149       0      0 0.00e+00    0 0.00e+00  0
VecMDot               10 1.0 4.0262e-03 1.1 5.50e+06 1.0 0.0e+00 0.0e+00 1.0e+01  0  0  0  0  0   0  0  0  0  0 27320       0      0 0.00e+00    0 0.00e+00  0
VecNorm              961 1.0 1.6217e-01 1.5 9.61e+07 1.0 0.0e+00 0.0e+00 9.6e+02  1  2  0  0 40   1  2  0  0 40 11852       0      0 0.00e+00    0 0.00e+00  0
VecScale            1908 1.0 4.2272e-02 1.0 9.54e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  2  0  0  0   0  2  0  0  0 45137       0      0 0.00e+00    0 0.00e+00  0
VecCopy             7117 1.0 3.9656e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  4  0  0  0  0   4  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet              2856 1.0 1.1189e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             4265 1.0 2.4917e-01 1.0 4.26e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3 10  0  0  0   3 10  0  0  0 34234       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             3320 1.0 4.5863e-01 1.0 2.61e+08 1.0 0.0e+00 0.0e+00 0.0e+00  5  6  0  0  0   5  6  0  0  0 11375       0      0 0.00e+00    0 0.00e+00  0
VecAXPBYCZ          1423 1.0 2.4610e-01 1.0 3.56e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3  9  0  0  0   3  9  0  0  0 28912       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY              11 1.0 5.0296e-03 1.0 6.50e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 25847       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     2856 1.0 5.6306e-02 1.7 0.00e+00 0.0 1.1e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       2856 1.0 1.5304e-01 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecNormalize          11 1.0 8.0278e-03 1.0 1.65e+06 1.0 0.0e+00 0.0e+00 1.1e+01  0  0  0  0  0   0  0  0  0  0  4111       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.6650e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.2275e-03 3.1 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      2856 1.0 5.0188e-02 1.7 0.00e+00 0.0 1.1e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        2856 1.0 1.4943e-01 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              2856 1.0 1.1391e-0211.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            2856 1.0 1.7387e-03 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.4943e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  2  1  0  0  1   2  1  0  0  1  3179       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 9.1522e+00 1.0 4.09e+09 1.0 1.1e+05 8.0e+03 2.4e+03 96100100100 99  96100100100100  8935       0      0 0.00e+00    0 0.00e+00  0
KSPGMRESOrthog        10 1.0 1.2491e-02 1.0 1.10e+07 1.0 0.0e+00 0.0e+00 1.0e+01  0  0  0  0  0   0  0  0  0  0 17613       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 1.4626e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  2  1  0  0  1   2  1  0  0  1  3248       0      0 0.00e+00    0 0.00e+00  0
PCApply             1434 1.0 6.6924e+00 1.0 2.63e+09 1.0 5.4e+04 8.0e+03 0.0e+00 70 64 50 50  0  70 64 50 50  0  7846       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    41             41     15678720     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     3              3        33432     0.
      Preconditioner     3              3         2936     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.86e-08
Average time for MPI_Barrier(): 9.4858e-06
Average time for zero size MPI_Send(): 6.22075e-06
#PETSc Option Table entries:
-ksp_type tcqmr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 7.08235e-05 iterations 490
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:08:53 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.042e+00     1.000   3.042e+00
Objects:              3.000e+01     1.000   3.000e+01
Flop:                 1.345e+09     1.001   1.345e+09  2.689e+10
Flop/sec:             4.420e+08     1.001   4.419e+08  8.839e+09
MPI Messages:         1.966e+03     2.000   1.868e+03  3.735e+04
MPI Message Lengths:  1.570e+07     2.000   7.988e+03  2.984e+08
MPI Reductions:       1.491e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.0423e+00 100.0%  2.6891e+10 100.0%  3.735e+04 100.0%  7.988e+03      100.0%  1.484e+03  99.5%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 4.0726e-0337.9 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 3.9893e-0374.3 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult              981 1.0 1.0353e+00 1.0 4.41e+08 1.0 3.7e+04 8.0e+03 0.0e+00 33 33100100  0  33 33100100  0  8520       0      0 0.00e+00    0 0.00e+00  0
MatSolve             981 1.0 1.0951e+00 1.0 4.37e+08 1.0 0.0e+00 0.0e+00 0.0e+00 36 33  0  0  0  36 33  0  0  0  7987       0      0 0.00e+00    0 0.00e+00  0
MatLUFactorNum         1 1.0 7.5302e-03 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1441       0      0 0.00e+00    0 0.00e+00  0
MatILUFactorSym        1 1.0 3.3343e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 4.0403e-0339.3 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7821e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetRowIJ            1 1.0 2.6310e-06 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetOrdering         1 1.0 6.9208e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot               981 1.0 1.6592e-01 1.5 9.81e+07 1.0 0.0e+00 0.0e+00 9.8e+02  5  7  0  0 66   5  7  0  0 66 11825       0      0 0.00e+00    0 0.00e+00  0
VecNorm              492 1.0 4.6928e-02 1.2 4.92e+07 1.0 0.0e+00 0.0e+00 4.9e+02  1  4  0  0 33   1  4  0  0 33 20968       0      0 0.00e+00    0 0.00e+00  0
VecCopy                4 1.0 2.7888e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet               984 1.0 1.7558e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             1470 1.0 8.8261e-02 1.1 1.47e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3 11  0  0  0   3 11  0  0  0 33310       0      0 0.00e+00    0 0.00e+00  0
VecWAXPY            1958 1.0 2.5350e-01 1.0 1.71e+08 1.0 0.0e+00 0.0e+00 0.0e+00  8 13  0  0  0   8 13  0  0  0 13515       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin      981 1.0 1.3047e-02 1.3 0.00e+00 0.0 3.7e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd        981 1.0 6.7669e-02 4.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.5330e-06 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 2.6881e-0313.0 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin       981 1.0 1.1719e-02 1.4 0.00e+00 0.0 3.7e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd         981 1.0 6.6582e-02 4.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack               981 1.0 2.4345e-03 7.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack             981 1.0 4.7327e-04 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.4170e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.6592e+00 1.0 1.34e+09 1.0 3.7e+04 8.0e+03 1.5e+03 87100100100 99  87100100100 99 10107       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                2 1.0 2.4957e-02 1.2 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0   435       0      0 0.00e+00    0 0.00e+00  0
PCSetUpOnBlocks        1 1.0 1.1613e-02 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   935       0      0 0.00e+00    0 0.00e+00  0
PCApply              981 1.0 1.1181e+00 1.0 4.37e+08 1.0 0.0e+00 0.0e+00 0.0e+00 36 33  0  0  0  36 33  0  0  0  7823       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              4     11791508     0.
         Vec Scatter     1              1          816     0.
              Vector    14             14      4031808     0.
           Index Set     5              5       608512     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     2              2         2832     0.
      Preconditioner     2              2         1944     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.04e-08
Average time for MPI_Barrier(): 1.24682e-05
Average time for zero size MPI_Send(): 6.2527e-06
#PETSc Option Table entries:
-ksp_type cgs
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type bjacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 8.26951e-08 iterations 1553
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:09:00 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           5.401e+00     1.000   5.401e+00
Objects:              2.300e+01     1.000   2.300e+01
Flop:                 3.028e+09     1.002   3.028e+09  6.055e+10
Flop/sec:             5.607e+08     1.002   5.605e+08  1.121e+10
MPI Messages:         6.218e+03     2.000   5.907e+03  1.181e+05
MPI Message Lengths:  4.972e+07     2.000   7.996e+03  9.447e+08
MPI Reductions:       4.682e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 5.4014e+00 100.0%  6.0554e+10 100.0%  1.181e+05 100.0%  7.996e+03      100.0%  4.675e+03  99.9%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 3.9328e-0342.5 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 3.6189e-0372.1 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             3107 1.0 3.1364e+00 1.0 1.40e+09 1.0 1.2e+05 8.0e+03 0.0e+00 58 46100100  0  58 46100100  0  8908       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 3.6695e-0335.4 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6885e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot              3107 1.0 3.4367e-01 1.2 3.11e+08 1.0 0.0e+00 0.0e+00 3.1e+03  6 10  0  0 66   6 10  0  0 66 18081       0      0 0.00e+00    0 0.00e+00  0
VecNorm             1555 1.0 1.2109e-01 1.0 1.56e+08 1.0 0.0e+00 0.0e+00 1.6e+03  2  5  0  0 33   2  5  0  0 33 25684       0      0 0.00e+00    0 0.00e+00  0
VecCopy                4 1.0 2.3100e-04 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 3 1.0 2.1033e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             4659 1.0 2.6717e-01 1.1 4.66e+08 1.0 0.0e+00 0.0e+00 0.0e+00  5 15  0  0  0   5 15  0  0  0 34876       0      0 0.00e+00    0 0.00e+00  0
VecWAXPY            6210 1.0 7.9685e-01 1.0 5.43e+08 1.0 0.0e+00 0.0e+00 0.0e+00 15 18  0  0  0  15 18  0  0  0 13637       0      0 0.00e+00    0 0.00e+00  0
VecPointwiseMult    3107 1.0 4.2226e-01 1.0 1.55e+08 1.0 0.0e+00 0.0e+00 0.0e+00  8  5  0  0  0   8  5  0  0  0  7358       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     3107 1.0 3.4223e-02 1.5 0.00e+00 0.0 1.2e+05 8.0e+03 0.0e+00  1  0100100  0   1  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       3107 1.0 1.0904e-01 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.6250e-06 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.4606e-03 3.8 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      3107 1.0 3.0581e-02 1.5 0.00e+00 0.0 1.2e+05 8.0e+03 0.0e+00  1  0100100  0   1  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        3107 1.0 1.0599e-01 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              3107 1.0 7.6237e-03 5.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            3107 1.0 1.5269e-03 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 1.4665e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 5.0207e+00 1.0 3.03e+09 1.0 1.2e+05 8.0e+03 4.7e+03 93100100100100  93100100100100 12058       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 5.6600e-07 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply             3107 1.0 4.2731e-01 1.0 1.55e+08 1.0 0.0e+00 0.0e+00 2.0e+00  8  5  0  0  0   8  5  0  0  0  7271       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      8013484     0.
         Vec Scatter     1              1          816     0.
              Vector    13             13      4430336     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1416     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.96e-08
Average time for MPI_Barrier(): 1.45958e-05
Average time for zero size MPI_Send(): 7.2372e-06
#PETSc Option Table entries:
-ksp_type cgs
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type jacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 8.535e-05 iterations 546
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:09:06 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.687e+00     1.000   3.687e+00
Objects:              2.200e+01     1.000   2.200e+01
Flop:                 1.552e+09     1.001   1.552e+09  3.104e+10
Flop/sec:             4.210e+08     1.001   4.209e+08  8.419e+09
MPI Messages:         2.190e+03     2.000   2.080e+03  4.161e+04
MPI Message Lengths:  1.750e+07     2.000   7.989e+03  3.324e+08
MPI Reductions:       1.659e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.6873e+00 100.0%  3.1043e+10 100.0%  4.161e+04 100.0%  7.989e+03      100.0%  1.652e+03  99.6%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 3.8756e-0345.8 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 3.7785e-0366.8 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             1093 1.0 1.1056e+00 1.0 4.92e+08 1.0 4.2e+04 8.0e+03 0.0e+00 30 32100100  0  30 32100100  0  8890       0      0 0.00e+00    0 0.00e+00  0
MatSOR              1093 1.0 1.6192e+00 1.0 5.42e+08 1.0 0.0e+00 0.0e+00 0.0e+00 44 35  0  0  0  44 35  0  0  0  6694       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 3.8280e-0333.9 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7109e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot              1093 1.0 1.5508e-01 1.1 1.09e+08 1.0 0.0e+00 0.0e+00 1.1e+03  4  7  0  0 66   4  7  0  0 66 14096       0      0 0.00e+00    0 0.00e+00  0
VecNorm              548 1.0 5.1700e-02 1.1 5.48e+07 1.0 0.0e+00 0.0e+00 5.5e+02  1  4  0  0 33   1  4  0  0 33 21199       0      0 0.00e+00    0 0.00e+00  0
VecCopy                4 1.0 2.5649e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 3 1.0 2.2101e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             1638 1.0 1.1249e-01 1.1 1.64e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3 11  0  0  0   3 11  0  0  0 29122       0      0 0.00e+00    0 0.00e+00  0
VecWAXPY            2182 1.0 3.0367e-01 1.0 1.91e+08 1.0 0.0e+00 0.0e+00 0.0e+00  8 12  0  0  0   8 12  0  0  0 12573       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     1093 1.0 1.8219e-02 1.4 0.00e+00 0.0 4.2e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       1093 1.0 3.9426e-02 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.4430e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.8286e-03 4.6 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      1093 1.0 1.6661e-02 1.5 0.00e+00 0.0 4.2e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        1093 1.0 3.8141e-02 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              1093 1.0 3.9528e-03 9.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            1093 1.0 6.8559e-04 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 1.4548e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 3.3157e+00 1.0 1.55e+09 1.0 4.1e+04 8.0e+03 1.6e+03 90100100100 99  90100100100 99  9359       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 3.1400e-07 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply             1093 1.0 1.6212e+00 1.0 5.42e+08 1.0 0.0e+00 0.0e+00 0.0e+00 44 35  0  0  0  44 35  0  0  0  6686       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    12             12      4028608     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1416     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.09e-08
Average time for MPI_Barrier(): 1.34696e-05
Average time for zero size MPI_Send(): 6.2603e-06
#PETSc Option Table entries:
-ksp_type cgs
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type sor
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.000193207 iterations 288
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:09:12 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           4.227e+00     1.000   4.227e+00
Objects:              4.500e+01     1.000   4.500e+01
Flop:                 1.619e+09     1.001   1.619e+09  3.238e+10
Flop/sec:             3.831e+08     1.001   3.831e+08  7.661e+09
MPI Messages:         2.332e+03     2.000   2.215e+03  4.431e+04
MPI Message Lengths:  1.863e+07     2.000   7.990e+03  3.540e+08
MPI Reductions:       9.080e+02     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 4.2272e+00 100.0%  3.2385e+10 100.0%  4.431e+04 100.0%  7.990e+03      100.0%  9.010e+02  99.2%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 2.6504e-0338.9 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 2.4729e-0349.9 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             1164 1.0 1.2637e+00 1.1 5.24e+08 1.0 4.4e+04 8.0e+03 0.0e+00 29 32100100  0  29 32100100  0  8282       0      0 0.00e+00    0 0.00e+00  0
MatSOR              1165 1.0 1.8445e+00 1.1 5.78e+08 1.0 0.0e+00 0.0e+00 0.0e+00 42 36  0  0  0  42 36  0  0  0  6264       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 2.5240e-0324.8 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7700e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  1   1  0  0  0  1     0       0      0 0.00e+00    0 0.00e+00  0
VecDot               577 1.0 1.5525e-01 2.7 5.77e+07 1.0 0.0e+00 0.0e+00 5.8e+02  3  4  0  0 64   3  4  0  0 64  7433       0      0 0.00e+00    0 0.00e+00  0
VecMDot               10 1.0 4.0661e-03 1.2 5.50e+06 1.0 0.0e+00 0.0e+00 1.0e+01  0  0  0  0  1   0  0  0  0  1 27053       0      0 0.00e+00    0 0.00e+00  0
VecNorm              301 1.0 3.4225e-02 1.1 3.01e+07 1.0 0.0e+00 0.0e+00 3.0e+02  1  2  0  0 33   1  2  0  0 33 17589       0      0 0.00e+00    0 0.00e+00  0
VecScale              11 1.0 2.5312e-04 1.0 5.50e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 43458       0      0 0.00e+00    0 0.00e+00  0
VecCopy             1159 1.0 5.3771e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet              1159 1.0 3.4286e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY              865 1.0 6.2948e-02 1.1 8.65e+07 1.0 0.0e+00 0.0e+00 0.0e+00  1  5  0  0  0   1  5  0  0  0 27483       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             1154 1.0 1.6132e-01 1.0 8.66e+07 1.0 0.0e+00 0.0e+00 0.0e+00  4  5  0  0  0   4  5  0  0  0 10730       0      0 0.00e+00    0 0.00e+00  0
VecAXPBYCZ           577 1.0 1.0311e-01 1.1 1.44e+08 1.0 0.0e+00 0.0e+00 0.0e+00  2  9  0  0  0   2  9  0  0  0 27979       0      0 0.00e+00    0 0.00e+00  0
VecWAXPY            1150 1.0 1.7214e-01 1.1 1.01e+08 1.0 0.0e+00 0.0e+00 0.0e+00  4  6  0  0  0   4  6  0  0  0 11688       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY              11 1.0 4.7612e-03 1.0 6.50e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 27304       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     1164 1.0 2.0748e-02 1.5 0.00e+00 0.0 4.4e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       1164 1.0 1.0096e-01 3.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecNormalize          11 1.0 8.1382e-03 1.1 1.65e+06 1.0 0.0e+00 0.0e+00 1.1e+01  0  0  0  0  1   0  0  0  0  1  4055       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.5560e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.3120e-03 3.3 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      1164 1.0 1.8665e-02 1.5 0.00e+00 0.0 4.4e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        1164 1.0 9.9498e-02 3.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              1164 1.0 4.2103e-03 7.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            1164 1.0 6.1918e-04 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.4541e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  3  1  1  1  3   3  1  1  1  3  3266       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 3.8460e+00 1.0 1.62e+09 1.0 4.4e+04 8.0e+03 8.9e+02 91100100100 98  91100100100 99  8417       0      0 0.00e+00    0 0.00e+00  0
KSPGMRESOrthog        10 1.0 1.2242e-02 1.1 1.10e+07 1.0 0.0e+00 0.0e+00 1.0e+01  0  1  0  0  1   0  1  0  0  1 17971       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 1.4428e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  3  1  1  1  3   3  1  1  1  3  3292       0      0 0.00e+00    0 0.00e+00  0
PCApply              588 1.0 2.8021e+00 1.0 1.07e+09 1.0 2.2e+04 8.0e+03 0.0e+00 65 66 49 50  0  65 66 49 50  0  7622       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    31             31     11661440     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     3              3        33432     0.
      Preconditioner     3              3         2936     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.79e-08
Average time for MPI_Barrier(): 1.23754e-05
Average time for zero size MPI_Send(): 6.1395e-06
#PETSc Option Table entries:
-ksp_type cgs
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.00114039 iterations 479
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:09:17 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.022e+00     1.000   3.022e+00
Objects:              2.900e+01     1.000   2.900e+01
Flop:                 1.387e+09     1.001   1.386e+09  2.773e+10
Flop/sec:             4.589e+08     1.001   4.588e+08  9.177e+09
MPI Messages:         1.922e+03     2.000   1.826e+03  3.652e+04
MPI Message Lengths:  1.535e+07     2.000   7.988e+03  2.917e+08
MPI Reductions:       1.936e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.0216e+00 100.0%  2.7729e+10 100.0%  3.652e+04 100.0%  7.988e+03      100.0%  1.929e+03  99.6%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 5.6210e-0380.2 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 5.5179e-03121.2 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult              959 1.0 9.9641e-01 1.0 4.31e+08 1.0 3.6e+04 8.0e+03 0.0e+00 33 31100100  0  33 31100100  0  8654       0      0 0.00e+00    0 0.00e+00  0
MatSolve             959 1.0 1.0814e+00 1.0 4.28e+08 1.0 0.0e+00 0.0e+00 0.0e+00 35 31  0  0  0  35 31  0  0  0  7907       0      0 0.00e+00    0 0.00e+00  0
MatLUFactorNum         1 1.0 7.5188e-03 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1443       0      0 0.00e+00    0 0.00e+00  0
MatILUFactorSym        1 1.0 3.3818e-03 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 5.5726e-0356.2 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7378e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetRowIJ            1 1.0 2.6970e-06 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetOrdering         1 1.0 6.8996e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot               958 1.0 1.2352e-01 1.3 9.58e+07 1.0 0.0e+00 0.0e+00 9.6e+02  4  7  0  0 49   4  7  0  0 50 15512       0      0 0.00e+00    0 0.00e+00  0
VecDotNorm2          479 1.0 1.2197e-01 1.3 9.58e+07 1.0 0.0e+00 0.0e+00 4.8e+02  4  7  0  0 25   4  7  0  0 25 15708       0      0 0.00e+00    0 0.00e+00  0
VecNorm              481 1.0 4.9637e-02 1.3 4.81e+07 1.0 0.0e+00 0.0e+00 4.8e+02  2  3  0  0 25   2  3  0  0 25 19381       0      0 0.00e+00    0 0.00e+00  0
VecCopy                2 1.0 1.5630e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet               964 1.0 2.0951e-02 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY                1 1.0 8.9797e-05 1.1 1.00e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 22272       0      0 0.00e+00    0 0.00e+00  0
VecAXPBYCZ           958 1.0 1.6144e-01 1.1 1.92e+08 1.0 0.0e+00 0.0e+00 0.0e+00  5 14  0  0  0   5 14  0  0  0 23737       0      0 0.00e+00    0 0.00e+00  0
VecWAXPY             958 1.0 1.2613e-01 1.0 9.58e+07 1.0 0.0e+00 0.0e+00 0.0e+00  4  7  0  0  0   4  7  0  0  0 15190       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin      959 1.0 1.3608e-02 1.4 0.00e+00 0.0 3.6e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd        959 1.0 5.3128e-02 2.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.6380e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 2.1490e-03 5.4 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin       959 1.0 1.2322e-02 1.4 0.00e+00 0.0 3.6e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd         959 1.0 5.2181e-02 2.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack               959 1.0 2.4747e-03 5.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack             959 1.0 4.8140e-04 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.2487e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.6416e+00 1.0 1.39e+09 1.0 3.6e+04 8.0e+03 1.9e+03 87100100100 99  87100100100 99 10492       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                2 1.0 1.6728e-02 1.2 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0   649       0      0 0.00e+00    0 0.00e+00  0
PCSetUpOnBlocks        1 1.0 1.1655e-02 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   931       0      0 0.00e+00    0 0.00e+00  0
PCApply              959 1.0 1.1072e+00 1.0 4.28e+08 1.0 0.0e+00 0.0e+00 0.0e+00 36 31  0  0  0  36 31  0  0  0  7723       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              4     11791508     0.
         Vec Scatter     1              1          816     0.
              Vector    13             13      3630080     0.
           Index Set     5              5       608512     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     2              2         2840     0.
      Preconditioner     2              2         1944     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.08e-08
Average time for MPI_Barrier(): 1.0241e-05
Average time for zero size MPI_Send(): 6.57935e-06
#PETSc Option Table entries:
-ksp_type bcgs
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type bjacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.00878969 iterations 1368
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:09:24 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           4.939e+00     1.000   4.939e+00
Objects:              2.200e+01     1.000   2.200e+01
Flop:                 2.873e+09     1.002   2.873e+09  5.745e+10
Flop/sec:             5.817e+08     1.002   5.816e+08  1.163e+10
MPI Messages:         5.478e+03     2.000   5.204e+03  1.041e+05
MPI Message Lengths:  4.380e+07     2.000   7.996e+03  8.322e+08
MPI Reductions:       5.494e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 4.9389e+00 100.0%  5.7450e+10 100.0%  1.041e+05 100.0%  7.996e+03      100.0%  5.487e+03  99.9%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 4.9920e-0357.1 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 4.7095e-0399.5 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             2737 1.0 2.7670e+00 1.0 1.23e+09 1.0 1.0e+05 8.0e+03 0.0e+00 56 43100100  0  56 43100100  0  8894       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 4.7617e-0349.5 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7750e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot              2736 1.0 2.7350e-01 1.1 2.74e+08 1.0 0.0e+00 0.0e+00 2.7e+03  5 10  0  0 50   5 10  0  0 50 20007       0      0 0.00e+00    0 0.00e+00  0
VecDotNorm2         1368 1.0 2.9406e-01 1.2 2.74e+08 1.0 0.0e+00 0.0e+00 1.4e+03  5 10  0  0 25   5 10  0  0 25 18608       0      0 0.00e+00    0 0.00e+00  0
VecNorm             1370 1.0 1.1552e-01 1.1 1.37e+08 1.0 0.0e+00 0.0e+00 1.4e+03  2  5  0  0 25   2  5  0  0 25 23719       0      0 0.00e+00    0 0.00e+00  0
VecCopy                2 1.0 1.1396e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 5 1.0 3.3344e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY                1 1.0 8.6677e-05 1.1 1.00e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 23074       0      0 0.00e+00    0 0.00e+00  0
VecAXPBYCZ          2736 1.0 4.4814e-01 1.0 5.47e+08 1.0 0.0e+00 0.0e+00 0.0e+00  9 19  0  0  0   9 19  0  0  0 24421       0      0 0.00e+00    0 0.00e+00  0
VecWAXPY            2736 1.0 3.5543e-01 1.0 2.74e+08 1.0 0.0e+00 0.0e+00 0.0e+00  7 10  0  0  0   7 10  0  0  0 15396       0      0 0.00e+00    0 0.00e+00  0
VecPointwiseMult    2737 1.0 3.7977e-01 1.0 1.37e+08 1.0 0.0e+00 0.0e+00 0.0e+00  8  5  0  0  0   8  5  0  0  0  7207       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     2737 1.0 3.6297e-02 1.7 0.00e+00 0.0 1.0e+05 8.0e+03 0.0e+00  1  0100100  0   1  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       2737 1.0 1.0192e-01 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.5370e-06 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.2313e-03 3.0 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      2737 1.0 3.1867e-02 1.8 0.00e+00 0.0 1.0e+05 8.0e+03 0.0e+00  1  0100100  0   1  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        2737 1.0 9.9265e-02 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              2737 1.0 6.9902e-03 7.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            2737 1.0 1.4632e-03 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 1.2453e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 4.5576e+00 1.0 2.87e+09 1.0 1.0e+05 8.0e+03 5.5e+03 92100100100100  92100100100100 12602       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 6.9900e-07 2.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply             2737 1.0 3.8415e-01 1.0 1.37e+08 1.0 0.0e+00 0.0e+00 2.0e+00  8  5  0  0  0   8  5  0  0  0  7125       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      8013484     0.
         Vec Scatter     1              1          816     0.
              Vector    12             12      4028608     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1424     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.12e-08
Average time for MPI_Barrier(): 1.04122e-05
Average time for zero size MPI_Send(): 6.455e-06
#PETSc Option Table entries:
-ksp_type bcgs
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type jacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.00987163 iterations 515
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:09:30 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.652e+00     1.000   3.652e+00
Objects:              2.100e+01     1.000   2.100e+01
Flop:                 1.542e+09     1.001   1.542e+09  3.083e+10
Flop/sec:             4.222e+08     1.001   4.222e+08  8.443e+09
MPI Messages:         2.066e+03     2.000   1.963e+03  3.925e+04
MPI Message Lengths:  1.650e+07     2.000   7.988e+03  3.136e+08
MPI Reductions:       2.080e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.6515e+00 100.0%  3.0831e+10 100.0%  3.925e+04 100.0%  7.988e+03      100.0%  2.073e+03  99.7%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 5.9382e-0353.7 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 5.8176e-0395.7 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             1031 1.0 1.0740e+00 1.0 4.64e+08 1.0 3.9e+04 8.0e+03 0.0e+00 29 30100100  0  29 30100100  0  8632       0      0 0.00e+00    0 0.00e+00  0
MatSOR              1031 1.0 1.5658e+00 1.0 5.11e+08 1.0 0.0e+00 0.0e+00 0.0e+00 42 33  0  0  0  42 33  0  0  0  6530       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 5.8670e-0350.9 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7194e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot              1030 1.0 1.5561e-01 1.2 1.03e+08 1.0 0.0e+00 0.0e+00 1.0e+03  4  7  0  0 50   4  7  0  0 50 13238       0      0 0.00e+00    0 0.00e+00  0
VecDotNorm2          515 1.0 1.4807e-01 1.2 1.03e+08 1.0 0.0e+00 0.0e+00 5.2e+02  4  7  0  0 25   4  7  0  0 25 13912       0      0 0.00e+00    0 0.00e+00  0
VecNorm              517 1.0 5.6274e-02 1.1 5.17e+07 1.0 0.0e+00 0.0e+00 5.2e+02  1  3  0  0 25   1  3  0  0 25 18374       0      0 0.00e+00    0 0.00e+00  0
VecCopy                2 1.0 1.2204e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 5 1.0 3.4192e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY                1 1.0 1.0057e-04 1.2 1.00e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 19886       0      0 0.00e+00    0 0.00e+00  0
VecAXPBYCZ          1030 1.0 1.7376e-01 1.0 2.06e+08 1.0 0.0e+00 0.0e+00 0.0e+00  5 13  0  0  0   5 13  0  0  0 23710       0      0 0.00e+00    0 0.00e+00  0
VecWAXPY            1030 1.0 1.5046e-01 1.0 1.03e+08 1.0 0.0e+00 0.0e+00 0.0e+00  4  7  0  0  0   4  7  0  0  0 13692       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     1031 1.0 1.8731e-02 1.5 0.00e+00 0.0 3.9e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       1031 1.0 5.0632e-02 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 1.1703e-05 2.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.2466e-03 3.2 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      1031 1.0 1.6612e-02 1.6 0.00e+00 0.0 3.9e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        1031 1.0 4.9280e-02 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              1031 1.0 4.0405e-03 8.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            1031 1.0 5.3998e-04 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 1.2381e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 3.2714e+00 1.0 1.54e+09 1.0 3.9e+04 8.0e+03 2.1e+03 90100100100 99  90100100100 99  9421       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 3.5500e-07 2.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply             1031 1.0 1.5676e+00 1.0 5.11e+08 1.0 0.0e+00 0.0e+00 0.0e+00 42 33  0  0  0  42 33  0  0  0  6522       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    11             11      3626880     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1424     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.19e-08
Average time for MPI_Barrier(): 1.10574e-05
Average time for zero size MPI_Send(): 6.42435e-06
#PETSc Option Table entries:
-ksp_type bcgs
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type sor
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.000785404 iterations 301
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:09:36 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           4.393e+00     1.000   4.393e+00
Objects:              4.400e+01     1.000   4.400e+01
Flop:                 1.737e+09     1.001   1.737e+09  3.473e+10
Flop/sec:             3.954e+08     1.001   3.953e+08  7.906e+09
MPI Messages:         2.436e+03     2.000   2.314e+03  4.628e+04
MPI Message Lengths:  1.946e+07     2.000   7.990e+03  3.698e+08
MPI Reductions:       1.247e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 4.3927e+00 100.0%  3.4730e+10 100.0%  4.628e+04 100.0%  7.990e+03      100.0%  1.240e+03  99.4%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 4.2555e-0341.4 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 4.1938e-0364.5 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             1216 1.0 1.3092e+00 1.0 5.47e+08 1.0 4.6e+04 8.0e+03 0.0e+00 29 31100100  0  29 31100100  0  8352       0      0 0.00e+00    0 0.00e+00  0
MatSOR              1217 1.0 1.8747e+00 1.0 6.03e+08 1.0 0.0e+00 0.0e+00 0.0e+00 42 35  0  0  0  42 35  0  0  0  6438       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 4.2484e-0335.0 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.8241e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot               602 1.0 9.9793e-02 1.4 6.02e+07 1.0 0.0e+00 0.0e+00 6.0e+02  2  3  0  0 48   2  3  0  0 49 12065       0      0 0.00e+00    0 0.00e+00  0
VecDotNorm2          301 1.0 9.7877e-02 1.4 6.02e+07 1.0 0.0e+00 0.0e+00 3.0e+02  2  3  0  0 24   2  3  0  0 24 12301       0      0 0.00e+00    0 0.00e+00  0
VecMDot               10 1.0 4.1658e-03 1.2 5.50e+06 1.0 0.0e+00 0.0e+00 1.0e+01  0  0  0  0  1   0  0  0  0  1 26406       0      0 0.00e+00    0 0.00e+00  0
VecNorm              314 1.0 3.8873e-02 1.1 3.14e+07 1.0 0.0e+00 0.0e+00 3.1e+02  1  2  0  0 25   1  2  0  0 25 16155       0      0 0.00e+00    0 0.00e+00  0
VecScale              11 1.0 2.6959e-04 1.1 5.50e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 40802       0      0 0.00e+00    0 0.00e+00  0
VecCopy             1209 1.0 5.6833e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet              1213 1.0 4.1299e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY                2 1.0 1.6943e-04 1.2 2.00e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 23608       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             1206 1.0 1.7033e-01 1.0 9.04e+07 1.0 0.0e+00 0.0e+00 0.0e+00  4  5  0  0  0   4  5  0  0  0 10620       0      0 0.00e+00    0 0.00e+00  0
VecAXPBYCZ          1205 1.0 2.1123e-01 1.0 2.71e+08 1.0 0.0e+00 0.0e+00 0.0e+00  5 16  0  0  0   5 16  0  0  0 25674       0      0 0.00e+00    0 0.00e+00  0
VecWAXPY             602 1.0 9.0134e-02 1.0 6.02e+07 1.0 0.0e+00 0.0e+00 0.0e+00  2  3  0  0  0   2  3  0  0  0 13358       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY              11 1.0 5.0618e-03 1.0 6.50e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 25683       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     1216 1.0 2.1661e-02 1.5 0.00e+00 0.0 4.6e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       1216 1.0 9.9388e-02 2.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecNormalize          11 1.0 8.0844e-03 1.1 1.65e+06 1.0 0.0e+00 0.0e+00 1.1e+01  0  0  0  0  1   0  0  0  0  1  4082       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.6780e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.2475e-03 3.0 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      1216 1.0 1.9585e-02 1.5 0.00e+00 0.0 4.6e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        1216 1.0 9.7889e-02 2.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              1216 1.0 4.3429e-03 8.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            1216 1.0 6.2943e-04 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.4816e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  3  1  1  1  2   3  1  1  1  2  3206       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 4.0106e+00 1.0 1.74e+09 1.0 4.6e+04 8.0e+03 1.2e+03 91100100100 98  91100100100 99  8656       0      0 0.00e+00    0 0.00e+00  0
KSPGMRESOrthog        10 1.0 1.2761e-02 1.1 1.10e+07 1.0 0.0e+00 0.0e+00 1.0e+01  0  1  0  0  1   0  1  0  0  1 17240       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 1.4720e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  3  1  1  1  2   3  1  1  1  2  3227       0      0 0.00e+00    0 0.00e+00  0
PCApply              614 1.0 2.8876e+00 1.0 1.12e+09 1.0 2.3e+04 8.0e+03 0.0e+00 65 64 50 50  0  65 64 50 50  0  7728       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    30             30     11259712     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     3              3        33440     0.
      Preconditioner     3              3         2936     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3e-08
Average time for MPI_Barrier(): 1.53604e-05
Average time for zero size MPI_Send(): 6.18965e-06
#PETSc Option Table entries:
-ksp_type bcgs
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 8.37695e-05 iterations 475
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:09:41 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.170e+00     1.000   3.170e+00
Objects:              3.200e+01     1.000   3.200e+01
Flop:                 1.446e+09     1.001   1.446e+09  2.891e+10
Flop/sec:             4.560e+08     1.001   4.559e+08  9.119e+09
MPI Messages:         1.906e+03     2.000   1.811e+03  3.621e+04
MPI Message Lengths:  1.522e+07     2.000   7.987e+03  2.893e+08
MPI Reductions:       1.445e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.1704e+00 100.0%  2.8910e+10 100.0%  3.621e+04 100.0%  7.987e+03      100.0%  1.438e+03  99.5%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 3.6155e-0344.6 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 2.4161e-0349.3 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult              951 1.0 1.0240e+00 1.0 4.28e+08 1.0 3.6e+04 8.0e+03 0.0e+00 31 30100100  0  31 30100100  0  8351       0      0 0.00e+00    0 0.00e+00  0
MatSolve             951 1.0 1.0791e+00 1.0 4.24e+08 1.0 0.0e+00 0.0e+00 0.0e+00 33 29  0  0  0  33 29  0  0  0  7857       0      0 0.00e+00    0 0.00e+00  0
MatLUFactorNum         1 1.0 7.5230e-03 1.0 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1443       0      0 0.00e+00    0 0.00e+00  0
MatILUFactorSym        1 1.0 3.4390e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 2.4692e-0324.3 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.8136e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetRowIJ            1 1.0 2.7630e-06 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetOrdering         1 1.0 6.9472e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot               950 1.0 1.4874e-01 1.5 9.50e+07 1.0 0.0e+00 0.0e+00 9.5e+02  4  7  0  0 66   4  7  0  0 66 12774       0      0 0.00e+00    0 0.00e+00  0
VecNorm              477 1.0 8.4858e-02 1.8 4.77e+07 1.0 0.0e+00 0.0e+00 4.8e+02  2  3  0  0 33   2  3  0  0 33 11242       0      0 0.00e+00    0 0.00e+00  0
VecCopy                5 1.0 3.5896e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet               955 1.0 2.8708e-02 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             1899 1.0 1.0988e-01 1.1 1.90e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3 13  0  0  0   3 13  0  0  0 34566       0      0 0.00e+00    0 0.00e+00  0
VecAYPX              949 1.0 1.2596e-01 1.0 9.48e+07 1.0 0.0e+00 0.0e+00 0.0e+00  4  7  0  0  0   4  7  0  0  0 15052       0      0 0.00e+00    0 0.00e+00  0
VecWAXPY            1898 1.0 2.5503e-01 1.0 1.66e+08 1.0 0.0e+00 0.0e+00 0.0e+00  8 11  0  0  0   8 11  0  0  0 13022       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin      951 1.0 1.5345e-02 1.6 0.00e+00 0.0 3.6e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd        951 1.0 5.7887e-02 3.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.7380e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.3345e-03 3.3 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin       951 1.0 1.3910e-02 1.7 0.00e+00 0.0 3.6e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd         951 1.0 5.6894e-02 3.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack               951 1.0 3.0491e-03 8.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack             951 1.0 4.7905e-04 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.8668e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.7901e+00 1.0 1.45e+09 1.0 3.6e+04 8.0e+03 1.4e+03 88100100100 99  88100100100 99 10357       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                2 1.0 2.5023e-02 1.3 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0   434       0      0 0.00e+00    0 0.00e+00  0
PCSetUpOnBlocks        1 1.0 1.1689e-02 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   928       0      0 0.00e+00    0 0.00e+00  0
PCApply              951 1.0 1.1146e+00 1.0 4.24e+08 1.0 0.0e+00 0.0e+00 0.0e+00 34 29  0  0  0  34 29  0  0  0  7607       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              4     11791508     0.
         Vec Scatter     1              1          816     0.
              Vector    16             16      4835264     0.
           Index Set     5              5       608512     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     2              2         2832     0.
      Preconditioner     2              2         1944     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.87e-08
Average time for MPI_Barrier(): 1.25634e-05
Average time for zero size MPI_Send(): 6.2398e-06
#PETSc Option Table entries:
-ksp_type tfqmr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type bjacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 3.10408e-07 iterations 1511
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:09:49 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           5.956e+00     1.000   5.956e+00
Objects:              2.500e+01     1.000   2.500e+01
Flop:                 3.399e+09     1.002   3.399e+09  6.797e+10
Flop/sec:             5.707e+08     1.002   5.706e+08  1.141e+10
MPI Messages:         6.050e+03     2.000   5.748e+03  1.150e+05
MPI Message Lengths:  4.838e+07     2.000   7.996e+03  9.191e+08
MPI Reductions:       4.555e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 5.9561e+00 100.0%  6.7975e+10 100.0%  1.150e+05 100.0%  7.996e+03      100.0%  4.548e+03  99.8%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 6.0482e-0353.2 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 5.0639e-03109.8 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             3023 1.0 3.2118e+00 1.0 1.36e+09 1.0 1.1e+05 8.0e+03 0.0e+00 53 40100100  0  53 40100100  0  8463       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 5.1166e-0353.6 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7678e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot              3022 1.0 3.7282e-01 1.4 3.02e+08 1.0 0.0e+00 0.0e+00 3.0e+03  6  9  0  0 66   6  9  0  0 66 16211       0      0 0.00e+00    0 0.00e+00  0
VecNorm             1513 1.0 2.0853e-01 1.6 1.51e+08 1.0 0.0e+00 0.0e+00 1.5e+03  3  4  0  0 33   3  4  0  0 33 14511       0      0 0.00e+00    0 0.00e+00  0
VecCopy                5 1.0 3.3598e-04 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 4 1.0 2.8035e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             6043 1.0 3.1911e-01 1.1 6.04e+08 1.0 0.0e+00 0.0e+00 0.0e+00  5 18  0  0  0   5 18  0  0  0 37874       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             3021 1.0 3.9985e-01 1.0 3.02e+08 1.0 0.0e+00 0.0e+00 0.0e+00  7  9  0  0  0   7  9  0  0  0 15106       0      0 0.00e+00    0 0.00e+00  0
VecWAXPY            6042 1.0 8.0630e-01 1.0 5.29e+08 1.0 0.0e+00 0.0e+00 0.0e+00 13 16  0  0  0  13 16  0  0  0 13113       0      0 0.00e+00    0 0.00e+00  0
VecPointwiseMult    3023 1.0 4.3652e-01 1.0 1.51e+08 1.0 0.0e+00 0.0e+00 0.0e+00  7  4  0  0  0   7  4  0  0  0  6925       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     3023 1.0 3.5806e-02 1.6 0.00e+00 0.0 1.1e+05 8.0e+03 0.0e+00  1  0100100  0   1  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       3023 1.0 2.1219e-01 3.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.7570e-06 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.5641e-03 3.9 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      3023 1.0 3.2338e-02 1.7 0.00e+00 0.0 1.1e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        3023 1.0 2.0875e-01 3.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              3023 1.0 8.3102e-03 8.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            3023 1.0 1.8071e-03 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 1.8471e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 5.5781e+00 1.0 3.40e+09 1.0 1.1e+05 8.0e+03 4.5e+03 94100100100100  94100100100100 12184       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 8.7100e-07 2.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply             3023 1.0 4.4071e-01 1.1 1.51e+08 1.0 0.0e+00 0.0e+00 2.0e+00  7  4  0  0  0   7  4  0  0  0  6859       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      8013484     0.
         Vec Scatter     1              1          816     0.
              Vector    15             15      5233792     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1416     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.14e-08
Average time for MPI_Barrier(): 1.07258e-05
Average time for zero size MPI_Send(): 6.43705e-06
#PETSc Option Table entries:
-ksp_type tfqmr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type jacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.000233762 iterations 533
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:09:55 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.916e+00     1.000   3.916e+00
Objects:              2.400e+01     1.000   2.400e+01
Flop:                 1.675e+09     1.001   1.675e+09  3.349e+10
Flop/sec:             4.278e+08     1.001   4.277e+08  8.554e+09
MPI Messages:         2.138e+03     2.000   2.031e+03  4.062e+04
MPI Message Lengths:  1.708e+07     2.000   7.989e+03  3.245e+08
MPI Reductions:       1.619e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.9156e+00 100.0%  3.3495e+10 100.0%  4.062e+04 100.0%  7.989e+03      100.0%  1.612e+03  99.6%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 1.8550e-0321.0 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 1.4177e-0326.7 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             1067 1.0 1.1187e+00 1.0 4.80e+08 1.0 4.1e+04 8.0e+03 0.0e+00 28 29100100  0  28 29100100  0  8576       0      0 0.00e+00    0 0.00e+00  0
MatSOR              1067 1.0 1.6197e+00 1.0 5.29e+08 1.0 0.0e+00 0.0e+00 0.0e+00 41 32  0  0  0  41 32  0  0  0  6533       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 1.4689e-0313.9 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7827e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot              1066 1.0 1.7977e-01 1.3 1.07e+08 1.0 0.0e+00 0.0e+00 1.1e+03  4  6  0  0 66   4  6  0  0 66 11859       0      0 0.00e+00    0 0.00e+00  0
VecNorm              535 1.0 9.1403e-02 1.4 5.35e+07 1.0 0.0e+00 0.0e+00 5.4e+02  2  3  0  0 33   2  3  0  0 33 11706       0      0 0.00e+00    0 0.00e+00  0
VecCopy                5 1.0 3.6906e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 4 1.0 2.8287e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             2131 1.0 1.4085e-01 1.1 2.13e+08 1.0 0.0e+00 0.0e+00 0.0e+00  4 13  0  0  0   4 13  0  0  0 30260       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             1065 1.0 1.5285e-01 1.1 1.06e+08 1.0 0.0e+00 0.0e+00 0.0e+00  4  6  0  0  0   4  6  0  0  0 13922       0      0 0.00e+00    0 0.00e+00  0
VecWAXPY            2130 1.0 3.0230e-01 1.0 1.86e+08 1.0 0.0e+00 0.0e+00 0.0e+00  8 11  0  0  0   8 11  0  0  0 12329       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     1067 1.0 1.8500e-02 1.5 0.00e+00 0.0 4.1e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       1067 1.0 6.2415e-02 2.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 1.0029e-05 2.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.1394e-03 3.0 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      1067 1.0 1.6625e-02 1.6 0.00e+00 0.0 4.1e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        1067 1.0 6.0954e-02 2.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              1067 1.0 4.2067e-0311.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            1067 1.0 6.0198e-04 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 1.8204e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 3.5387e+00 1.0 1.67e+09 1.0 4.1e+04 8.0e+03 1.6e+03 90100100100 99  90100100100 99  9462       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 6.1100e-07 4.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply             1067 1.0 1.6219e+00 1.0 5.29e+08 1.0 0.0e+00 0.0e+00 0.0e+00 41 32  0  0  0  41 32  0  0  0  6524       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    14             14      4832064     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1416     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.11e-08
Average time for MPI_Barrier(): 1.09586e-05
Average time for zero size MPI_Send(): 6.17815e-06
#PETSc Option Table entries:
-ksp_type tfqmr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type sor
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.0017322 iterations 281
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:10:01 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           4.281e+00     1.000   4.281e+00
Objects:              4.700e+01     1.000   4.700e+01
Flop:                 1.665e+09     1.001   1.664e+09  3.329e+10
Flop/sec:             3.889e+08     1.001   3.888e+08  7.776e+09
MPI Messages:         2.276e+03     2.000   2.162e+03  4.324e+04
MPI Message Lengths:  1.818e+07     2.000   7.989e+03  3.455e+08
MPI Reductions:       8.860e+02     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 4.2807e+00 100.0%  3.3288e+10 100.0%  4.324e+04 100.0%  7.989e+03      100.0%  8.790e+02  99.2%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 5.4324e-0359.0 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 5.2907e-0395.7 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             1136 1.0 1.2595e+00 1.1 5.11e+08 1.0 4.3e+04 8.0e+03 0.0e+00 29 31100100  0  29 31100100  0  8111       0      0 0.00e+00    0 0.00e+00  0
MatSOR              1137 1.0 1.8138e+00 1.1 5.64e+08 1.0 0.0e+00 0.0e+00 0.0e+00 41 34  0  0  0  41 34  0  0  0  6217       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 5.3411e-0350.0 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7056e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  1   1  0  0  0  1     0       0      0 0.00e+00    0 0.00e+00  0
VecDot               562 1.0 1.1396e-01 1.5 5.62e+07 1.0 0.0e+00 0.0e+00 5.6e+02  2  3  0  0 63   2  3  0  0 64  9863       0      0 0.00e+00    0 0.00e+00  0
VecMDot               10 1.0 4.5054e-03 1.3 5.50e+06 1.0 0.0e+00 0.0e+00 1.0e+01  0  0  0  0  1   0  0  0  0  1 24415       0      0 0.00e+00    0 0.00e+00  0
VecNorm              294 1.0 7.1348e-02 1.9 2.94e+07 1.0 0.0e+00 0.0e+00 2.9e+02  1  2  0  0 33   1  2  0  0 33  8241       0      0 0.00e+00    0 0.00e+00  0
VecScale              11 1.0 2.5698e-04 1.1 5.50e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 42804       0      0 0.00e+00    0 0.00e+00  0
VecCopy             1132 1.0 5.5400e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet              1132 1.0 3.4496e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             1124 1.0 7.6153e-02 1.1 1.12e+08 1.0 0.0e+00 0.0e+00 0.0e+00  2  7  0  0  0   2  7  0  0  0 29519       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             1687 1.0 2.4451e-01 1.1 1.40e+08 1.0 0.0e+00 0.0e+00 0.0e+00  6  8  0  0  0   6  8  0  0  0 11488       0      0 0.00e+00    0 0.00e+00  0
VecAXPBYCZ           563 1.0 1.0103e-01 1.1 1.41e+08 1.0 0.0e+00 0.0e+00 0.0e+00  2  8  0  0  0   2  8  0  0  0 27864       0      0 0.00e+00    0 0.00e+00  0
VecWAXPY            1122 1.0 1.6572e-01 1.1 9.82e+07 1.0 0.0e+00 0.0e+00 0.0e+00  4  6  0  0  0   4  6  0  0  0 11845       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY              11 1.0 4.8480e-03 1.0 6.50e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 26815       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     1136 1.0 2.2583e-02 1.6 0.00e+00 0.0 4.3e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       1136 1.0 1.2185e-01 4.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecNormalize          11 1.0 7.6082e-03 1.1 1.65e+06 1.0 0.0e+00 0.0e+00 1.1e+01  0  0  0  0  1   0  0  0  0  1  4337       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.6660e-06 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.3900e-03 3.4 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      1136 1.0 2.0336e-02 1.7 0.00e+00 0.0 4.3e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        1136 1.0 1.2024e-01 4.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              1136 1.0 4.1188e-03 9.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            1136 1.0 6.7288e-04 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.4363e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  3  1  1  1  3   3  1  1  1  3  3307       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 3.8987e+00 1.0 1.66e+09 1.0 4.3e+04 8.0e+03 8.7e+02 91100100100 98  91100100100 99  8535       0      0 0.00e+00    0 0.00e+00  0
KSPGMRESOrthog        10 1.0 1.2636e-02 1.1 1.10e+07 1.0 0.0e+00 0.0e+00 1.0e+01  0  1  0  0  1   0  1  0  0  1 17410       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 1.4207e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  3  1  1  1  3   3  1  1  1  3  3343       0      0 0.00e+00    0 0.00e+00  0
PCApply              574 1.0 2.7515e+00 1.0 1.04e+09 1.0 2.1e+04 8.0e+03 0.0e+00 63 63 49 50  0  63 63 49 50  0  7575       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    33             33     12464896     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     3              3        33432     0.
      Preconditioner     3              3         2936     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.88e-08
Average time for MPI_Barrier(): 1.3788e-05
Average time for zero size MPI_Send(): 6.6057e-06
#PETSc Option Table entries:
-ksp_type tfqmr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.00115167 iterations 573
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:10:05 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           2.058e+00     1.000   2.058e+00
Objects:              2.900e+01     1.000   2.900e+01
Flop:                 9.164e+08     1.001   9.163e+08  1.833e+10
Flop/sec:             4.452e+08     1.001   4.451e+08  8.903e+09
MPI Messages:         1.154e+03     2.000   1.096e+03  2.193e+04
MPI Message Lengths:  9.208e+06     2.000   7.979e+03  1.750e+08
MPI Reductions:       1.166e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 2.0583e+00 100.0%  1.8325e+10 100.0%  2.193e+04 100.0%  7.979e+03      100.0%  1.159e+03  99.4%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 4.1046e-0355.6 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 4.0287e-0378.2 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult              575 1.0 6.2076e-01 1.0 2.59e+08 1.0 2.2e+04 8.0e+03 0.0e+00 30 28100100  0  30 28100100  0  8329       0      0 0.00e+00    0 0.00e+00  0
MatSolve             574 1.0 6.6868e-01 1.1 2.56e+08 1.0 0.0e+00 0.0e+00 0.0e+00 32 28  0  0  0  32 28  0  0  0  7654       0      0 0.00e+00    0 0.00e+00  0
MatLUFactorNum         1 1.0 7.5041e-03 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1446       0      0 0.00e+00    0 0.00e+00  0
MatILUFactorSym        1 1.0 3.3733e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 4.0807e-0336.6 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7786e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetRowIJ            1 1.0 2.8140e-06 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetOrdering         1 1.0 6.9258e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot               573 1.0 9.5648e-02 2.0 5.73e+07 1.0 0.0e+00 0.0e+00 5.7e+02  4  6  0  0 49   4  6  0  0 49 11981       0      0 0.00e+00    0 0.00e+00  0
VecNorm                1 1.0 1.1047e-04 1.1 1.00e+05 1.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0 18104       0      0 0.00e+00    0 0.00e+00  0
VecCopy                3 1.0 2.4990e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet               577 1.0 1.0063e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             1147 1.0 6.1207e-02 1.1 1.15e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3 13  0  0  0   3 13  0  0  0 37480       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             1144 1.0 1.5013e-01 1.1 1.14e+08 1.0 0.0e+00 0.0e+00 0.0e+00  7 12  0  0  0   7 12  0  0  0 15241       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin      575 1.0 9.4659e-03 1.5 0.00e+00 0.0 2.2e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd        575 1.0 5.0370e-02 3.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecReduceArith      1148 1.0 8.4741e-02 1.2 1.15e+08 1.0 0.0e+00 0.0e+00 0.0e+00  4 13  0  0  0   4 13  0  0  0 27094       0      0 0.00e+00    0 0.00e+00  0
VecReduceComm        574 1.0 3.0253e-02 2.7 0.00e+00 0.0 0.0e+00 0.0e+00 5.7e+02  1  0  0  0 49   1  0  0  0 50     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.6610e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.2135e-03 3.2 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin       575 1.0 8.6118e-03 1.6 0.00e+00 0.0 2.2e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd         575 1.0 4.9767e-02 3.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack               575 1.0 1.4260e-03 6.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack             575 1.0 3.1673e-04 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.2285e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 1.6792e+00 1.0 9.16e+08 1.0 2.2e+04 8.0e+03 1.1e+03 82100 99100 98  82100 99100 99 10906       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                2 1.0 2.5707e-02 1.2 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0   422       0      0 0.00e+00    0 0.00e+00  0
PCSetUpOnBlocks        1 1.0 1.1566e-02 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0   938       0      0 0.00e+00    0 0.00e+00  0
PCApply              574 1.0 6.8229e-01 1.1 2.56e+08 1.0 0.0e+00 0.0e+00 0.0e+00 32 28  0  0  0  32 28  0  0  0  7501       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              4     11791508     0.
         Vec Scatter     1              1          816     0.
              Vector    13             13      3630080     0.
           Index Set     5              5       608512     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     2              2         2832     0.
      Preconditioner     2              2         1944     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.06e-08
Average time for MPI_Barrier(): 1.12998e-05
Average time for zero size MPI_Send(): 7.2189e-06
#PETSc Option Table entries:
-ksp_type cr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type bjacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.000525242 iterations 1635
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:10:10 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.282e+00     1.000   3.282e+00
Objects:              2.200e+01     1.000   2.200e+01
Flop:                 1.963e+09     1.002   1.962e+09  3.925e+10
Flop/sec:             5.980e+08     1.002   5.979e+08  1.196e+10
MPI Messages:         3.278e+03     2.000   3.114e+03  6.228e+04
MPI Message Lengths:  2.620e+07     2.000   7.993e+03  4.978e+08
MPI Reductions:       3.292e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.2825e+00 100.0%  3.9250e+10 100.0%  6.228e+04 100.0%  7.993e+03      100.0%  3.285e+03  99.8%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 3.7948e-0334.5 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 3.7274e-0371.9 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             1637 1.0 1.7229e+00 1.0 7.36e+08 1.0 6.2e+04 8.0e+03 0.0e+00 52 38100100  0  52 38100100  0  8544       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 3.7825e-0333.9 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7716e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot              1635 1.0 1.5988e-01 1.3 1.63e+08 1.0 0.0e+00 0.0e+00 1.6e+03  4  8  0  0 50   4  8  0  0 50 20452       0      0 0.00e+00    0 0.00e+00  0
VecNorm                1 1.0 9.6276e-05 1.3 1.00e+05 1.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0 20774       0      0 0.00e+00    0 0.00e+00  0
VecCopy                3 1.0 2.3010e-04 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 3 1.0 2.2223e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             3271 1.0 1.6342e-01 1.1 3.27e+08 1.0 0.0e+00 0.0e+00 0.0e+00  5 17  0  0  0   5 17  0  0  0 40032       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             3268 1.0 4.2096e-01 1.1 3.27e+08 1.0 0.0e+00 0.0e+00 0.0e+00 12 17  0  0  0  12 17  0  0  0 15526       0      0 0.00e+00    0 0.00e+00  0
VecPointwiseMult    1636 1.0 2.3111e-01 1.1 8.18e+07 1.0 0.0e+00 0.0e+00 0.0e+00  7  4  0  0  0   7  4  0  0  0  7079       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     1637 1.0 2.2829e-02 1.9 0.00e+00 0.0 6.2e+04 8.0e+03 0.0e+00  1  0100100  0   1  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       1637 1.0 1.2446e-01 4.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecReduceArith      3272 1.0 2.1446e-01 1.0 3.27e+08 1.0 0.0e+00 0.0e+00 0.0e+00  6 17  0  0  0   6 17  0  0  0 30513       0      0 0.00e+00    0 0.00e+00  0
VecReduceComm       1636 1.0 6.9563e-02 3.3 0.00e+00 0.0 0.0e+00 0.0e+00 1.6e+03  1  0  0  0 50   1  0  0  0 50     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.6890e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.1103e-03 2.7 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      1637 1.0 2.0918e-02 2.1 0.00e+00 0.0 6.2e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        1637 1.0 1.2286e-01 5.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              1637 1.0 4.5504e-03 8.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            1637 1.0 8.8714e-04 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 1.2362e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.9001e+00 1.0 1.96e+09 1.0 6.2e+04 8.0e+03 3.3e+03 88100100100 99  88100100100100 13529       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 1.0750e-06 3.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply             1636 1.0 2.3460e-01 1.1 8.18e+07 1.0 0.0e+00 0.0e+00 2.0e+00  7  4  0  0  0   7  4  0  0  0  6974       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      8013484     0.
         Vec Scatter     1              1          816     0.
              Vector    12             12      4028608     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1416     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.5e-08
Average time for MPI_Barrier(): 1.44916e-05
Average time for zero size MPI_Send(): 6.5132e-06
#PETSc Option Table entries:
-ksp_type cr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type jacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.000702134 iterations 672
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:10:15 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           2.612e+00     1.000   2.612e+00
Objects:              2.100e+01     1.000   2.100e+01
Flop:                 1.107e+09     1.001   1.107e+09  2.215e+10
Flop/sec:             4.239e+08     1.001   4.239e+08  8.478e+09
MPI Messages:         1.352e+03     2.000   1.284e+03  2.569e+04
MPI Message Lengths:  1.079e+07     2.000   7.982e+03  2.050e+08
MPI Reductions:       1.364e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 2.6124e+00 100.0%  2.2147e+10 100.0%  2.569e+04 100.0%  7.982e+03      100.0%  1.357e+03  99.5%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 4.9685e-0379.2 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 3.8624e-0385.6 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult              674 1.0 7.0506e-01 1.0 3.03e+08 1.0 2.6e+04 8.0e+03 0.0e+00 27 27100100  0  27 27100100  0  8596       0      0 0.00e+00    0 0.00e+00  0
MatSOR               673 1.0 1.0241e+00 1.0 3.34e+08 1.0 0.0e+00 0.0e+00 0.0e+00 39 30  0  0  0  39 30  0  0  0  6517       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 3.9171e-0340.4 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7547e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot               672 1.0 1.1072e-01 1.4 6.72e+07 1.0 0.0e+00 0.0e+00 6.7e+02  4  6  0  0 49   4  6  0  0 50 12139       0      0 0.00e+00    0 0.00e+00  0
VecNorm                1 1.0 9.3997e-05 1.2 1.00e+05 1.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0 21277       0      0 0.00e+00    0 0.00e+00  0
VecCopy                3 1.0 2.3967e-04 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 3 1.0 2.1002e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             1345 1.0 1.2059e-01 1.1 1.34e+08 1.0 0.0e+00 0.0e+00 0.0e+00  4 12  0  0  0   4 12  0  0  0 22306       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             1342 1.0 1.7807e-01 1.0 1.34e+08 1.0 0.0e+00 0.0e+00 0.0e+00  7 12  0  0  0   7 12  0  0  0 15073       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin      674 1.0 1.4891e-02 1.6 0.00e+00 0.0 2.6e+04 8.0e+03 0.0e+00  1  0100100  0   1  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd        674 1.0 3.4252e-02 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecReduceArith      1346 1.0 1.0365e-01 1.2 1.35e+08 1.0 0.0e+00 0.0e+00 0.0e+00  4 12  0  0  0   4 12  0  0  0 25972       0      0 0.00e+00    0 0.00e+00  0
VecReduceComm        673 1.0 5.3790e-02 2.7 0.00e+00 0.0 0.0e+00 0.0e+00 6.7e+02  1  0  0  0 49   1  0  0  0 50     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 1.3435e-05 3.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.2425e-03 3.4 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin       674 1.0 1.3374e-02 1.7 0.00e+00 0.0 2.6e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd         674 1.0 3.3210e-02 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack               674 1.0 2.9503e-03 7.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack             674 1.0 3.8205e-04 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 1.2308e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.2334e+00 1.0 1.11e+09 1.0 2.6e+04 8.0e+03 1.3e+03 85100100100 99  85100100100 99  9910       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 3.0000e-07 2.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply              673 1.0 1.0256e+00 1.0 3.34e+08 1.0 0.0e+00 0.0e+00 0.0e+00 39 30  0  0  0  39 30  0  0  0  6508       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    11             11      3626880     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1416     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.18e-08
Average time for MPI_Barrier(): 1.27146e-05
Average time for zero size MPI_Send(): 6.39575e-06
#PETSc Option Table entries:
-ksp_type cr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type sor
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.000656407 iterations 374
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:10:20 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.045e+00     1.000   3.045e+00
Objects:              4.400e+01     1.000   4.400e+01
Flop:                 1.145e+09     1.001   1.145e+09  2.290e+10
Flop/sec:             3.761e+08     1.001   3.761e+08  7.521e+09
MPI Messages:         1.526e+03     2.000   1.450e+03  2.899e+04
MPI Message Lengths:  1.218e+07     2.000   7.984e+03  2.315e+08
MPI Reductions:       7.910e+02     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.0453e+00 100.0%  2.2905e+10 100.0%  2.899e+04 100.0%  7.984e+03      100.0%  7.840e+02  99.1%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 3.5583e-0337.4 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 3.5222e-0364.9 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult              761 1.0 8.4692e-01 1.0 3.42e+08 1.0 2.9e+04 8.0e+03 0.0e+00 27 30100100  0  27 30100100  0  8080       0      0 0.00e+00    0 0.00e+00  0
MatSOR               761 1.0 1.1915e+00 1.0 3.77e+08 1.0 0.0e+00 0.0e+00 0.0e+00 38 33  0  0  0  38 33  0  0  0  6334       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 3.5768e-0333.4 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7085e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  1   1  0  0  0  1     0       0      0 0.00e+00    0 0.00e+00  0
VecDot               374 1.0 9.4817e-02 1.9 3.74e+07 1.0 0.0e+00 0.0e+00 3.7e+02  2  3  0  0 47   2  3  0  0 48  7889       0      0 0.00e+00    0 0.00e+00  0
VecMDot               10 1.0 4.7262e-03 1.4 5.50e+06 1.0 0.0e+00 0.0e+00 1.0e+01  0  0  0  0  1   0  0  0  0  1 23274       0      0 0.00e+00    0 0.00e+00  0
VecNorm               12 1.0 7.8195e-03 1.2 1.20e+06 1.0 0.0e+00 0.0e+00 1.2e+01  0  0  0  0  2   0  0  0  0  2  3069       0      0 0.00e+00    0 0.00e+00  0
VecScale              11 1.0 2.6720e-04 1.1 5.50e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 41167       0      0 0.00e+00    0 0.00e+00  0
VecCopy              754 1.0 3.3590e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet               755 1.0 1.9672e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY              750 1.0 8.1145e-02 1.1 7.50e+07 1.0 0.0e+00 0.0e+00 0.0e+00  3  7  0  0  0   3  7  0  0  0 18486       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             1496 1.0 2.0966e-01 1.0 1.31e+08 1.0 0.0e+00 0.0e+00 0.0e+00  7 11  0  0  0   7 11  0  0  0 12482       0      0 0.00e+00    0 0.00e+00  0
VecAXPBYCZ           375 1.0 6.7847e-02 1.1 9.38e+07 1.0 0.0e+00 0.0e+00 0.0e+00  2  8  0  0  0   2  8  0  0  0 27636       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY              11 1.0 5.0376e-03 1.1 6.50e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 25806       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin      761 1.0 1.5761e-02 1.5 0.00e+00 0.0 2.9e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd        761 1.0 7.8079e-02 2.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecReduceArith       750 1.0 5.6128e-02 1.1 7.50e+07 1.0 0.0e+00 0.0e+00 0.0e+00  2  7  0  0  0   2  7  0  0  0 26724       0      0 0.00e+00    0 0.00e+00  0
VecReduceComm        375 1.0 2.8201e-02 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 3.8e+02  1  0  0  0 47   1  0  0  0 48     0       0      0 0.00e+00    0 0.00e+00  0
VecNormalize          11 1.0 8.0169e-03 1.2 1.65e+06 1.0 0.0e+00 0.0e+00 1.1e+01  0  0  0  0  1   0  0  0  0  1  4116       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.5040e-06 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.4562e-03 3.7 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin       761 1.0 1.4168e-02 1.5 0.00e+00 0.0 2.9e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd         761 1.0 7.7109e-02 2.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack               761 1.0 2.8217e-03 6.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack             761 1.0 4.2459e-04 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.4234e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  5  2  1  1  3   5  2  1  1  3  3337       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.6619e+00 1.0 1.14e+09 1.0 2.9e+04 8.0e+03 7.7e+02 87100100100 98  87100100100 98  8600       0      0 0.00e+00    0 0.00e+00  0
KSPGMRESOrthog        10 1.0 1.2834e-02 1.1 1.10e+07 1.0 0.0e+00 0.0e+00 1.0e+01  0  1  0  0  1   0  1  0  0  1 17143       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 1.4142e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  5  2  1  1  3   5  2  1  1  3  3359       0      0 0.00e+00    0 0.00e+00  0
PCApply              386 1.0 1.8305e+00 1.0 6.96e+08 1.0 1.4e+04 8.0e+03 0.0e+00 60 61 49 49  0  60 61 49 49  0  7604       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    30             30     11259712     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     3              3        33432     0.
      Preconditioner     3              3         2936     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.86e-08
Average time for MPI_Barrier(): 1.26442e-05
Average time for zero size MPI_Send(): 6.3028e-06
#PETSc Option Table entries:
-ksp_type cr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.0311837 iterations 7307
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:11:01 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.911e+01     1.000   3.911e+01
Objects:              8.400e+01     1.000   8.400e+01
Flop:                 4.268e+10     1.000   4.268e+10  8.536e+11
Flop/sec:             1.091e+09     1.000   1.091e+09  2.183e+10
MPI Messages:         1.462e+04     2.000   1.389e+04  2.778e+05
MPI Message Lengths:  1.170e+08     2.000   7.998e+03  2.222e+09
MPI Reductions:       2.170e+04     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.9109e+01 100.0%  8.5362e+11 100.0%  2.778e+05 100.0%  7.998e+03      100.0%  2.169e+04 100.0%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 4.3130e-0350.6 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 3.9974e-0374.5 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             7309 1.0 7.4913e+00 1.0 3.29e+09 1.0 2.8e+05 8.0e+03 0.0e+00 19  8100100  0  19  8100100  0  8773       0      0 0.00e+00    0 0.00e+00  0
MatSolve            7307 1.0 8.0186e+00 1.0 3.26e+09 1.0 0.0e+00 0.0e+00 0.0e+00 20  8  0  0  0  20  8  0  0  0  8125       0      0 0.00e+00    0 0.00e+00  0
MatLUFactorNum         1 1.0 7.5295e-03 1.0 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1441       0      0 0.00e+00    0 0.00e+00  0
MatILUFactorSym        1 1.0 3.4312e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 4.0499e-0337.7 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7713e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetRowIJ            1 1.0 2.9650e-06 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetOrdering         1 1.0 6.9409e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDotNorm2         7307 1.0 1.6376e+00 1.1 1.46e+09 1.0 0.0e+00 0.0e+00 7.3e+03  4  3  0  0 34   4  3  0  0 34 17848       0      0 0.00e+00    0 0.00e+00  0
VecMDot             7063 1.0 5.9555e+00 1.0 1.06e+10 1.0 0.0e+00 0.0e+00 7.1e+03 15 25  0  0 33  15 25  0  0 33 35544       0      0 0.00e+00    0 0.00e+00  0
VecNorm             7309 1.0 6.6346e-01 1.3 7.31e+08 1.0 0.0e+00 0.0e+00 7.3e+03  2  2  0  0 34   2  2  0  0 34 22033       0      0 0.00e+00    0 0.00e+00  0
VecScale           14614 1.0 3.1880e-01 1.1 7.31e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1  2  0  0  0   1  2  0  0  0 45841       0      0 0.00e+00    0 0.00e+00  0
VecSet              7310 1.0 4.5801e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY            14615 1.0 8.0054e-01 1.2 1.46e+09 1.0 0.0e+00 0.0e+00 0.0e+00  2  3  0  0  0   2  3  0  0  0 36513       0      0 0.00e+00    0 0.00e+00  0
VecAYPX                1 1.0 1.6648e-04 1.0 5.00e+04 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  6007       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY           14126 1.0 1.3724e+01 1.0 2.12e+10 1.0 0.0e+00 0.0e+00 0.0e+00 35 50  0  0  0  35 50  0  0  0 30848       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     7309 1.0 1.1227e-01 1.5 0.00e+00 0.0 2.8e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       7309 1.0 2.7646e-01 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.8190e-06 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.2452e-03 3.2 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      7309 1.0 9.8780e-02 1.6 0.00e+00 0.0 2.8e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        7309 1.0 2.6726e-01 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              7309 1.0 2.6620e-02 5.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            7309 1.0 4.0195e-03 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.3818e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 3.8727e+01 1.0 4.27e+10 1.0 2.8e+05 8.0e+03 2.2e+04 99100100100100  99100100100100 22042       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                2 1.0 2.7169e-02 1.0 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   399       0      0 0.00e+00    0 0.00e+00  0
PCSetUpOnBlocks        1 1.0 1.1690e-02 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   928       0      0 0.00e+00    0 0.00e+00  0
PCApply             7307 1.0 8.5495e+00 1.0 3.26e+09 1.0 0.0e+00 0.0e+00 0.0e+00 22  8  0  0  0  22  8  0  0  0  7620       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              4     11791508     0.
         Vec Scatter     1              1          816     0.
              Vector    68             68     25725120     0.
           Index Set     5              5       608512     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     2              2         2896     0.
      Preconditioner     2              2         1944     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.05e-08
Average time for MPI_Barrier(): 1.42188e-05
Average time for zero size MPI_Send(): 6.75545e-06
#PETSc Option Table entries:
-ksp_type gcr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type bjacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 186.34 iterations 10000
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:11:46 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           4.331e+01     1.000   4.331e+01
Objects:              7.700e+01     1.000   7.700e+01
Flop:                 5.447e+10     1.000   5.447e+10  1.089e+12
Flop/sec:             1.258e+09     1.000   1.258e+09  2.515e+10
MPI Messages:         2.001e+04     2.000   1.901e+04  3.802e+05
MPI Message Lengths:  1.600e+08     2.000   7.999e+03  3.041e+09
MPI Reductions:       2.969e+04     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 4.3309e+01 100.0%  1.0893e+12 100.0%  3.802e+05 100.0%  7.999e+03      100.0%  2.968e+04 100.0%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 4.2353e-0349.3 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 4.1310e-0375.9 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult            10002 1.0 1.0321e+01 1.0 4.50e+09 1.0 3.8e+05 8.0e+03 0.0e+00 24  8100100  0  24  8100100  0  8714       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 4.1818e-0339.7 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7517e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDotNorm2        10000 1.0 2.2860e+00 1.1 2.00e+09 1.0 0.0e+00 0.0e+00 1.0e+04  5  4  0  0 34   5  4  0  0 34 17498       0      0 0.00e+00    0 0.00e+00  0
VecMDot             9666 1.0 7.8480e+00 1.0 1.45e+10 1.0 0.0e+00 0.0e+00 9.7e+03 18 27  0  0 33  18 27  0  0 33 36926       0      0 0.00e+00    0 0.00e+00  0
VecNorm            10002 1.0 1.0246e+00 1.4 1.00e+09 1.0 0.0e+00 0.0e+00 1.0e+04  2  2  0  0 34   2  2  0  0 34 19524       0      0 0.00e+00    0 0.00e+00  0
VecScale           20000 1.0 4.5927e-01 1.0 1.00e+09 1.0 0.0e+00 0.0e+00 0.0e+00  1  2  0  0  0   1  2  0  0  0 43547       0      0 0.00e+00    0 0.00e+00  0
VecSet                 3 1.0 2.2050e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY            20001 1.0 1.1986e+00 1.3 2.00e+09 1.0 0.0e+00 0.0e+00 0.0e+00  2  4  0  0  0   2  4  0  0  0 33374       0      0 0.00e+00    0 0.00e+00  0
VecAYPX                1 1.0 1.6817e-04 1.0 5.00e+04 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  5946       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY           19332 1.0 1.8839e+01 1.0 2.90e+10 1.0 0.0e+00 0.0e+00 0.0e+00 43 53  0  0  0  43 53  0  0  0 30766       0      0 0.00e+00    0 0.00e+00  0
VecPointwiseMult   10000 1.0 1.5202e+00 1.0 5.00e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3  1  0  0  0   3  1  0  0  0  6578       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin    10002 1.0 1.5819e-01 1.5 0.00e+00 0.0 3.8e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd      10002 1.0 3.5724e-01 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.7550e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.2783e-03 3.2 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin     10002 1.0 1.4040e-01 1.6 0.00e+00 0.0 3.8e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd       10002 1.0 3.4531e-01 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack             10002 1.0 3.9656e-02 7.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack           10002 1.0 5.1782e-03 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 1.4170e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 4.2923e+01 1.0 5.45e+10 1.0 3.8e+05 8.0e+03 3.0e+04 99100100100100  99100100100100 25379       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 1.2660e-06 2.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply            10000 1.0 1.5347e+00 1.0 5.00e+08 1.0 0.0e+00 0.0e+00 2.0e+00  4  1  0  0  0   4  1  0  0  0  6516       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      8013484     0.
         Vec Scatter     1              1          816     0.
              Vector    67             67     26123648     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1480     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.03e-08
Average time for MPI_Barrier(): 1.2301e-05
Average time for zero size MPI_Send(): 6.3569e-06
#PETSc Option Table entries:
-ksp_type gcr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type jacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.0316741 iterations 6946
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:12:29 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           4.064e+01     1.000   4.064e+01
Objects:              7.600e+01     1.000   7.600e+01
Flop:                 4.092e+10     1.000   4.092e+10  8.183e+11
Flop/sec:             1.007e+09     1.000   1.007e+09  2.013e+10
MPI Messages:         1.390e+04     2.000   1.320e+04  2.641e+05
MPI Message Lengths:  1.112e+08     2.000   7.998e+03  2.112e+09
MPI Reductions:       2.063e+04     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 4.0644e+01 100.0%  8.1834e+11 100.0%  2.641e+05 100.0%  7.998e+03      100.0%  2.062e+04 100.0%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 4.9232e-0355.5 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 4.8579e-03105.6 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             6948 1.0 7.1471e+00 1.0 3.13e+09 1.0 2.6e+05 8.0e+03 0.0e+00 17  8100100  0  17  8100100  0  8741       0      0 0.00e+00    0 0.00e+00  0
MatSOR              6946 1.0 1.0881e+01 1.0 3.44e+09 1.0 0.0e+00 0.0e+00 0.0e+00 27  8  0  0  0  27  8  0  0  0  6330       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 4.9106e-0350.4 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.8255e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDotNorm2         6946 1.0 1.5602e+00 1.1 1.39e+09 1.0 0.0e+00 0.0e+00 6.9e+03  4  3  0  0 34   4  3  0  0 34 17808       0      0 0.00e+00    0 0.00e+00  0
VecMDot             6714 1.0 6.0542e+00 1.0 1.01e+10 1.0 0.0e+00 0.0e+00 6.7e+03 15 25  0  0 33  15 25  0  0 33 33234       0      0 0.00e+00    0 0.00e+00  0
VecNorm             6948 1.0 7.1984e-01 1.4 6.95e+08 1.0 0.0e+00 0.0e+00 6.9e+03  2  2  0  0 34   2  2  0  0 34 19304       0      0 0.00e+00    0 0.00e+00  0
VecScale           13892 1.0 3.2892e-01 1.0 6.95e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1  2  0  0  0   1  2  0  0  0 42235       0      0 0.00e+00    0 0.00e+00  0
VecSet                 3 1.0 2.2754e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY            13893 1.0 8.7318e-01 1.3 1.39e+09 1.0 0.0e+00 0.0e+00 0.0e+00  2  3  0  0  0   2  3  0  0  0 31822       0      0 0.00e+00    0 0.00e+00  0
VecAYPX                1 1.0 1.6085e-04 1.1 5.00e+04 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  6217       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY           13428 1.0 1.3127e+01 1.0 2.01e+10 1.0 0.0e+00 0.0e+00 0.0e+00 32 49  0  0  0  32 49  0  0  0 30657       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     6948 1.0 1.1165e-01 1.6 0.00e+00 0.0 2.6e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       6948 1.0 3.2330e-01 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.5110e-06 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.2177e-03 3.1 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      6948 1.0 9.7833e-02 1.7 0.00e+00 0.0 2.6e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        6948 1.0 3.1496e-01 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              6948 1.0 2.7922e-02 7.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            6948 1.0 3.5842e-03 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 1.3776e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 4.0257e+01 1.0 4.09e+10 1.0 2.6e+05 8.0e+03 2.1e+04 99100100100100  99100100100100 20328       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 4.0800e-07 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply             6946 1.0 1.0894e+01 1.0 3.44e+09 1.0 0.0e+00 0.0e+00 0.0e+00 27  8  0  0  0  27  8  0  0  0  6322       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    66             66     25721920     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1480     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.96e-08
Average time for MPI_Barrier(): 1.21188e-05
Average time for zero size MPI_Send(): 6.3042e-06
#PETSc Option Table entries:
-ksp_type gcr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type sor
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.0315697 iterations 3254
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8072 with 20 processors, by luciano.siqueira Wed Sep  9 17:13:01 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           2.984e+01     1.000   2.984e+01
Objects:              9.900e+01     1.000   9.900e+01
Flop:                 2.355e+10     1.001   2.355e+10  4.711e+11
Flop/sec:             7.893e+08     1.001   7.892e+08  1.578e+10
MPI Messages:         1.304e+04     2.000   1.239e+04  2.478e+05
MPI Message Lengths:  1.043e+08     2.000   7.998e+03  1.982e+09
MPI Reductions:       9.698e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 2.9844e+01 100.0%  4.7106e+11 100.0%  2.478e+05 100.0%  7.998e+03      100.0%  9.691e+03  99.9%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 3.3096e-0318.4 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 3.2631e-0363.2 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             6520 1.0 6.8931e+00 1.0 2.93e+09 1.0 2.5e+05 8.0e+03 0.0e+00 23 12100100  0  23 12100100  0  8505       0      0 0.00e+00    0 0.00e+00  0
MatSOR              6519 1.0 1.0076e+01 1.0 3.23e+09 1.0 0.0e+00 0.0e+00 0.0e+00 33 14  0  0  0  33 14  0  0  0  6415       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 3.3162e-0332.3 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7873e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDotNorm2         3254 1.0 7.5769e-01 1.1 6.51e+08 1.0 0.0e+00 0.0e+00 3.3e+03  2  3  0  0 34   2  3  0  0 34 17179       0      0 0.00e+00    0 0.00e+00  0
VecMDot             3155 1.0 2.9487e+00 1.1 4.71e+09 1.0 0.0e+00 0.0e+00 3.2e+03 10 20  0  0 33  10 20  0  0 33 31964       0      0 0.00e+00    0 0.00e+00  0
VecNorm             3267 1.0 3.3594e-01 1.3 3.27e+08 1.0 0.0e+00 0.0e+00 3.3e+03  1  1  0  0 34   1  1  0  0 34 19450       0      0 0.00e+00    0 0.00e+00  0
VecScale            6519 1.0 1.5552e-01 1.1 3.26e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1  1  0  0  0   1  1  0  0  0 41918       0      0 0.00e+00    0 0.00e+00  0
VecCopy             6509 1.0 2.7715e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet              6513 1.0 2.5548e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             6510 1.0 4.0740e-01 1.2 6.51e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1  3  0  0  0   1  3  0  0  0 31959       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             6509 1.0 8.8111e-01 1.0 4.88e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3  2  0  0  0   3  2  0  0  0 11080       0      0 0.00e+00    0 0.00e+00  0
VecAXPBYCZ          3254 1.0 5.5165e-01 1.0 8.14e+08 1.0 0.0e+00 0.0e+00 0.0e+00  2  3  0  0  0   2  3  0  0  0 29493       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY            6301 1.0 6.2034e+00 1.0 9.42e+09 1.0 0.0e+00 0.0e+00 0.0e+00 21 40  0  0  0  21 40  0  0  0 30373       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     6520 1.0 1.1807e-01 1.7 0.00e+00 0.0 2.5e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       6520 1.0 4.6680e-01 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecNormalize          11 1.0 7.6909e-03 1.1 1.65e+06 1.0 0.0e+00 0.0e+00 1.1e+01  0  0  0  0  0   0  0  0  0  0  4291       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.6130e-06 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 2.3044e-0312.0 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      6520 1.0 1.0431e-01 1.8 0.00e+00 0.0 2.5e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        6520 1.0 4.5669e-01 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              6520 1.0 2.5128e-02 8.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            6520 1.0 4.8594e-03 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.5671e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.5e+01  1  0  0  0  0   1  0  0  0  0  3031       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.9461e+01 1.0 2.36e+10 1.0 2.5e+05 8.0e+03 9.7e+03 99100100100100  99100100100100 15989       0      0 0.00e+00    0 0.00e+00  0
KSPGMRESOrthog        10 1.0 1.3129e-02 1.1 1.10e+07 1.0 0.0e+00 0.0e+00 1.0e+01  0  0  0  0  0   0  0  0  0  0 16757       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 1.4261e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  0  0  0  0  0   0  0  0  0  0  3331       0      0 0.00e+00    0 0.00e+00  0
PCApply             3265 1.0 1.5436e+01 1.0 6.00e+09 1.0 1.2e+05 8.0e+03 0.0e+00 51 25 50 50  0  51 25 50 50  0  7770       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    85             85     33354752     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     3              3        33496     0.
      Preconditioner     3              3         2936     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.03e-08
Average time for MPI_Barrier(): 1.13548e-05
Average time for zero size MPI_Send(): 6.20035e-06
#PETSc Option Table entries:
-ksp_type gcr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

