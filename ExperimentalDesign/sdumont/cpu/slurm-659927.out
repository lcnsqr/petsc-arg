sdumont8080
Norm of error 0.000101716 iterations 641
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8080 with 20 processors, by luciano.siqueira Wed Sep  9 17:31:12 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           2.200e+00     1.000   2.200e+00
Objects:              2.600e+01     1.000   2.600e+01
Flop:                 9.603e+08     1.001   9.602e+08  1.920e+10
Flop/sec:             4.364e+08     1.001   4.364e+08  8.727e+09
MPI Messages:         1.288e+03     2.000   1.224e+03  2.447e+04
MPI Message Lengths:  1.028e+07     2.000   7.981e+03  1.953e+08
MPI Reductions:       1.943e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 2.2004e+00 100.0%  1.9204e+10 100.0%  2.447e+04 100.0%  7.981e+03      100.0%  1.936e+03  99.6%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 2.2546e-0312.2 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 2.1709e-0339.8 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult              642 1.0 7.0125e-01 1.0 2.89e+08 1.0 2.4e+04 8.0e+03 0.0e+00 32 30100100  0  32 30100100  0  8232       0      0 0.00e+00    0 0.00e+00  0
MatSolve             642 1.0 7.7594e-01 1.1 2.86e+08 1.0 0.0e+00 0.0e+00 0.0e+00 33 30  0  0  0  33 30  0  0  0  7377       0      0 0.00e+00    0 0.00e+00  0
MatLUFactorNum         1 1.0 7.6025e-03 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1428       0      0 0.00e+00    0 0.00e+00  0
MatILUFactorSym        1 1.0 3.5124e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 2.2259e-0320.6 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7729e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetRowIJ            1 1.0 2.6840e-06 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetOrdering         1 1.0 6.9924e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecTDot             1282 1.0 1.1391e-01 1.1 1.28e+08 1.0 0.0e+00 0.0e+00 1.3e+03  5 13  0  0 66   5 13  0  0 66 22509       0      0 0.00e+00    0 0.00e+00  0
VecNorm              643 1.0 1.3754e-01 2.4 6.43e+07 1.0 0.0e+00 0.0e+00 6.4e+02  5  7  0  0 33   5  7  0  0 33  9350       0      0 0.00e+00    0 0.00e+00  0
VecCopy                2 1.0 1.5340e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet               645 1.0 5.1952e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             1283 1.0 5.7834e-02 1.1 1.28e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3 13  0  0  0   3 13  0  0  0 44368       0      0 0.00e+00    0 0.00e+00  0
VecAYPX              640 1.0 8.4781e-02 1.1 6.40e+07 1.0 0.0e+00 0.0e+00 0.0e+00  4  7  0  0  0   4  7  0  0  0 15098       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin      642 1.0 8.8321e-03 1.4 0.00e+00 0.0 2.4e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd        642 1.0 7.3929e-02 5.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 1.4715e-05 3.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 5.2930e-04 2.2 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin       642 1.0 8.0383e-03 1.5 0.00e+00 0.0 2.4e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd         642 1.0 7.3279e-02 6.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack               642 1.0 1.5590e-03 5.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack             642 1.0 3.3143e-04 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 6.6063e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 1.8174e+00 1.0 9.60e+08 1.0 2.4e+04 8.0e+03 1.9e+03 83100100100 99  83100100100 99 10559       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                2 1.0 2.7635e-02 1.0 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0   393       0      0 0.00e+00    0 0.00e+00  0
PCSetUpOnBlocks        1 1.0 1.1873e-02 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0   914       0      0 0.00e+00    0 0.00e+00  0
PCApply              642 1.0 7.8723e-01 1.1 2.86e+08 1.0 0.0e+00 0.0e+00 0.0e+00 34 30  0  0  0  34 30  0  0  0  7271       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              4     11791508     0.
         Vec Scatter     1              1          816     0.
              Vector    10             10      2424896     0.
           Index Set     5              5       608512     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     2              2         2904     0.
      Preconditioner     2              2         1944     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.16e-08
Average time for MPI_Barrier(): 1.26506e-05
Average time for zero size MPI_Send(): 6.4955e-06
#PETSc Option Table entries:
-ksp_type cg
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type bjacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 4.69193e-05 iterations 1715
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8080 with 20 processors, by luciano.siqueira Wed Sep  9 17:31:17 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.122e+00     1.000   3.121e+00
Objects:              1.900e+01     1.000   1.900e+01
Flop:                 1.887e+09     1.002   1.887e+09  3.773e+10
Flop/sec:             6.045e+08     1.002   6.044e+08  1.209e+10
MPI Messages:         3.436e+03     2.000   3.264e+03  6.528e+04
MPI Message Lengths:  2.746e+07     2.000   7.993e+03  5.218e+08
MPI Reductions:       5.167e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.1213e+00 100.0%  3.7730e+10 100.0%  6.528e+04 100.0%  7.993e+03      100.0%  5.160e+03  99.9%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 5.3948e-0355.0 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 5.3308e-03112.2 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             1716 1.0 1.7607e+00 1.0 7.72e+08 1.0 6.5e+04 8.0e+03 0.0e+00 56 41100100  0  56 41100100  0  8764       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 5.3794e-0354.0 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6434e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecTDot             3430 1.0 2.8336e-01 1.1 3.43e+08 1.0 0.0e+00 0.0e+00 3.4e+03  8 18  0  0 66   8 18  0  0 66 24209       0      0 0.00e+00    0 0.00e+00  0
VecNorm             1717 1.0 1.5354e-01 1.2 1.72e+08 1.0 0.0e+00 0.0e+00 1.7e+03  5  9  0  0 33   5  9  0  0 33 22366       0      0 0.00e+00    0 0.00e+00  0
VecCopy                2 1.0 1.0001e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 3 1.0 2.2140e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             3431 1.0 1.5664e-01 1.1 3.43e+08 1.0 0.0e+00 0.0e+00 0.0e+00  5 18  0  0  0   5 18  0  0  0 43807       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             1714 1.0 2.1631e-01 1.0 1.71e+08 1.0 0.0e+00 0.0e+00 0.0e+00  7  9  0  0  0   7  9  0  0  0 15847       0      0 0.00e+00    0 0.00e+00  0
VecPointwiseMult    1716 1.0 2.3644e-01 1.0 8.58e+07 1.0 0.0e+00 0.0e+00 0.0e+00  7  5  0  0  0   7  5  0  0  0  7258       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     1716 1.0 1.9648e-02 1.7 0.00e+00 0.0 6.5e+04 8.0e+03 0.0e+00  1  0100100  0   1  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       1716 1.0 9.8175e-02 3.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 1.0158e-05 2.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 2.2417e-03 5.6 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      1716 1.0 1.7430e-02 1.7 0.00e+00 0.0 6.5e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        1716 1.0 9.6364e-02 3.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              1716 1.0 4.0638e-03 7.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            1716 1.0 8.0456e-04 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 6.5874e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.7610e+00 1.0 1.89e+09 1.0 6.5e+04 8.0e+03 5.1e+03 88100100100100  88100100100100 13661       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 8.0300e-07 2.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply             1716 1.0 2.4063e-01 1.0 8.58e+07 1.0 0.0e+00 0.0e+00 2.0e+00  8  5  0  0  0   8  5  0  0  0  7131       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      8013484     0.
         Vec Scatter     1              1          816     0.
              Vector     9              9      2823424     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1488     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.16e-08
Average time for MPI_Barrier(): 1.2272e-05
Average time for zero size MPI_Send(): 6.30415e-06
#PETSc Option Table entries:
-ksp_type cg
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type jacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.00061416 iterations 692
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8080 with 20 processors, by luciano.siqueira Wed Sep  9 17:31:21 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           2.539e+00     1.000   2.539e+00
Objects:              1.800e+01     1.000   1.800e+01
Flop:                 1.071e+09     1.001   1.071e+09  2.141e+10
Flop/sec:             4.218e+08     1.001   4.217e+08  8.435e+09
MPI Messages:         1.390e+03     2.000   1.320e+03  2.641e+04
MPI Message Lengths:  1.110e+07     2.000   7.983e+03  2.108e+08
MPI Reductions:       2.096e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 2.5386e+00 100.0%  2.1412e+10 100.0%  2.641e+04 100.0%  7.983e+03      100.0%  2.089e+03  99.7%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 6.7423e-0364.8 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 6.6895e-03152.1 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult              693 1.0 7.1361e-01 1.0 3.12e+08 1.0 2.6e+04 8.0e+03 0.0e+00 28 29100100  0  28 29100100  0  8732       0      0 0.00e+00    0 0.00e+00  0
MatSOR               693 1.0 1.1073e+00 1.1 3.44e+08 1.0 0.0e+00 0.0e+00 0.0e+00 41 32  0  0  0  41 32  0  0  0  6207       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 6.7404e-0369.4 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6018e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecTDot             1384 1.0 1.4017e-01 1.1 1.38e+08 1.0 0.0e+00 0.0e+00 1.4e+03  5 13  0  0 66   5 13  0  0 66 19747       0      0 0.00e+00    0 0.00e+00  0
VecNorm              694 1.0 1.4933e-01 2.4 6.94e+07 1.0 0.0e+00 0.0e+00 6.9e+02  5  6  0  0 33   5  6  0  0 33  9295       0      0 0.00e+00    0 0.00e+00  0
VecCopy                2 1.0 1.1764e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 3 1.0 2.1651e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             1385 1.0 7.3264e-02 1.1 1.38e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3 13  0  0  0   3 13  0  0  0 37808       0      0 0.00e+00    0 0.00e+00  0
VecAYPX              691 1.0 9.1894e-02 1.0 6.91e+07 1.0 0.0e+00 0.0e+00 0.0e+00  4  6  0  0  0   4  6  0  0  0 15039       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin      693 1.0 1.2538e-02 1.7 0.00e+00 0.0 2.6e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd        693 1.0 3.3848e-02 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.7550e-06 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.4785e-03 3.6 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin       693 1.0 1.1160e-02 1.8 0.00e+00 0.0 2.6e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd         693 1.0 3.2836e-02 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack               693 1.0 2.6271e-0311.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack             693 1.0 4.2878e-04 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 6.4403e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.1762e+00 1.0 1.07e+09 1.0 2.6e+04 8.0e+03 2.1e+03 86100100100 99  86100100100 99  9833       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 3.3700e-07 2.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply              693 1.0 1.1089e+00 1.1 3.44e+08 1.0 0.0e+00 0.0e+00 0.0e+00 41 32  0  0  0  41 32  0  0  0  6198       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector     8              8      2421696     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1488     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.36e-08
Average time for MPI_Barrier(): 1.20922e-05
Average time for zero size MPI_Send(): 6.50235e-06
#PETSc Option Table entries:
-ksp_type cg
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type sor
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.000603072 iterations 383
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8080 with 20 processors, by luciano.siqueira Wed Sep  9 17:31:26 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.062e+00     1.000   3.062e+00
Objects:              4.100e+01     1.000   4.100e+01
Flop:                 1.134e+09     1.001   1.133e+09  2.267e+10
Flop/sec:             3.702e+08     1.001   3.702e+08  7.403e+09
MPI Messages:         1.560e+03     2.000   1.482e+03  2.964e+04
MPI Message Lengths:  1.246e+07     2.000   7.985e+03  2.367e+08
MPI Reductions:       1.192e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.0619e+00 100.0%  2.2668e+10 100.0%  2.964e+04 100.0%  7.985e+03      100.0%  1.185e+03  99.4%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 6.5165e-03 7.9 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 4.6303e-0394.0 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult              778 1.0 9.0297e-01 1.1 3.50e+08 1.0 3.0e+04 8.0e+03 0.0e+00 28 31100100  0  28 31100100  0  7748       0      0 0.00e+00    0 0.00e+00  0
MatSOR               779 1.0 1.2828e+00 1.1 3.86e+08 1.0 0.0e+00 0.0e+00 0.0e+00 40 34  0  0  0  40 34  0  0  0  6022       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 4.6848e-0345.8 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7875e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecMDot               10 1.0 3.9379e-03 1.1 5.50e+06 1.0 0.0e+00 0.0e+00 1.0e+01  0  0  0  0  1   0  0  0  0  1 27933       0      0 0.00e+00    0 0.00e+00  0
VecTDot              766 1.0 8.5980e-02 1.1 7.66e+07 1.0 0.0e+00 0.0e+00 7.7e+02  3  7  0  0 64   3  7  0  0 65 17818       0      0 0.00e+00    0 0.00e+00  0
VecNorm              396 1.0 1.1647e-01 2.7 3.96e+07 1.0 0.0e+00 0.0e+00 4.0e+02  3  3  0  0 33   3  3  0  0 33  6800       0      0 0.00e+00    0 0.00e+00  0
VecScale              11 1.0 2.6845e-04 1.1 5.50e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 40976       0      0 0.00e+00    0 0.00e+00  0
VecCopy              771 1.0 3.4666e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet               773 1.0 6.1021e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY              768 1.0 4.6826e-02 1.1 7.68e+07 1.0 0.0e+00 0.0e+00 0.0e+00  1  7  0  0  0   1  7  0  0  0 32802       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             1150 1.0 1.6507e-01 1.1 9.58e+07 1.0 0.0e+00 0.0e+00 0.0e+00  5  8  0  0  0   5  8  0  0  0 11607       0      0 0.00e+00    0 0.00e+00  0
VecAXPBYCZ           384 1.0 7.0318e-02 1.1 9.60e+07 1.0 0.0e+00 0.0e+00 0.0e+00  2  8  0  0  0   2  8  0  0  0 27305       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY              11 1.0 5.0039e-03 1.0 6.50e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 25980       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin      778 1.0 1.4325e-02 1.5 0.00e+00 0.0 3.0e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd        778 1.0 1.1620e-01 5.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecNormalize          11 1.0 7.8814e-03 1.2 1.65e+06 1.0 0.0e+00 0.0e+00 1.1e+01  0  0  0  0  1   0  0  0  0  1  4187       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 1.0212e-05 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 3.3187e-03 7.8 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin       778 1.0 1.2926e-02 1.6 0.00e+00 0.0 3.0e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd         778 1.0 1.1514e-01 5.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack               778 1.0 2.7703e-03 9.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack             778 1.0 4.0872e-04 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.4116e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  5  2  1  1  2   5  2  1  1  2  3365       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.6825e+00 1.0 1.13e+09 1.0 3.0e+04 8.0e+03 1.2e+03 88100100100 98  88100100100 99  8446       0      0 0.00e+00    0 0.00e+00  0
KSPGMRESOrthog        10 1.0 1.2204e-02 1.0 1.10e+07 1.0 0.0e+00 0.0e+00 1.0e+01  0  1  0  0  1   0  1  0  0  1 18027       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 1.4076e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  5  2  1  1  2   5  2  1  1  2  3374       0      0 0.00e+00    0 0.00e+00  0
PCApply              395 1.0 1.9270e+00 1.0 7.13e+08 1.0 1.5e+04 8.0e+03 0.0e+00 61 63 49 49  0  61 63 49 49  0  7395       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    27             27     10054528     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     3              3        33504     0.
      Preconditioner     3              3         2936     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.84e-08
Average time for MPI_Barrier(): 1.11762e-05
Average time for zero size MPI_Send(): 6.27185e-06
#PETSc Option Table entries:
-ksp_type cg
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.0125906 iterations 6472
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8080 with 20 processors, by luciano.siqueira Wed Sep  9 17:31:55 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           2.685e+01     1.000   2.685e+01
Objects:              5.700e+01     1.000   5.700e+01
Flop:                 2.773e+10     1.000   2.773e+10  5.546e+11
Flop/sec:             1.033e+09     1.000   1.033e+09  2.065e+10
MPI Messages:         1.338e+04     2.000   1.271e+04  2.542e+05
MPI Message Lengths:  1.070e+08     2.000   7.998e+03  2.033e+09
MPI Reductions:       1.318e+04     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 2.6854e+01 100.0%  5.5456e+11 100.0%  2.542e+05 100.0%  7.998e+03      100.0%  1.317e+04  99.9%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 4.4084e-0355.8 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 4.3535e-0387.0 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             6688 1.0 6.8265e+00 1.0 3.01e+09 1.0 2.5e+05 8.0e+03 0.0e+00 25 11100100  0  25 11100100  0  8810       0      0 0.00e+00    0 0.00e+00  0
MatSolve            6688 1.0 7.3728e+00 1.0 2.98e+09 1.0 0.0e+00 0.0e+00 0.0e+00 27 11  0  0  0  27 11  0  0  0  8088       0      0 0.00e+00    0 0.00e+00  0
MatLUFactorNum         1 1.0 8.1166e-03 1.2 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1337       0      0 0.00e+00    0 0.00e+00  0
MatILUFactorSym        1 1.0 4.3317e-03 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 4.4093e-0342.6 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6691e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetRowIJ            1 1.0 2.5840e-06 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetOrdering         1 1.0 6.9660e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecMDot             6472 1.0 4.8501e+00 1.0 1.00e+10 1.0 0.0e+00 0.0e+00 6.5e+03 18 36  0  0 49  18 36  0  0 49 41330       0      0 0.00e+00    0 0.00e+00  0
VecNorm             6689 1.0 6.4176e-01 1.1 6.69e+08 1.0 0.0e+00 0.0e+00 6.7e+03  2  2  0  0 51   2  2  0  0 51 20846       0      0 0.00e+00    0 0.00e+00  0
VecScale            6688 1.0 1.3568e-01 1.1 3.34e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 49294       0      0 0.00e+00    0 0.00e+00  0
VecCopy              216 1.0 1.8748e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet              6907 1.0 4.2526e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY              432 1.0 2.0633e-02 1.0 4.32e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 41875       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY            6688 1.0 6.3577e+00 1.0 1.07e+10 1.0 0.0e+00 0.0e+00 0.0e+00 24 38  0  0  0  24 38  0  0  0 33566       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     6688 1.0 8.9410e-02 1.5 0.00e+00 0.0 2.5e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       6688 1.0 2.6845e-01 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecNormalize        6688 1.0 7.7999e-01 1.1 1.00e+09 1.0 0.0e+00 0.0e+00 6.7e+03  3  4  0  0 51   3  4  0  0 51 25724       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.5950e-06 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 2.0884e-03 5.3 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      6688 1.0 7.8966e-02 1.6 0.00e+00 0.0 2.5e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        6688 1.0 2.5966e-01 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              6688 1.0 2.0871e-02 6.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            6688 1.0 3.9966e-03 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.9524e-03 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.6484e+01 1.0 2.77e+10 1.0 2.5e+05 8.0e+03 1.3e+04 99100100100100  99100100100100 20939       0      0 0.00e+00    0 0.00e+00  0
KSPGMRESOrthog      6472 1.0 1.0813e+01 1.0 2.00e+10 1.0 0.0e+00 0.0e+00 6.5e+03 40 72  0  0 49  40 72  0  0 49 37077       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                2 1.0 1.9518e-02 1.3 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   556       0      0 0.00e+00    0 0.00e+00  0
PCSetUpOnBlocks        1 1.0 1.3209e-02 1.3 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   822       0      0 0.00e+00    0 0.00e+00  0
PCApply             6688 1.0 7.8392e+00 1.0 2.98e+09 1.0 0.0e+00 0.0e+00 0.0e+00 29 11  0  0  0  29 11  0  0  0  7607       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              4     11791508     0.
         Vec Scatter     1              1          816     0.
              Vector    41             41     14878464     0.
           Index Set     5              5       608512     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     2              2        20072     0.
      Preconditioner     2              2         1944     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.97e-08
Average time for MPI_Barrier(): 1.12408e-05
Average time for zero size MPI_Send(): 6.4547e-06
#PETSc Option Table entries:
-ksp_type gmres
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type bjacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 186.34 iterations 10000
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8080 with 20 processors, by luciano.siqueira Wed Sep  9 17:32:28 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.192e+01     1.000   3.192e+01
Objects:              5.000e+01     1.000   5.000e+01
Flop:                 3.876e+10     1.001   3.876e+10  7.752e+11
Flop/sec:             1.214e+09     1.001   1.214e+09  2.429e+10
MPI Messages:         2.067e+04     2.000   1.964e+04  3.928e+05
MPI Message Lengths:  1.654e+08     2.000   7.999e+03  3.142e+09
MPI Reductions:       2.036e+04     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.1917e+01 100.0%  7.7519e+11 100.0%  3.928e+05 100.0%  7.999e+03      100.0%  2.035e+04 100.0%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 3.1428e-0335.4 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 3.1158e-0361.5 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult            10334 1.0 1.0864e+01 1.1 4.65e+09 1.0 3.9e+05 8.0e+03 0.0e+00 33 12100100  0  33 12100100  0  8553       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 3.1691e-0330.5 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6703e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecMDot            10000 1.0 8.0332e+00 1.1 1.55e+10 1.0 0.0e+00 0.0e+00 1.0e+04 25 40  0  0 49  25 40  0  0 49 38564       0      0 0.00e+00    0 0.00e+00  0
VecNorm            10335 1.0 9.3947e-01 1.1 1.03e+09 1.0 0.0e+00 0.0e+00 1.0e+04  3  3  0  0 51   3  3  0  0 51 22002       0      0 0.00e+00    0 0.00e+00  0
VecScale           10334 1.0 2.0628e-01 1.0 5.17e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1  1  0  0  0   1  1  0  0  0 50096       0      0 0.00e+00    0 0.00e+00  0
VecCopy              334 1.0 2.8894e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet               337 1.0 2.1052e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY              668 1.0 3.1948e-02 1.0 6.68e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 41818       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY           10334 1.0 1.0350e+01 1.0 1.65e+10 1.0 0.0e+00 0.0e+00 0.0e+00 32 43  0  0  0  32 43  0  0  0 31864       0      0 0.00e+00    0 0.00e+00  0
VecPointwiseMult   10334 1.0 1.5636e+00 1.0 5.17e+08 1.0 0.0e+00 0.0e+00 0.0e+00  5  1  0  0  0   5  1  0  0  0  6609       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin    10334 1.0 1.3380e-01 1.6 0.00e+00 0.0 3.9e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd      10334 1.0 6.9360e-01 2.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecNormalize       10334 1.0 1.1567e+00 1.0 1.55e+09 1.0 0.0e+00 0.0e+00 1.0e+04  4  4  0  0 51   4  4  0  0 51 26803       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.8880e-06 2.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.3481e-03 3.2 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin     10334 1.0 1.1789e-01 1.7 0.00e+00 0.0 3.9e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd       10334 1.0 6.8074e-01 3.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack             10334 1.0 3.2348e-02 7.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack           10334 1.0 5.5905e-03 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 1.0281e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 3.1550e+01 1.0 3.88e+10 1.0 3.9e+05 8.0e+03 2.0e+04 99100100100100  99100100100100 24570       0      0 0.00e+00    0 0.00e+00  0
KSPGMRESOrthog     10000 1.0 1.7768e+01 1.0 3.10e+10 1.0 0.0e+00 0.0e+00 1.0e+04 55 80  0  0 49  55 80  0  0 49 34871       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 8.1800e-07 2.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply            10334 1.0 1.5786e+00 1.0 5.17e+08 1.0 0.0e+00 0.0e+00 2.0e+00  5  1  0  0  0   5  1  0  0  0  6546       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      8013484     0.
         Vec Scatter     1              1          816     0.
              Vector    40             40     15276992     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1        18656     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.04e-08
Average time for MPI_Barrier(): 1.10964e-05
Average time for zero size MPI_Send(): 6.2715e-06
#PETSc Option Table entries:
-ksp_type gmres
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type jacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.0487126 iterations 10000
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8080 with 20 processors, by luciano.siqueira Wed Sep  9 17:33:17 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           4.674e+01     1.000   4.674e+01
Objects:              4.900e+01     1.000   4.900e+01
Flop:                 4.337e+10     1.000   4.337e+10  8.673e+11
Flop/sec:             9.279e+08     1.000   9.279e+08  1.856e+10
MPI Messages:         2.067e+04     2.000   1.964e+04  3.928e+05
MPI Message Lengths:  1.654e+08     2.000   7.999e+03  3.142e+09
MPI Reductions:       2.035e+04     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 4.6738e+01 100.0%  8.6733e+11 100.0%  3.928e+05 100.0%  7.999e+03      100.0%  2.035e+04 100.0%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 3.6055e-0341.2 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 3.5624e-0365.0 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult            10334 1.0 1.0490e+01 1.0 4.65e+09 1.0 3.9e+05 8.0e+03 0.0e+00 22 11100100  0  22 11100100  0  8858       0      0 0.00e+00    0 0.00e+00  0
MatSOR             10334 1.0 1.5664e+01 1.0 5.12e+09 1.0 0.0e+00 0.0e+00 0.0e+00 33 12  0  0  0  33 12  0  0  0  6542       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 3.6151e-0333.7 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6352e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecMDot            10000 1.0 8.6954e+00 1.0 1.55e+10 1.0 0.0e+00 0.0e+00 1.0e+04 18 36  0  0 49  18 36  0  0 49 35628       0      0 0.00e+00    0 0.00e+00  0
VecNorm            10335 1.0 1.0292e+00 1.1 1.03e+09 1.0 0.0e+00 0.0e+00 1.0e+04  2  2  0  0 51   2  2  0  0 51 20083       0      0 0.00e+00    0 0.00e+00  0
VecScale           10334 1.0 2.1088e-01 1.0 5.17e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 49005       0      0 0.00e+00    0 0.00e+00  0
VecCopy              334 1.0 2.8634e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet               337 1.0 2.1086e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY              668 1.0 3.2023e-02 1.1 6.68e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 41720       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY           10334 1.0 1.0584e+01 1.0 1.65e+10 1.0 0.0e+00 0.0e+00 0.0e+00 23 38  0  0  0  23 38  0  0  0 31159       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin    10334 1.0 1.4673e-01 1.5 0.00e+00 0.0 3.9e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd      10334 1.0 3.7151e-01 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecNormalize       10334 1.0 1.2515e+00 1.1 1.55e+09 1.0 0.0e+00 0.0e+00 1.0e+04  3  4  0  0 51   3  4  0  0 51 24772       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.7760e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.2969e-03 3.1 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin     10334 1.0 1.2938e-01 1.6 0.00e+00 0.0 3.9e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd       10334 1.0 3.5921e-01 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack             10334 1.0 3.7098e-02 5.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack           10334 1.0 6.0184e-03 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 1.0649e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 4.6372e+01 1.0 4.34e+10 1.0 3.9e+05 8.0e+03 2.0e+04 99100100100100  99100100100100 18703       0      0 0.00e+00    0 0.00e+00  0
KSPGMRESOrthog     10000 1.0 1.8648e+01 1.0 3.10e+10 1.0 0.0e+00 0.0e+00 1.0e+04 40 71  0  0 49  40 71  0  0 49 33226       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 3.6100e-07 2.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply            10334 1.0 1.5680e+01 1.0 5.12e+09 1.0 0.0e+00 0.0e+00 0.0e+00 33 12  0  0  0  33 12  0  0  0  6535       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    39             39     14875264     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1        18656     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.13e-08
Average time for MPI_Barrier(): 1.06814e-05
Average time for zero size MPI_Send(): 8.72765e-06
#PETSc Option Table entries:
-ksp_type gmres
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type sor
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.00984153 iterations 3132
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8080 with 20 processors, by luciano.siqueira Wed Sep  9 17:33:44 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           2.496e+01     1.000   2.496e+01
Objects:              7.200e+01     1.000   7.200e+01
Flop:                 1.795e+10     1.001   1.795e+10  3.589e+11
Flop/sec:             7.190e+08     1.001   7.189e+08  1.438e+10
MPI Messages:         1.297e+04     2.000   1.232e+04  2.465e+05
MPI Message Lengths:  1.038e+08     2.000   7.998e+03  1.971e+09
MPI Reductions:       6.411e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 2.4962e+01 100.0%  3.5893e+11 100.0%  2.465e+05 100.0%  7.998e+03      100.0%  6.404e+03  99.9%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 4.6387e-0350.9 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 4.5755e-0395.9 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             6484 1.0 6.6522e+00 1.0 2.92e+09 1.0 2.5e+05 8.0e+03 0.0e+00 26 16100100  0  26 16100100  0  8765       0      0 0.00e+00    0 0.00e+00  0
MatSOR              6485 1.0 9.8671e+00 1.0 3.22e+09 1.0 0.0e+00 0.0e+00 0.0e+00 39 18  0  0  0  39 18  0  0  0  6517       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 4.6308e-0345.1 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6489e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecMDot             3142 1.0 2.8243e+00 1.1 4.85e+09 1.0 0.0e+00 0.0e+00 3.1e+03 11 27  0  0 49  11 27  0  0 49 34340       0      0 0.00e+00    0 0.00e+00  0
VecNorm             3249 1.0 3.3513e-01 1.1 3.25e+08 1.0 0.0e+00 0.0e+00 3.2e+03  1  2  0  0 51   1  2  0  0 51 19389       0      0 0.00e+00    0 0.00e+00  0
VecScale            3248 1.0 6.5445e-02 1.0 1.62e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 49630       0      0 0.00e+00    0 0.00e+00  0
VecCopy             6580 1.0 3.4114e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet              6584 1.0 2.5793e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY              211 1.0 1.0080e-02 1.0 2.11e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 41867       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             6474 1.0 8.7942e-01 1.0 4.86e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3  3  0  0  0   3  3  0  0  0 11042       0      0 0.00e+00    0 0.00e+00  0
VecAXPBYCZ          3237 1.0 5.4868e-01 1.0 8.09e+08 1.0 0.0e+00 0.0e+00 0.0e+00  2  5  0  0  0   2  5  0  0  0 29498       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY            3248 1.0 3.0982e+00 1.0 5.16e+09 1.0 0.0e+00 0.0e+00 0.0e+00 12 29  0  0  0  12 29  0  0  0 33332       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     6484 1.0 1.0084e-01 1.5 0.00e+00 0.0 2.5e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       6484 1.0 3.1453e-01 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecNormalize        3248 1.0 4.0436e-01 1.1 4.87e+08 1.0 0.0e+00 0.0e+00 3.2e+03  2  3  0  0 51   2  3  0  0 51 24097       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.6980e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.1806e-03 3.0 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      6484 1.0 8.8182e-02 1.5 0.00e+00 0.0 2.5e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        6484 1.0 3.0686e-01 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              6484 1.0 2.1504e-02 7.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            6484 1.0 3.4216e-03 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.4250e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  1  0  0  0  0   1  0  0  0  0  3333       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.4598e+01 1.0 1.79e+10 1.0 2.5e+05 8.0e+03 6.4e+03 99100100100100  99100100100100 14591       0      0 0.00e+00    0 0.00e+00  0
KSPGMRESOrthog      3142 1.0 5.7450e+00 1.1 9.70e+09 1.0 0.0e+00 0.0e+00 3.1e+03 23 54  0  0 49  23 54  0  0 49 33763       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 1.4172e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  1  0  0  0  0   1  0  0  0  0  3352       0      0 0.00e+00    0 0.00e+00  0
PCApply             3248 1.0 1.5184e+01 1.0 5.97e+09 1.0 1.2e+05 8.0e+03 0.0e+00 60 33 50 50  0  60 33 50 50  0  7857       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    58             58     22508096     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     3              3        50672     0.
      Preconditioner     3              3         2936     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.78e-08
Average time for MPI_Barrier(): 1.11436e-05
Average time for zero size MPI_Send(): 6.21555e-06
#PETSc Option Table entries:
-ksp_type gmres
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.000101716 iterations 641
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8080 with 20 processors, by luciano.siqueira Wed Sep  9 17:33:49 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.245e+00     1.000   3.245e+00
Objects:              8.700e+01     1.000   8.700e+01
Flop:                 2.892e+09     1.000   2.892e+09  5.784e+10
Flop/sec:             8.912e+08     1.000   8.912e+08  1.782e+10
MPI Messages:         1.288e+03     2.000   1.224e+03  2.447e+04
MPI Message Lengths:  1.028e+07     2.000   7.981e+03  1.953e+08
MPI Reductions:       2.583e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.2454e+00 100.0%  5.7844e+10 100.0%  2.447e+04 100.0%  7.981e+03      100.0%  2.576e+03  99.7%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 3.9018e-0351.8 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 3.8739e-0378.9 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult              642 1.0 6.8468e-01 1.1 2.89e+08 1.0 2.4e+04 8.0e+03 0.0e+00 20 10100100  0  20 10100100  0  8431       0      0 0.00e+00    0 0.00e+00  0
MatSolve             642 1.0 7.0574e-01 1.0 2.86e+08 1.0 0.0e+00 0.0e+00 0.0e+00 22 10  0  0  0  22 10  0  0  0  8111       0      0 0.00e+00    0 0.00e+00  0
MatLUFactorNum         1 1.0 7.5337e-03 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1441       0      0 0.00e+00    0 0.00e+00  0
MatILUFactorSym        1 1.0 4.3995e-03 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 3.9281e-0339.7 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.5874e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetRowIJ            1 1.0 2.9780e-06 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetOrdering         1 1.0 7.0074e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecTDot             1282 1.0 1.5433e-01 1.4 1.28e+08 1.0 0.0e+00 0.0e+00 1.3e+03  4  4  0  0 50   4  4  0  0 50 16613       0      0 0.00e+00    0 0.00e+00  0
VecMTDot             640 1.0 5.2987e-01 1.0 9.82e+08 1.0 0.0e+00 0.0e+00 6.4e+02 16 34  0  0 25  16 34  0  0 25 37065       0      0 0.00e+00    0 0.00e+00  0
VecNorm              643 1.0 7.5233e-02 1.3 6.43e+07 1.0 0.0e+00 0.0e+00 6.4e+02  2  2  0  0 25   2  2  0  0 25 17094       0      0 0.00e+00    0 0.00e+00  0
VecScale             640 1.0 2.2573e-02 1.1 3.20e+07 1.0 0.0e+00 0.0e+00 0.0e+00  1  1  0  0  0   1  1  0  0  0 28352       0      0 0.00e+00    0 0.00e+00  0
VecCopy              642 1.0 3.9418e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet               645 1.0 1.3939e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             1283 1.0 6.7809e-02 1.0 1.28e+08 1.0 0.0e+00 0.0e+00 0.0e+00  2  4  0  0  0   2  4  0  0  0 37842       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY             640 1.0 6.0534e-01 1.0 9.82e+08 1.0 0.0e+00 0.0e+00 0.0e+00 19 34  0  0  0  19 34  0  0  0 32445       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin      642 1.0 1.0773e-02 1.4 0.00e+00 0.0 2.4e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd        642 1.0 4.6649e-02 3.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.5440e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 2.1014e-03 5.4 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin       642 1.0 9.7703e-03 1.5 0.00e+00 0.0 2.4e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd         642 1.0 4.5804e-02 3.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack               642 1.0 1.9763e-03 7.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack             642 1.0 3.8933e-04 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 5.4264e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.8842e+00 1.0 2.89e+09 1.0 2.4e+04 8.0e+03 2.6e+03 89100100100 99  89100100100100 20051       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                2 1.0 2.4874e-02 1.7 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0   436       0      0 0.00e+00    0 0.00e+00  0
PCSetUpOnBlocks        1 1.0 1.2644e-02 1.2 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   858       0      0 0.00e+00    0 0.00e+00  0
PCApply              642 1.0 7.2609e-01 1.0 2.86e+08 1.0 0.0e+00 0.0e+00 0.0e+00 22 10  0  0  0  22 10  0  0  0  7883       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              4     11791508     0.
         Vec Scatter     1              1          816     0.
              Vector    71             71     26930304     0.
           Index Set     5              5       608512     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     2              2         4080     0.
      Preconditioner     2              2         1944     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.92e-08
Average time for MPI_Barrier(): 1.16106e-05
Average time for zero size MPI_Send(): 6.50695e-06
#PETSc Option Table entries:
-ksp_type fcg
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type bjacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 4.69193e-05 iterations 1715
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8080 with 20 processors, by luciano.siqueira Wed Sep  9 17:33:57 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           6.261e+00     1.000   6.261e+00
Objects:              8.000e+01     1.000   8.000e+01
Flop:                 7.104e+09     1.000   7.104e+09  1.421e+11
Flop/sec:             1.135e+09     1.000   1.135e+09  2.269e+10
MPI Messages:         3.436e+03     2.000   3.264e+03  6.528e+04
MPI Message Lengths:  2.746e+07     2.000   7.993e+03  5.218e+08
MPI Reductions:       6.881e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 6.2611e+00 100.0%  1.4208e+11 100.0%  6.528e+04 100.0%  7.993e+03      100.0%  6.874e+03  99.9%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 5.1324e-0354.9 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 5.0976e-03113.3 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             1716 1.0 1.7643e+00 1.0 7.72e+08 1.0 6.5e+04 8.0e+03 0.0e+00 28 11100100  0  28 11100100  0  8746       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 5.1475e-0351.3 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6526e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecTDot             3430 1.0 3.2616e-01 1.2 3.43e+08 1.0 0.0e+00 0.0e+00 3.4e+03  5  5  0  0 50   5  5  0  0 50 21033       0      0 0.00e+00    0 0.00e+00  0
VecMTDot            1714 1.0 1.3636e+00 1.0 2.65e+09 1.0 0.0e+00 0.0e+00 1.7e+03 22 37  0  0 25  22 37  0  0 25 38890       0      0 0.00e+00    0 0.00e+00  0
VecNorm             1717 1.0 1.4440e-01 1.1 1.72e+08 1.0 0.0e+00 0.0e+00 1.7e+03  2  2  0  0 25   2  2  0  0 25 23782       0      0 0.00e+00    0 0.00e+00  0
VecScale            1714 1.0 5.1043e-02 1.2 8.57e+07 1.0 0.0e+00 0.0e+00 0.0e+00  1  1  0  0  0   1  1  0  0  0 33580       0      0 0.00e+00    0 0.00e+00  0
VecCopy             1716 1.0 1.1082e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 3 1.0 2.2195e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             3431 1.0 1.8460e-01 1.0 3.43e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3  5  0  0  0   3  5  0  0  0 37173       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY            1714 1.0 1.7268e+00 1.0 2.65e+09 1.0 0.0e+00 0.0e+00 0.0e+00 27 37  0  0  0  27 37  0  0  0 30711       0      0 0.00e+00    0 0.00e+00  0
VecPointwiseMult    1716 1.0 2.4459e-01 1.0 8.58e+07 1.0 0.0e+00 0.0e+00 0.0e+00  4  1  0  0  0   4  1  0  0  0  7016       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     1716 1.0 2.3628e-02 1.4 0.00e+00 0.0 6.5e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       1716 1.0 6.9560e-02 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.7940e-06 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.2109e-03 3.1 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      1716 1.0 2.0868e-02 1.5 0.00e+00 0.0 6.5e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        1716 1.0 6.7620e-02 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              1716 1.0 5.4577e-03 9.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            1716 1.0 1.2678e-03 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 4.6061e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 5.8905e+00 1.0 7.10e+09 1.0 6.5e+04 8.0e+03 6.9e+03 94100100100100  94100100100100 24117       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 7.8300e-07 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply             1716 1.0 2.4880e-01 1.0 8.58e+07 1.0 0.0e+00 0.0e+00 2.0e+00  4  1  0  0  0   4  1  0  0  0  6897       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      8013484     0.
         Vec Scatter     1              1          816     0.
              Vector    70             70     27328832     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         2664     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.05e-08
Average time for MPI_Barrier(): 1.25826e-05
Average time for zero size MPI_Send(): 6.51635e-06
#PETSc Option Table entries:
-ksp_type fcg
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type jacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.000614161 iterations 692
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8080 with 20 processors, by luciano.siqueira Wed Sep  9 17:34:03 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.899e+00     1.000   3.899e+00
Objects:              7.900e+01     1.000   7.900e+01
Flop:                 3.175e+09     1.000   3.175e+09  6.351e+10
Flop/sec:             8.144e+08     1.000   8.143e+08  1.629e+10
MPI Messages:         1.390e+03     2.000   1.320e+03  2.641e+04
MPI Message Lengths:  1.110e+07     2.000   7.983e+03  2.108e+08
MPI Reductions:       2.787e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.8992e+00 100.0%  6.3505e+10 100.0%  2.641e+04 100.0%  7.983e+03      100.0%  2.780e+03  99.7%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 4.3850e-0341.1 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 4.3288e-0394.3 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult              693 1.0 7.1144e-01 1.0 3.12e+08 1.0 2.6e+04 8.0e+03 0.0e+00 18 10100100  0  18 10100100  0  8759       0      0 0.00e+00    0 0.00e+00  0
MatSOR               693 1.0 1.0531e+00 1.0 3.44e+08 1.0 0.0e+00 0.0e+00 0.0e+00 27 11  0  0  0  27 11  0  0  0  6526       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 4.3788e-0344.3 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6795e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecTDot             1384 1.0 1.5477e-01 1.1 1.38e+08 1.0 0.0e+00 0.0e+00 1.4e+03  4  4  0  0 50   4  4  0  0 50 17885       0      0 0.00e+00    0 0.00e+00  0
VecMTDot             691 1.0 6.7161e-01 1.0 1.07e+09 1.0 0.0e+00 0.0e+00 6.9e+02 17 34  0  0 25  17 34  0  0 25 31852       0      0 0.00e+00    0 0.00e+00  0
VecNorm              694 1.0 9.3561e-02 1.3 6.94e+07 1.0 0.0e+00 0.0e+00 6.9e+02  2  2  0  0 25   2  2  0  0 25 14835       0      0 0.00e+00    0 0.00e+00  0
VecScale             691 1.0 3.1349e-02 1.0 3.46e+07 1.0 0.0e+00 0.0e+00 0.0e+00  1  1  0  0  0   1  1  0  0  0 22042       0      0 0.00e+00    0 0.00e+00  0
VecCopy              693 1.0 4.4051e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 3 1.0 2.1694e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             1385 1.0 8.1793e-02 1.1 1.38e+08 1.0 0.0e+00 0.0e+00 0.0e+00  2  4  0  0  0   2  4  0  0  0 33866       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY             691 1.0 7.0378e-01 1.0 1.07e+09 1.0 0.0e+00 0.0e+00 0.0e+00 18 34  0  0  0  18 34  0  0  0 30396       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin      693 1.0 1.2511e-02 1.4 0.00e+00 0.0 2.6e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd        693 1.0 2.4966e-02 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.9270e-06 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 2.4573e-0311.0 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin       693 1.0 1.1251e-02 1.5 0.00e+00 0.0 2.6e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd         693 1.0 2.4056e-02 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack               693 1.0 2.4710e-03 8.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack             693 1.0 4.4542e-04 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 6.3434e-03 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 3.5301e+00 1.0 3.17e+09 1.0 2.6e+04 8.0e+03 2.8e+03 90100100100 99  90100100100100 17986       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 4.6100e-07 2.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply              693 1.0 1.0546e+00 1.0 3.44e+08 1.0 0.0e+00 0.0e+00 0.0e+00 27 11  0  0  0  27 11  0  0  0  6517       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    69             69     26927104     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         2664     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.14e-08
Average time for MPI_Barrier(): 1.17772e-05
Average time for zero size MPI_Send(): 6.51835e-06
#PETSc Option Table entries:
-ksp_type fcg
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type sor
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.000603072 iterations 383
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8080 with 20 processors, by luciano.siqueira Wed Sep  9 17:34:09 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.834e+00     1.000   3.834e+00
Objects:              1.020e+02     1.000   1.020e+02
Flop:                 2.281e+09     1.001   2.281e+09  4.562e+10
Flop/sec:             5.950e+08     1.001   5.950e+08  1.190e+10
MPI Messages:         1.560e+03     2.000   1.482e+03  2.964e+04
MPI Message Lengths:  1.246e+07     2.000   7.985e+03  2.367e+08
MPI Reductions:       1.574e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.8338e+00 100.0%  4.5618e+10 100.0%  2.964e+04 100.0%  7.985e+03      100.0%  1.567e+03  99.6%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 4.5338e-0341.0 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 4.5151e-0381.3 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult              778 1.0 8.9085e-01 1.1 3.50e+08 1.0 3.0e+04 8.0e+03 0.0e+00 23 15100100  0  23 15100100  0  7853       0      0 0.00e+00    0 0.00e+00  0
MatSOR               779 1.0 1.2637e+00 1.1 3.86e+08 1.0 0.0e+00 0.0e+00 0.0e+00 32 17  0  0  0  32 17  0  0  0  6113       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 4.5679e-0342.1 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7070e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecMDot               10 1.0 4.8280e-03 1.5 5.50e+06 1.0 0.0e+00 0.0e+00 1.0e+01  0  0  0  0  1   0  0  0  0  1 22783       0      0 0.00e+00    0 0.00e+00  0
VecTDot              766 1.0 1.0910e-01 1.3 7.66e+07 1.0 0.0e+00 0.0e+00 7.7e+02  3  3  0  0 49   3  3  0  0 49 14042       0      0 0.00e+00    0 0.00e+00  0
VecMTDot             382 1.0 3.8275e-01 1.0 5.83e+08 1.0 0.0e+00 0.0e+00 3.8e+02 10 26  0  0 24  10 26  0  0 24 30479       0      0 0.00e+00    0 0.00e+00  0
VecNorm              396 1.0 1.0948e-01 2.2 3.96e+07 1.0 0.0e+00 0.0e+00 4.0e+02  2  2  0  0 25   2  2  0  0 25  7234       0      0 0.00e+00    0 0.00e+00  0
VecScale             393 1.0 2.1873e-02 1.0 1.96e+07 1.0 0.0e+00 0.0e+00 0.0e+00  1  1  0  0  0   1  1  0  0  0 17968       0      0 0.00e+00    0 0.00e+00  0
VecCopy             1153 1.0 6.1819e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet               773 1.0 1.8546e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY              768 1.0 4.7328e-02 1.1 7.68e+07 1.0 0.0e+00 0.0e+00 0.0e+00  1  3  0  0  0   1  3  0  0  0 32455       0      0 0.00e+00    0 0.00e+00  0
VecAYPX              768 1.0 1.1038e-01 1.1 5.76e+07 1.0 0.0e+00 0.0e+00 0.0e+00  3  3  0  0  0   3  3  0  0  0 10437       0      0 0.00e+00    0 0.00e+00  0
VecAXPBYCZ           384 1.0 6.8898e-02 1.1 9.60e+07 1.0 0.0e+00 0.0e+00 0.0e+00  2  4  0  0  0   2  4  0  0  0 27867       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY             393 1.0 3.9106e-01 1.1 5.90e+08 1.0 0.0e+00 0.0e+00 0.0e+00 10 26  0  0  0  10 26  0  0  0 30164       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin      778 1.0 1.4228e-02 1.3 0.00e+00 0.0 3.0e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd        778 1.0 1.1568e-01 6.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecNormalize          11 1.0 7.4031e-03 1.1 1.65e+06 1.0 0.0e+00 0.0e+00 1.1e+01  0  0  0  0  1   0  0  0  0  1  4458       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.5190e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 2.4762e-0314.3 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin       778 1.0 1.2779e-02 1.4 0.00e+00 0.0 3.0e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd         778 1.0 1.1465e-01 6.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack               778 1.0 2.6495e-03 9.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack             778 1.0 4.6194e-04 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.4303e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  4  1  1  1  1   4  1  1  1  1  3321       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 3.4567e+00 1.0 2.28e+09 1.0 3.0e+04 8.0e+03 1.6e+03 90100100100 99  90100100100 99 13193       0      0 0.00e+00    0 0.00e+00  0
KSPGMRESOrthog        10 1.0 1.2804e-02 1.1 1.10e+07 1.0 0.0e+00 0.0e+00 1.0e+01  0  0  0  0  1   0  0  0  0  1 17182       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 1.3885e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  4  1  1  1  1   4  1  1  1  1  3421       0      0 0.00e+00    0 0.00e+00  0
PCApply              395 1.0 1.9144e+00 1.0 7.13e+08 1.0 1.5e+04 8.0e+03 0.0e+00 49 31 49 49  0  49 31 49 49  0  7444       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    88             88     34559936     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     3              3        34680     0.
      Preconditioner     3              3         2936     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.86e-08
Average time for MPI_Barrier(): 1.80864e-05
Average time for zero size MPI_Send(): 6.40885e-06
#PETSc Option Table entries:
-ksp_type fcg
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 8.47251e-05 iterations 1671
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8080 with 20 processors, by luciano.siqueira Wed Sep  9 17:34:24 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           1.390e+01     1.000   1.390e+01
Objects:              4.000e+01     1.000   4.000e+01
Flop:                 7.332e+09     1.001   7.331e+09  1.466e+11
Flop/sec:             5.275e+08     1.001   5.275e+08  1.055e+10
MPI Messages:         1.003e+04     2.000   9.530e+03  1.906e+05
MPI Message Lengths:  8.023e+07     2.000   7.998e+03  1.524e+09
MPI Reductions:       8.375e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 1.3898e+01 100.0%  1.4662e+11 100.0%  1.906e+05 100.0%  7.998e+03      100.0%  8.368e+03  99.9%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 2.9142e-0335.4 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 2.4335e-0347.7 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             5014 1.0 5.1004e+00 1.0 2.26e+09 1.0 1.9e+05 8.0e+03 0.0e+00 36 31100100  0  36 31100100  0  8840       0      0 0.00e+00    0 0.00e+00  0
MatSolve            5014 1.0 5.4922e+00 1.0 2.24e+09 1.0 0.0e+00 0.0e+00 0.0e+00 39 30  0  0  0  39 30  0  0  0  8140       0      0 0.00e+00    0 0.00e+00  0
MatLUFactorNum         1 1.0 7.5246e-03 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1442       0      0 0.00e+00    0 0.00e+00  0
MatILUFactorSym        1 1.0 4.3867e-03 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 2.4900e-0324.1 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6783e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetRowIJ            1 1.0 2.8410e-06 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetOrdering         1 1.0 6.9813e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot              5013 1.0 4.9967e-01 1.1 5.01e+08 1.0 0.0e+00 0.0e+00 5.0e+03  3  7  0  0 60   3  7  0  0 60 20065       0      0 0.00e+00    0 0.00e+00  0
VecNorm             3344 1.0 3.6763e-01 1.4 3.34e+08 1.0 0.0e+00 0.0e+00 3.3e+03  2  5  0  0 40   2  5  0  0 40 18192       0      0 0.00e+00    0 0.00e+00  0
VecScale            6685 1.0 1.4059e-01 1.1 3.34e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1  5  0  0  0   1  5  0  0  0 47548       0      0 0.00e+00    0 0.00e+00  0
VecCopy            15043 1.0 6.8787e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  5  0  0  0  0   5  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet              5022 1.0 3.0657e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY            15037 1.0 8.1195e-01 1.0 1.50e+09 1.0 0.0e+00 0.0e+00 0.0e+00  6 21  0  0  0   6 21  0  0  0 37039       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             1671 1.0 2.2199e-01 1.0 1.67e+08 1.0 0.0e+00 0.0e+00 0.0e+00  2  2  0  0  0   2  2  0  0  0 15055       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     5014 1.0 8.1834e-02 1.7 0.00e+00 0.0 1.9e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       5014 1.0 1.9120e-01 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 1.0086e-05 2.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.3670e-03 3.3 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      5014 1.0 7.2708e-02 1.9 0.00e+00 0.0 1.9e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        5014 1.0 1.8559e-01 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              5014 1.0 1.8355e-02 6.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            5014 1.0 2.9317e-03 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 4.4613e-03 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 1.3528e+01 1.0 7.33e+09 1.0 1.9e+05 8.0e+03 8.4e+03 97100100100100  97100100100100 10837       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                2 1.0 2.6656e-02 1.2 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   407       0      0 0.00e+00    0 0.00e+00  0
PCSetUpOnBlocks        1 1.0 1.2673e-02 1.2 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   856       0      0 0.00e+00    0 0.00e+00  0
PCApply             5014 1.0 5.8306e+00 1.0 2.24e+09 1.0 0.0e+00 0.0e+00 0.0e+00 42 30  0  0  0  42 30  0  0  0  7667       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              4     11791508     0.
         Vec Scatter     1              1          816     0.
              Vector    24             24      8049088     0.
           Index Set     5              5       608512     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     2              2         2832     0.
      Preconditioner     2              2         1944     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.96e-08
Average time for MPI_Barrier(): 1.12036e-05
Average time for zero size MPI_Send(): 6.4137e-06
#PETSc Option Table entries:
-ksp_type tcqmr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type bjacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 2.62209e-05 iterations 6622
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8080 with 20 processors, by luciano.siqueira Wed Sep  9 17:35:00 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.347e+01     1.000   3.347e+01
Objects:              3.300e+01     1.000   3.300e+01
Flop:                 2.119e+10     1.002   2.118e+10  4.237e+11
Flop/sec:             6.331e+08     1.002   6.329e+08  1.266e+10
MPI Messages:         3.974e+04     2.000   3.775e+04  7.550e+05
MPI Message Lengths:  3.179e+08     2.000   7.999e+03  6.040e+09
MPI Reductions:       3.313e+04     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.3467e+01 100.0%  4.2366e+11 100.0%  7.550e+05 100.0%  7.999e+03      100.0%  3.312e+04 100.0%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 4.7159e-0334.4 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 4.6795e-0392.9 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult            19867 1.0 2.0123e+01 1.0 8.94e+09 1.0 7.5e+05 8.0e+03 0.0e+00 60 42100100  0  60 42100100  0  8878       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 4.7308e-0344.9 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6172e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot             19866 1.0 1.8376e+00 1.1 1.99e+09 1.0 0.0e+00 0.0e+00 2.0e+04  5  9  0  0 60   5  9  0  0 60 21621       0      0 0.00e+00    0 0.00e+00  0
VecNorm            13246 1.0 1.2546e+00 1.2 1.32e+09 1.0 0.0e+00 0.0e+00 1.3e+04  3  6  0  0 40   3  6  0  0 40 21116       0      0 0.00e+00    0 0.00e+00  0
VecScale           26489 1.0 4.9466e-01 1.0 1.32e+09 1.0 0.0e+00 0.0e+00 0.0e+00  1  6  0  0  0   1  6  0  0  0 53550       0      0 0.00e+00    0 0.00e+00  0
VecCopy            59602 1.0 2.7524e+00 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  8  0  0  0  0   8  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 8 1.0 5.3117e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY            59596 1.0 3.2641e+00 1.0 5.96e+09 1.0 0.0e+00 0.0e+00 0.0e+00 10 28  0  0  0  10 28  0  0  0 36516       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             6622 1.0 8.7136e-01 1.0 6.62e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3  3  0  0  0   3  3  0  0  0 15199       0      0 0.00e+00    0 0.00e+00  0
VecPointwiseMult   19867 1.0 2.8707e+00 1.0 9.93e+08 1.0 0.0e+00 0.0e+00 0.0e+00  9  5  0  0  0   9  5  0  0  0  6921       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin    19867 1.0 2.4977e-01 1.4 0.00e+00 0.0 7.5e+05 8.0e+03 0.0e+00  1  0100100  0   1  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd      19867 1.0 6.7155e-01 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.7260e-06 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 2.1235e-03 5.4 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin     19867 1.0 2.2211e-01 1.5 0.00e+00 0.0 7.5e+05 8.0e+03 0.0e+00  1  0100100  0   1  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd       19867 1.0 6.5176e-01 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack             19867 1.0 6.7443e-0210.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack           19867 1.0 8.6580e-03 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 3.5186e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 3.3103e+01 1.0 2.12e+10 1.0 7.5e+05 8.0e+03 3.3e+04 99100100100100  99100100100100 12798       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 8.5600e-07 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply            19867 1.0 2.8922e+00 1.0 9.93e+08 1.0 0.0e+00 0.0e+00 2.0e+00  9  5  0  0  0   9  5  0  0  0  6869       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      8013484     0.
         Vec Scatter     1              1          816     0.
              Vector    23             23      8447616     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1416     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.96e-08
Average time for MPI_Barrier(): 1.63416e-05
Average time for zero size MPI_Send(): 6.5453e-06
#PETSc Option Table entries:
-ksp_type tcqmr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type jacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.000177372 iterations 1033
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8080 with 20 processors, by luciano.siqueira Wed Sep  9 17:35:12 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           1.013e+01     1.000   1.013e+01
Objects:              3.200e+01     1.000   3.200e+01
Flop:                 4.688e+09     1.001   4.687e+09  9.374e+10
Flop/sec:             4.626e+08     1.001   4.625e+08  9.250e+09
MPI Messages:         6.204e+03     2.000   5.894e+03  1.179e+05
MPI Message Lengths:  4.961e+07     2.000   7.996e+03  9.426e+08
MPI Reductions:       5.185e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 1.0134e+01 100.0%  9.3739e+10 100.0%  1.179e+05 100.0%  7.996e+03      100.0%  5.178e+03  99.9%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 5.9944e-0373.3 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 5.9212e-03122.9 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             3100 1.0 3.2315e+00 1.0 1.39e+09 1.0 1.2e+05 8.0e+03 0.0e+00 32 30100100  0  32 30100100  0  8626       0      0 0.00e+00    0 0.00e+00  0
MatSOR              3100 1.0 4.7774e+00 1.0 1.54e+09 1.0 0.0e+00 0.0e+00 0.0e+00 46 33  0  0  0  46 33  0  0  0  6435       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 5.9740e-0358.3 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6310e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot              3099 1.0 4.1319e-01 1.3 3.10e+08 1.0 0.0e+00 0.0e+00 3.1e+03  4  7  0  0 60   4  7  0  0 60 15000       0      0 0.00e+00    0 0.00e+00  0
VecNorm             2068 1.0 3.5611e-01 1.8 2.07e+08 1.0 0.0e+00 0.0e+00 2.1e+03  3  4  0  0 40   3  4  0  0 40 11614       0      0 0.00e+00    0 0.00e+00  0
VecScale            4133 1.0 9.0596e-02 1.1 2.07e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1  4  0  0  0   1  4  0  0  0 45620       0      0 0.00e+00    0 0.00e+00  0
VecCopy             9301 1.0 4.9181e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  5  0  0  0  0   5  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 8 1.0 5.3085e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             9295 1.0 5.4452e-01 1.0 9.30e+08 1.0 0.0e+00 0.0e+00 0.0e+00  5 20  0  0  0   5 20  0  0  0 34140       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             1033 1.0 1.4433e-01 1.0 1.03e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1  2  0  0  0   1  2  0  0  0 14315       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     3100 1.0 5.2584e-02 1.6 0.00e+00 0.0 1.2e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       3100 1.0 1.4144e-01 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 1.0053e-05 2.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.1970e-03 3.1 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      3100 1.0 4.6972e-02 1.7 0.00e+00 0.0 1.2e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        3100 1.0 1.3784e-01 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              3100 1.0 1.4228e-0211.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            3100 1.0 1.4987e-03 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 5.2809e-03 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 9.7666e+00 1.0 4.69e+09 1.0 1.2e+05 8.0e+03 5.2e+03 96100100100100  96100100100100  9597       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 3.9300e-07 2.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply             3100 1.0 4.7831e+00 1.0 1.54e+09 1.0 0.0e+00 0.0e+00 0.0e+00 46 33  0  0  0  46 33  0  0  0  6427       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    22             22      8045888     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1416     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.07e-08
Average time for MPI_Barrier(): 1.06786e-05
Average time for zero size MPI_Send(): 6.41715e-06
#PETSc Option Table entries:
-ksp_type tcqmr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type sor
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.000147819 iterations 474
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8080 with 20 processors, by luciano.siqueira Wed Sep  9 17:35:23 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           9.524e+00     1.000   9.524e+00
Objects:              5.500e+01     1.000   5.500e+01
Flop:                 4.090e+09     1.001   4.089e+09  8.179e+10
Flop/sec:             4.294e+08     1.001   4.294e+08  8.587e+09
MPI Messages:         5.716e+03     2.000   5.430e+03  1.086e+05
MPI Message Lengths:  4.570e+07     2.000   7.996e+03  8.684e+08
MPI Reductions:       2.413e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 9.5241e+00 100.0%  8.1788e+10 100.0%  1.086e+05 100.0%  7.996e+03      100.0%  2.406e+03  99.7%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 6.4920e-0397.2 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 4.9546e-03107.5 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             2856 1.0 2.9843e+00 1.0 1.28e+09 1.0 1.1e+05 8.0e+03 0.0e+00 31 31100100  0  31 31100100  0  8605       0      0 0.00e+00    0 0.00e+00  0
MatSOR              2857 1.0 4.4038e+00 1.0 1.42e+09 1.0 0.0e+00 0.0e+00 0.0e+00 45 35  0  0  0  45 35  0  0  0  6433       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 5.0054e-0350.7 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6630e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot              1422 1.0 1.8633e-01 1.2 1.42e+08 1.0 0.0e+00 0.0e+00 1.4e+03  2  3  0  0 59   2  3  0  0 59 15263       0      0 0.00e+00    0 0.00e+00  0
VecMDot               10 1.0 5.5667e-03 1.4 5.50e+06 1.0 0.0e+00 0.0e+00 1.0e+01  0  0  0  0  0   0  0  0  0  0 19760       0      0 0.00e+00    0 0.00e+00  0
VecNorm              961 1.0 1.7795e-01 1.8 9.61e+07 1.0 0.0e+00 0.0e+00 9.6e+02  1  2  0  0 40   1  2  0  0 40 10801       0      0 0.00e+00    0 0.00e+00  0
VecScale            1908 1.0 4.2105e-02 1.1 9.54e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  2  0  0  0   0  2  0  0  0 45316       0      0 0.00e+00    0 0.00e+00  0
VecCopy             7117 1.0 3.7942e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  4  0  0  0  0   4  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet              2856 1.0 1.1167e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             4265 1.0 2.4448e-01 1.0 4.26e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3 10  0  0  0   3 10  0  0  0 34890       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             3320 1.0 4.5713e-01 1.0 2.61e+08 1.0 0.0e+00 0.0e+00 0.0e+00  5  6  0  0  0   5  6  0  0  0 11412       0      0 0.00e+00    0 0.00e+00  0
VecAXPBYCZ          1423 1.0 2.4429e-01 1.0 3.56e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3  9  0  0  0   3  9  0  0  0 29126       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY              11 1.0 4.9899e-03 1.1 6.50e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 26053       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     2856 1.0 4.7680e-02 1.5 0.00e+00 0.0 1.1e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       2856 1.0 1.3207e-01 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecNormalize          11 1.0 7.9004e-03 1.2 1.65e+06 1.0 0.0e+00 0.0e+00 1.1e+01  0  0  0  0  0   0  0  0  0  0  4177       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.5460e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 2.4218e-0312.4 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      2856 1.0 4.2172e-02 1.6 0.00e+00 0.0 1.1e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        2856 1.0 1.2827e-01 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              2856 1.0 1.0764e-02 9.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            2856 1.0 1.3744e-03 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.4429e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  2  1  0  0  1   2  1  0  0  1  3292       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 9.1501e+00 1.0 4.09e+09 1.0 1.1e+05 8.0e+03 2.4e+03 96100100100 99  96100100100100  8937       0      0 0.00e+00    0 0.00e+00  0
KSPGMRESOrthog        10 1.0 1.3559e-02 1.1 1.10e+07 1.0 0.0e+00 0.0e+00 1.0e+01  0  0  0  0  0   0  0  0  0  0 16226       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 1.4107e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  1  1  0  0  1   1  1  0  0  1  3367       0      0 0.00e+00    0 0.00e+00  0
PCApply             1434 1.0 6.7442e+00 1.0 2.63e+09 1.0 5.4e+04 8.0e+03 0.0e+00 70 64 50 50  0  70 64 50 50  0  7786       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    41             41     15678720     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     3              3        33432     0.
      Preconditioner     3              3         2936     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.87e-08
Average time for MPI_Barrier(): 1.0959e-05
Average time for zero size MPI_Send(): 6.24415e-06
#PETSc Option Table entries:
-ksp_type tcqmr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 7.08235e-05 iterations 490
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8080 with 20 processors, by luciano.siqueira Wed Sep  9 17:35:28 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.079e+00     1.000   3.078e+00
Objects:              3.000e+01     1.000   3.000e+01
Flop:                 1.345e+09     1.001   1.345e+09  2.689e+10
Flop/sec:             4.368e+08     1.001   4.367e+08  8.735e+09
MPI Messages:         1.966e+03     2.000   1.868e+03  3.735e+04
MPI Message Lengths:  1.570e+07     2.000   7.988e+03  2.984e+08
MPI Reductions:       1.491e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.0785e+00 100.0%  2.6891e+10 100.0%  3.735e+04 100.0%  7.988e+03      100.0%  1.484e+03  99.5%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 5.0417e-0342.3 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 4.9132e-03101.9 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult              981 1.0 1.0439e+00 1.0 4.41e+08 1.0 3.7e+04 8.0e+03 0.0e+00 34 33100100  0  34 33100100  0  8450       0      0 0.00e+00    0 0.00e+00  0
MatSolve             981 1.0 1.1256e+00 1.0 4.37e+08 1.0 0.0e+00 0.0e+00 0.0e+00 36 33  0  0  0  36 33  0  0  0  7770       0      0 0.00e+00    0 0.00e+00  0
MatLUFactorNum         1 1.0 7.5530e-03 1.0 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1437       0      0 0.00e+00    0 0.00e+00  0
MatILUFactorSym        1 1.0 3.4448e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 4.9656e-0349.5 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6401e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetRowIJ            1 1.0 2.7420e-06 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetOrdering         1 1.0 6.9632e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot               981 1.0 1.6958e-01 1.6 9.81e+07 1.0 0.0e+00 0.0e+00 9.8e+02  4  7  0  0 66   4  7  0  0 66 11570       0      0 0.00e+00    0 0.00e+00  0
VecNorm              492 1.0 4.9464e-02 1.2 4.92e+07 1.0 0.0e+00 0.0e+00 4.9e+02  1  4  0  0 33   1  4  0  0 33 19893       0      0 0.00e+00    0 0.00e+00  0
VecCopy                4 1.0 2.9497e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet               984 1.0 1.7947e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             1470 1.0 8.5620e-02 1.1 1.47e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3 11  0  0  0   3 11  0  0  0 34338       0      0 0.00e+00    0 0.00e+00  0
VecWAXPY            1958 1.0 2.6331e-01 1.0 1.71e+08 1.0 0.0e+00 0.0e+00 0.0e+00  8 13  0  0  0   8 13  0  0  0 13011       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin      981 1.0 1.4674e-02 1.6 0.00e+00 0.0 3.7e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd        981 1.0 7.7060e-02 3.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.9380e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.1862e-03 2.9 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin       981 1.0 1.3128e-02 1.6 0.00e+00 0.0 3.7e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd         981 1.0 7.5948e-02 4.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack               981 1.0 2.5813e-03 4.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack             981 1.0 5.3245e-04 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.4793e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.7109e+00 1.0 1.34e+09 1.0 3.7e+04 8.0e+03 1.5e+03 88100100100 99  88100100100 99  9915       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                2 1.0 2.5045e-02 1.2 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0   433       0      0 0.00e+00    0 0.00e+00  0
PCSetUpOnBlocks        1 1.0 1.1727e-02 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   925       0      0 0.00e+00    0 0.00e+00  0
PCApply              981 1.0 1.1502e+00 1.0 4.37e+08 1.0 0.0e+00 0.0e+00 0.0e+00 37 33  0  0  0  37 33  0  0  0  7604       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              4     11791508     0.
         Vec Scatter     1              1          816     0.
              Vector    14             14      4031808     0.
           Index Set     5              5       608512     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     2              2         2832     0.
      Preconditioner     2              2         1944     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.34e-08
Average time for MPI_Barrier(): 1.0434e-05
Average time for zero size MPI_Send(): 6.73415e-06
#PETSc Option Table entries:
-ksp_type cgs
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type bjacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 8.26951e-08 iterations 1553
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8080 with 20 processors, by luciano.siqueira Wed Sep  9 17:35:35 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           5.497e+00     1.000   5.497e+00
Objects:              2.300e+01     1.000   2.300e+01
Flop:                 3.028e+09     1.002   3.028e+09  6.055e+10
Flop/sec:             5.509e+08     1.002   5.508e+08  1.102e+10
MPI Messages:         6.218e+03     2.000   5.907e+03  1.181e+05
MPI Message Lengths:  4.972e+07     2.000   7.996e+03  9.447e+08
MPI Reductions:       4.682e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 5.4973e+00 100.0%  6.0554e+10 100.0%  1.181e+05 100.0%  7.996e+03      100.0%  4.675e+03  99.9%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 3.3304e-0341.6 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 2.8573e-0349.1 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             3107 1.0 3.2591e+00 1.1 1.40e+09 1.0 1.2e+05 8.0e+03 0.0e+00 57 46100100  0  57 46100100  0  8572       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 2.9114e-0326.1 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6808e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot              3107 1.0 4.3445e-01 1.6 3.11e+08 1.0 0.0e+00 0.0e+00 3.1e+03  7 10  0  0 66   7 10  0  0 66 14303       0      0 0.00e+00    0 0.00e+00  0
VecNorm             1555 1.0 1.2275e-01 1.1 1.56e+08 1.0 0.0e+00 0.0e+00 1.6e+03  2  5  0  0 33   2  5  0  0 33 25335       0      0 0.00e+00    0 0.00e+00  0
VecCopy                4 1.0 2.2502e-04 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 3 1.0 2.2008e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             4659 1.0 2.5309e-01 1.1 4.66e+08 1.0 0.0e+00 0.0e+00 0.0e+00  5 15  0  0  0   5 15  0  0  0 36817       0      0 0.00e+00    0 0.00e+00  0
VecWAXPY            6210 1.0 7.9992e-01 1.0 5.43e+08 1.0 0.0e+00 0.0e+00 0.0e+00 14 18  0  0  0  14 18  0  0  0 13585       0      0 0.00e+00    0 0.00e+00  0
VecPointwiseMult    3107 1.0 4.2481e-01 1.0 1.55e+08 1.0 0.0e+00 0.0e+00 0.0e+00  8  5  0  0  0   8  5  0  0  0  7314       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     3107 1.0 3.4787e-02 1.5 0.00e+00 0.0 1.2e+05 8.0e+03 0.0e+00  1  0100100  0   1  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       3107 1.0 2.0852e-01 2.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.7180e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 2.1551e-03 5.5 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      3107 1.0 3.1335e-02 1.6 0.00e+00 0.0 1.2e+05 8.0e+03 0.0e+00  1  0100100  0   1  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        3107 1.0 2.0559e-01 2.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              3107 1.0 8.8656e-03 7.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            3107 1.0 1.3608e-03 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 1.4680e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 5.1334e+00 1.0 3.03e+09 1.0 1.2e+05 8.0e+03 4.7e+03 93100100100100  93100100100100 11794       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 1.0180e-06 3.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply             3107 1.0 4.2881e-01 1.0 1.55e+08 1.0 0.0e+00 0.0e+00 2.0e+00  8  5  0  0  0   8  5  0  0  0  7246       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      8013484     0.
         Vec Scatter     1              1          816     0.
              Vector    13             13      4430336     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1416     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.98e-08
Average time for MPI_Barrier(): 1.2576e-05
Average time for zero size MPI_Send(): 6.19195e-06
#PETSc Option Table entries:
-ksp_type cgs
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type jacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 8.535e-05 iterations 546
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8080 with 20 processors, by luciano.siqueira Wed Sep  9 17:35:41 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.803e+00     1.000   3.803e+00
Objects:              2.200e+01     1.000   2.200e+01
Flop:                 1.552e+09     1.001   1.552e+09  3.104e+10
Flop/sec:             4.082e+08     1.001   4.081e+08  8.163e+09
MPI Messages:         2.190e+03     2.000   2.080e+03  4.161e+04
MPI Message Lengths:  1.750e+07     2.000   7.989e+03  3.324e+08
MPI Reductions:       1.659e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.8032e+00 100.0%  3.1043e+10 100.0%  4.161e+04 100.0%  7.989e+03      100.0%  1.652e+03  99.6%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 6.1218e-0370.2 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 4.8565e-0380.5 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             1093 1.0 1.1606e+00 1.0 4.92e+08 1.0 4.2e+04 8.0e+03 0.0e+00 30 32100100  0  30 32100100  0  8468       0      0 0.00e+00    0 0.00e+00  0
MatSOR              1093 1.0 1.6782e+00 1.0 5.42e+08 1.0 0.0e+00 0.0e+00 0.0e+00 43 35  0  0  0  43 35  0  0  0  6459       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 4.9100e-0342.7 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6915e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot              1093 1.0 2.2037e-01 1.6 1.09e+08 1.0 0.0e+00 0.0e+00 1.1e+03  5  7  0  0 66   5  7  0  0 66  9920       0      0 0.00e+00    0 0.00e+00  0
VecNorm              548 1.0 5.2946e-02 1.1 5.48e+07 1.0 0.0e+00 0.0e+00 5.5e+02  1  4  0  0 33   1  4  0  0 33 20701       0      0 0.00e+00    0 0.00e+00  0
VecCopy                4 1.0 2.4854e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 3 1.0 2.1650e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             1638 1.0 1.0468e-01 1.1 1.64e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3 11  0  0  0   3 11  0  0  0 31295       0      0 0.00e+00    0 0.00e+00  0
VecWAXPY            2182 1.0 3.0909e-01 1.0 1.91e+08 1.0 0.0e+00 0.0e+00 0.0e+00  8 12  0  0  0   8 12  0  0  0 12352       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     1093 1.0 2.1503e-02 1.8 0.00e+00 0.0 4.2e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       1093 1.0 8.8257e-02 3.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.6540e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.3893e-03 3.5 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      1093 1.0 1.9198e-02 1.9 0.00e+00 0.0 4.2e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        1093 1.0 8.7023e-02 3.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              1093 1.0 4.2409e-03 9.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            1093 1.0 7.8209e-04 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 1.4846e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 3.4404e+00 1.0 1.55e+09 1.0 4.1e+04 8.0e+03 1.6e+03 90100100100 99  90100100100 99  9019       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 2.9400e-07 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply             1093 1.0 1.6799e+00 1.0 5.42e+08 1.0 0.0e+00 0.0e+00 0.0e+00 43 35  0  0  0  43 35  0  0  0  6452       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    12             12      4028608     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1416     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.96e-08
Average time for MPI_Barrier(): 1.24592e-05
Average time for zero size MPI_Send(): 6.28735e-06
#PETSc Option Table entries:
-ksp_type cgs
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type sor
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.000193207 iterations 288
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8080 with 20 processors, by luciano.siqueira Wed Sep  9 17:35:47 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           4.212e+00     1.000   4.212e+00
Objects:              4.500e+01     1.000   4.500e+01
Flop:                 1.619e+09     1.001   1.619e+09  3.238e+10
Flop/sec:             3.845e+08     1.001   3.845e+08  7.689e+09
MPI Messages:         2.332e+03     2.000   2.215e+03  4.431e+04
MPI Message Lengths:  1.863e+07     2.000   7.990e+03  3.540e+08
MPI Reductions:       9.080e+02     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 4.2117e+00 100.0%  3.2385e+10 100.0%  4.431e+04 100.0%  7.990e+03      100.0%  9.010e+02  99.2%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 4.8710e-0356.1 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 4.8333e-0382.2 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             1164 1.0 1.2744e+00 1.0 5.24e+08 1.0 4.4e+04 8.0e+03 0.0e+00 30 32100100  0  30 32100100  0  8213       0      0 0.00e+00    0 0.00e+00  0
MatSOR              1165 1.0 1.8026e+00 1.0 5.78e+08 1.0 0.0e+00 0.0e+00 0.0e+00 42 36  0  0  0  42 36  0  0  0  6409       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 4.8836e-0341.9 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6053e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  1   1  0  0  0  1     0       0      0 0.00e+00    0 0.00e+00  0
VecDot               577 1.0 1.4675e-01 2.0 5.77e+07 1.0 0.0e+00 0.0e+00 5.8e+02  2  4  0  0 64   2  4  0  0 64  7864       0      0 0.00e+00    0 0.00e+00  0
VecMDot               10 1.0 5.6834e-03 1.5 5.50e+06 1.0 0.0e+00 0.0e+00 1.0e+01  0  0  0  0  1   0  0  0  0  1 19354       0      0 0.00e+00    0 0.00e+00  0
VecNorm              301 1.0 3.2656e-02 1.0 3.01e+07 1.0 0.0e+00 0.0e+00 3.0e+02  1  2  0  0 33   1  2  0  0 33 18435       0      0 0.00e+00    0 0.00e+00  0
VecScale              11 1.0 2.6304e-04 1.1 5.50e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 41818       0      0 0.00e+00    0 0.00e+00  0
VecCopy             1159 1.0 5.5648e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet              1159 1.0 3.5499e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY              865 1.0 5.8428e-02 1.1 8.65e+07 1.0 0.0e+00 0.0e+00 0.0e+00  1  5  0  0  0   1  5  0  0  0 29609       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             1154 1.0 1.6290e-01 1.0 8.66e+07 1.0 0.0e+00 0.0e+00 0.0e+00  4  5  0  0  0   4  5  0  0  0 10626       0      0 0.00e+00    0 0.00e+00  0
VecAXPBYCZ           577 1.0 1.0196e-01 1.1 1.44e+08 1.0 0.0e+00 0.0e+00 0.0e+00  2  9  0  0  0   2  9  0  0  0 28294       0      0 0.00e+00    0 0.00e+00  0
VecWAXPY            1150 1.0 1.7041e-01 1.0 1.01e+08 1.0 0.0e+00 0.0e+00 0.0e+00  4  6  0  0  0   4  6  0  0  0 11807       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY              11 1.0 4.9275e-03 1.1 6.50e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 26382       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     1164 1.0 2.3344e-02 1.6 0.00e+00 0.0 4.4e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       1164 1.0 1.2160e-01 3.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecNormalize          11 1.0 7.4254e-03 1.1 1.65e+06 1.0 0.0e+00 0.0e+00 1.1e+01  0  0  0  0  1   0  0  0  0  1  4444       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.9710e-06 2.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.0268e-03 2.7 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      1164 1.0 2.0406e-02 1.7 0.00e+00 0.0 4.4e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        1164 1.0 1.2007e-01 3.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              1164 1.0 4.1981e-03 9.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            1164 1.0 6.7947e-04 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.4153e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  3  1  1  1  3   3  1  1  1  3  3356       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 3.8401e+00 1.0 1.62e+09 1.0 4.4e+04 8.0e+03 8.9e+02 91100100100 98  91100100100 99  8430       0      0 0.00e+00    0 0.00e+00  0
KSPGMRESOrthog        10 1.0 1.3577e-02 1.1 1.10e+07 1.0 0.0e+00 0.0e+00 1.0e+01  0  1  0  0  1   0  1  0  0  1 16204       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 1.4036e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  3  1  1  1  3   3  1  1  1  3  3384       0      0 0.00e+00    0 0.00e+00  0
PCApply              588 1.0 2.7773e+00 1.0 1.07e+09 1.0 2.2e+04 8.0e+03 0.0e+00 65 66 49 50  0  65 66 49 50  0  7690       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    31             31     11661440     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     3              3        33432     0.
      Preconditioner     3              3         2936     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.79e-08
Average time for MPI_Barrier(): 1.04576e-05
Average time for zero size MPI_Send(): 6.11075e-06
#PETSc Option Table entries:
-ksp_type cgs
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.00114039 iterations 479
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8080 with 20 processors, by luciano.siqueira Wed Sep  9 17:35:52 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           2.959e+00     1.000   2.959e+00
Objects:              2.900e+01     1.000   2.900e+01
Flop:                 1.387e+09     1.001   1.386e+09  2.773e+10
Flop/sec:             4.686e+08     1.001   4.686e+08  9.372e+09
MPI Messages:         1.922e+03     2.000   1.826e+03  3.652e+04
MPI Message Lengths:  1.535e+07     2.000   7.988e+03  2.917e+08
MPI Reductions:       1.936e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 2.9588e+00 100.0%  2.7729e+10 100.0%  3.652e+04 100.0%  7.988e+03      100.0%  1.929e+03  99.6%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 6.2309e-0338.1 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 6.1493e-03121.3 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult              959 1.0 9.7155e-01 1.0 4.31e+08 1.0 3.6e+04 8.0e+03 0.0e+00 33 31100100  0  33 31100100  0  8876       0      0 0.00e+00    0 0.00e+00  0
MatSolve             959 1.0 1.0565e+00 1.0 4.28e+08 1.0 0.0e+00 0.0e+00 0.0e+00 36 31  0  0  0  36 31  0  0  0  8094       0      0 0.00e+00    0 0.00e+00  0
MatLUFactorNum         1 1.0 7.5552e-03 1.0 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1437       0      0 0.00e+00    0 0.00e+00  0
MatILUFactorSym        1 1.0 3.4573e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 6.2010e-0358.4 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.9700e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetRowIJ            1 1.0 2.9500e-06 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetOrdering         1 1.0 6.9331e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot               958 1.0 1.0984e-01 1.1 9.58e+07 1.0 0.0e+00 0.0e+00 9.6e+02  3  7  0  0 49   3  7  0  0 50 17444       0      0 0.00e+00    0 0.00e+00  0
VecDotNorm2          479 1.0 1.0753e-01 1.1 9.58e+07 1.0 0.0e+00 0.0e+00 4.8e+02  3  7  0  0 25   3  7  0  0 25 17819       0      0 0.00e+00    0 0.00e+00  0
VecNorm              481 1.0 4.8461e-02 1.2 4.81e+07 1.0 0.0e+00 0.0e+00 4.8e+02  1  3  0  0 25   1  3  0  0 25 19851       0      0 0.00e+00    0 0.00e+00  0
VecCopy                2 1.0 1.5787e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet               964 1.0 2.0956e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY                1 1.0 8.8840e-05 1.1 1.00e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 22512       0      0 0.00e+00    0 0.00e+00  0
VecAXPBYCZ           958 1.0 1.5533e-01 1.0 1.92e+08 1.0 0.0e+00 0.0e+00 0.0e+00  5 14  0  0  0   5 14  0  0  0 24670       0      0 0.00e+00    0 0.00e+00  0
VecWAXPY             958 1.0 1.2193e-01 1.0 9.58e+07 1.0 0.0e+00 0.0e+00 0.0e+00  4  7  0  0  0   4  7  0  0  0 15713       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin      959 1.0 1.3141e-02 1.4 0.00e+00 0.0 3.6e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd        959 1.0 2.9499e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.9570e-06 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 2.5666e-03 6.8 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin       959 1.0 1.1772e-02 1.4 0.00e+00 0.0 3.6e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd         959 1.0 2.8504e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack               959 1.0 2.7297e-03 5.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack             959 1.0 4.7340e-04 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.2627e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.5883e+00 1.0 1.39e+09 1.0 3.6e+04 8.0e+03 1.9e+03 87100100100 99  87100100100 99 10708       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                2 1.0 1.7954e-02 1.3 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0   604       0      0 0.00e+00    0 0.00e+00  0
PCSetUpOnBlocks        1 1.0 1.1747e-02 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   924       0      0 0.00e+00    0 0.00e+00  0
PCApply              959 1.0 1.0839e+00 1.0 4.28e+08 1.0 0.0e+00 0.0e+00 0.0e+00 36 31  0  0  0  36 31  0  0  0  7888       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              4     11791508     0.
         Vec Scatter     1              1          816     0.
              Vector    13             13      3630080     0.
           Index Set     5              5       608512     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     2              2         2840     0.
      Preconditioner     2              2         1944     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.18e-08
Average time for MPI_Barrier(): 1.28818e-05
Average time for zero size MPI_Send(): 6.4198e-06
#PETSc Option Table entries:
-ksp_type bcgs
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type bjacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.00878969 iterations 1368
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8080 with 20 processors, by luciano.siqueira Wed Sep  9 17:35:59 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           4.946e+00     1.000   4.946e+00
Objects:              2.200e+01     1.000   2.200e+01
Flop:                 2.873e+09     1.002   2.873e+09  5.745e+10
Flop/sec:             5.809e+08     1.002   5.808e+08  1.162e+10
MPI Messages:         5.478e+03     2.000   5.204e+03  1.041e+05
MPI Message Lengths:  4.380e+07     2.000   7.996e+03  8.322e+08
MPI Reductions:       5.494e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 4.9458e+00 100.0%  5.7450e+10 100.0%  1.041e+05 100.0%  7.996e+03      100.0%  5.487e+03  99.9%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 4.0254e-0352.2 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 3.4751e-0378.0 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             2737 1.0 2.7961e+00 1.0 1.23e+09 1.0 1.0e+05 8.0e+03 0.0e+00 56 43100100  0  56 43100100  0  8802       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 3.5305e-0334.8 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6439e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot              2736 1.0 2.6860e-01 1.1 2.74e+08 1.0 0.0e+00 0.0e+00 2.7e+03  5 10  0  0 50   5 10  0  0 50 20372       0      0 0.00e+00    0 0.00e+00  0
VecDotNorm2         1368 1.0 2.8780e-01 1.1 2.74e+08 1.0 0.0e+00 0.0e+00 1.4e+03  5 10  0  0 25   5 10  0  0 25 19013       0      0 0.00e+00    0 0.00e+00  0
VecNorm             1370 1.0 1.1869e-01 1.1 1.37e+08 1.0 0.0e+00 0.0e+00 1.4e+03  2  5  0  0 25   2  5  0  0 25 23086       0      0 0.00e+00    0 0.00e+00  0
VecCopy                2 1.0 1.1842e-04 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 5 1.0 3.2815e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY                1 1.0 7.9635e-05 1.1 1.00e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 25115       0      0 0.00e+00    0 0.00e+00  0
VecAXPBYCZ          2736 1.0 4.4872e-01 1.0 5.47e+08 1.0 0.0e+00 0.0e+00 0.0e+00  9 19  0  0  0   9 19  0  0  0 24389       0      0 0.00e+00    0 0.00e+00  0
VecWAXPY            2736 1.0 3.5141e-01 1.0 2.74e+08 1.0 0.0e+00 0.0e+00 0.0e+00  7 10  0  0  0   7 10  0  0  0 15571       0      0 0.00e+00    0 0.00e+00  0
VecPointwiseMult    2737 1.0 3.7671e-01 1.0 1.37e+08 1.0 0.0e+00 0.0e+00 0.0e+00  8  5  0  0  0   8  5  0  0  0  7265       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     2737 1.0 3.0560e-02 1.5 0.00e+00 0.0 1.0e+05 8.0e+03 0.0e+00  1  0100100  0   1  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       2737 1.0 9.1623e-02 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.7920e-06 2.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.0994e-03 2.8 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      2737 1.0 2.7124e-02 1.5 0.00e+00 0.0 1.0e+05 8.0e+03 0.0e+00  1  0100100  0   1  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        2737 1.0 8.8790e-02 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              2737 1.0 7.1001e-03 8.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            2737 1.0 1.1459e-03 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 1.2670e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 4.5832e+00 1.0 2.87e+09 1.0 1.0e+05 8.0e+03 5.5e+03 93100100100100  93100100100100 12532       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 6.7900e-07 2.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply             2737 1.0 3.8132e-01 1.0 1.37e+08 1.0 0.0e+00 0.0e+00 2.0e+00  8  5  0  0  0   8  5  0  0  0  7178       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      8013484     0.
         Vec Scatter     1              1          816     0.
              Vector    12             12      4028608     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1424     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.97e-08
Average time for MPI_Barrier(): 1.16608e-05
Average time for zero size MPI_Send(): 6.44195e-06
#PETSc Option Table entries:
-ksp_type bcgs
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type jacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.00987163 iterations 515
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8080 with 20 processors, by luciano.siqueira Wed Sep  9 17:36:04 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.703e+00     1.000   3.703e+00
Objects:              2.100e+01     1.000   2.100e+01
Flop:                 1.542e+09     1.001   1.542e+09  3.083e+10
Flop/sec:             4.164e+08     1.001   4.163e+08  8.327e+09
MPI Messages:         2.066e+03     2.000   1.963e+03  3.925e+04
MPI Message Lengths:  1.650e+07     2.000   7.988e+03  3.136e+08
MPI Reductions:       2.080e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.7027e+00 100.0%  3.0831e+10 100.0%  3.925e+04 100.0%  7.988e+03      100.0%  2.073e+03  99.7%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 2.8926e-0320.8 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 2.8280e-0340.9 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             1031 1.0 1.0961e+00 1.0 4.64e+08 1.0 3.9e+04 8.0e+03 0.0e+00 29 30100100  0  29 30100100  0  8458       0      0 0.00e+00    0 0.00e+00  0
MatSOR              1031 1.0 1.5937e+00 1.1 5.11e+08 1.0 0.0e+00 0.0e+00 0.0e+00 42 33  0  0  0  42 33  0  0  0  6415       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 2.8792e-0323.9 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6284e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot              1030 1.0 1.7461e-01 1.4 1.03e+08 1.0 0.0e+00 0.0e+00 1.0e+03  4  7  0  0 50   4  7  0  0 50 11798       0      0 0.00e+00    0 0.00e+00  0
VecDotNorm2          515 1.0 1.6634e-01 1.3 1.03e+08 1.0 0.0e+00 0.0e+00 5.2e+02  4  7  0  0 25   4  7  0  0 25 12384       0      0 0.00e+00    0 0.00e+00  0
VecNorm              517 1.0 6.2127e-02 1.2 5.17e+07 1.0 0.0e+00 0.0e+00 5.2e+02  2  3  0  0 25   2  3  0  0 25 16643       0      0 0.00e+00    0 0.00e+00  0
VecCopy                2 1.0 1.1926e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 5 1.0 3.4750e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY                1 1.0 8.4260e-05 1.1 1.00e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 23736       0      0 0.00e+00    0 0.00e+00  0
VecAXPBYCZ          1030 1.0 1.7767e-01 1.1 2.06e+08 1.0 0.0e+00 0.0e+00 0.0e+00  5 13  0  0  0   5 13  0  0  0 23189       0      0 0.00e+00    0 0.00e+00  0
VecWAXPY            1030 1.0 1.5076e-01 1.1 1.03e+08 1.0 0.0e+00 0.0e+00 0.0e+00  4  7  0  0  0   4  7  0  0  0 13664       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     1031 1.0 1.7635e-02 1.4 0.00e+00 0.0 3.9e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       1031 1.0 7.7907e-02 2.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.5700e-06 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.2903e-03 3.2 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      1031 1.0 1.5675e-02 1.5 0.00e+00 0.0 3.9e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        1031 1.0 7.6690e-02 2.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              1031 1.0 4.3588e-03 9.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            1031 1.0 6.8526e-04 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 1.2712e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 3.3354e+00 1.0 1.54e+09 1.0 3.9e+04 8.0e+03 2.1e+03 90100100100 99  90100100100 99  9240       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 3.6800e-07 2.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply             1031 1.0 1.5957e+00 1.1 5.11e+08 1.0 0.0e+00 0.0e+00 0.0e+00 42 33  0  0  0  42 33  0  0  0  6408       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    11             11      3626880     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1424     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.06e-08
Average time for MPI_Barrier(): 1.14126e-05
Average time for zero size MPI_Send(): 6.369e-06
#PETSc Option Table entries:
-ksp_type bcgs
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type sor
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.000785404 iterations 301
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8080 with 20 processors, by luciano.siqueira Wed Sep  9 17:36:11 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           4.510e+00     1.000   4.510e+00
Objects:              4.400e+01     1.000   4.400e+01
Flop:                 1.737e+09     1.001   1.737e+09  3.473e+10
Flop/sec:             3.851e+08     1.001   3.851e+08  7.702e+09
MPI Messages:         2.436e+03     2.000   2.314e+03  4.628e+04
MPI Message Lengths:  1.946e+07     2.000   7.990e+03  3.698e+08
MPI Reductions:       1.247e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 4.5095e+00 100.0%  3.4730e+10 100.0%  4.628e+04 100.0%  7.990e+03      100.0%  1.240e+03  99.4%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 4.3066e-0355.7 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 4.2895e-0386.6 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             1216 1.0 1.3861e+00 1.1 5.47e+08 1.0 4.6e+04 8.0e+03 0.0e+00 30 31100100  0  30 31100100  0  7889       0      0 0.00e+00    0 0.00e+00  0
MatSOR              1217 1.0 1.9404e+00 1.1 6.03e+08 1.0 0.0e+00 0.0e+00 0.0e+00 42 35  0  0  0  42 35  0  0  0  6220       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 4.3409e-0343.0 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6456e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot               602 1.0 1.1288e-01 1.5 6.02e+07 1.0 0.0e+00 0.0e+00 6.0e+02  2  3  0  0 48   2  3  0  0 49 10666       0      0 0.00e+00    0 0.00e+00  0
VecDotNorm2          301 1.0 1.1152e-01 1.6 6.02e+07 1.0 0.0e+00 0.0e+00 3.0e+02  2  3  0  0 24   2  3  0  0 24 10796       0      0 0.00e+00    0 0.00e+00  0
VecMDot               10 1.0 4.0778e-03 1.2 5.50e+06 1.0 0.0e+00 0.0e+00 1.0e+01  0  0  0  0  1   0  0  0  0  1 26975       0      0 0.00e+00    0 0.00e+00  0
VecNorm              314 1.0 4.2915e-02 1.2 3.14e+07 1.0 0.0e+00 0.0e+00 3.1e+02  1  2  0  0 25   1  2  0  0 25 14633       0      0 0.00e+00    0 0.00e+00  0
VecScale              11 1.0 2.6554e-04 1.1 5.50e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 41425       0      0 0.00e+00    0 0.00e+00  0
VecCopy             1209 1.0 5.7206e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet              1213 1.0 4.1232e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY                2 1.0 1.6186e-04 1.1 2.00e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 24712       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             1206 1.0 1.7551e-01 1.1 9.04e+07 1.0 0.0e+00 0.0e+00 0.0e+00  4  5  0  0  0   4  5  0  0  0 10307       0      0 0.00e+00    0 0.00e+00  0
VecAXPBYCZ          1205 1.0 2.1543e-01 1.1 2.71e+08 1.0 0.0e+00 0.0e+00 0.0e+00  5 16  0  0  0   5 16  0  0  0 25172       0      0 0.00e+00    0 0.00e+00  0
VecWAXPY             602 1.0 9.2719e-02 1.1 6.02e+07 1.0 0.0e+00 0.0e+00 0.0e+00  2  3  0  0  0   2  3  0  0  0 12986       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY              11 1.0 5.0213e-03 1.1 6.50e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 25890       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     1216 1.0 2.1288e-02 1.5 0.00e+00 0.0 4.6e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       1216 1.0 1.6927e-01 5.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecNormalize          11 1.0 7.8578e-03 1.1 1.65e+06 1.0 0.0e+00 0.0e+00 1.1e+01  0  0  0  0  1   0  0  0  0  1  4200       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 1.4699e-05 3.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.2604e-03 3.2 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      1216 1.0 1.9233e-02 1.5 0.00e+00 0.0 4.6e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        1216 1.0 1.6783e-01 5.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              1216 1.0 4.2719e-03 6.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            1216 1.0 7.3366e-04 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.4432e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  3  1  1  1  2   3  1  1  1  2  3291       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 4.1454e+00 1.0 1.74e+09 1.0 4.6e+04 8.0e+03 1.2e+03 92100100100 98  92100100100 99  8375       0      0 0.00e+00    0 0.00e+00  0
KSPGMRESOrthog        10 1.0 1.2284e-02 1.0 1.10e+07 1.0 0.0e+00 0.0e+00 1.0e+01  0  1  0  0  1   0  1  0  0  1 17909       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 1.4335e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  3  1  1  1  2   3  1  1  1  2  3314       0      0 0.00e+00    0 0.00e+00  0
PCApply              614 1.0 2.9877e+00 1.0 1.12e+09 1.0 2.3e+04 8.0e+03 0.0e+00 65 64 50 50  0  65 64 50 50  0  7469       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    30             30     11259712     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     3              3        33440     0.
      Preconditioner     3              3         2936     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.94e-08
Average time for MPI_Barrier(): 1.21776e-05
Average time for zero size MPI_Send(): 6.3661e-06
#PETSc Option Table entries:
-ksp_type bcgs
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 8.37695e-05 iterations 475
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8080 with 20 processors, by luciano.siqueira Wed Sep  9 17:36:16 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.115e+00     1.000   3.115e+00
Objects:              3.200e+01     1.000   3.200e+01
Flop:                 1.446e+09     1.001   1.446e+09  2.891e+10
Flop/sec:             4.641e+08     1.001   4.641e+08  9.281e+09
MPI Messages:         1.906e+03     2.000   1.811e+03  3.621e+04
MPI Message Lengths:  1.522e+07     2.000   7.987e+03  2.893e+08
MPI Reductions:       1.445e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.1148e+00 100.0%  2.8910e+10 100.0%  3.621e+04 100.0%  7.987e+03      100.0%  1.438e+03  99.5%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 3.4650e-0340.1 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 2.4624e-0348.4 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult              951 1.0 9.9825e-01 1.0 4.28e+08 1.0 3.6e+04 8.0e+03 0.0e+00 32 30100100  0  32 30100100  0  8566       0      0 0.00e+00    0 0.00e+00  0
MatSolve             951 1.0 1.0742e+00 1.0 4.24e+08 1.0 0.0e+00 0.0e+00 0.0e+00 34 29  0  0  0  34 29  0  0  0  7893       0      0 0.00e+00    0 0.00e+00  0
MatLUFactorNum         1 1.0 7.4954e-03 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1448       0      0 0.00e+00    0 0.00e+00  0
MatILUFactorSym        1 1.0 3.4348e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 2.5143e-0325.0 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6365e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetRowIJ            1 1.0 2.8410e-06 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetOrdering         1 1.0 6.9764e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot               950 1.0 1.3146e-01 1.3 9.50e+07 1.0 0.0e+00 0.0e+00 9.5e+02  4  7  0  0 66   4  7  0  0 66 14453       0      0 0.00e+00    0 0.00e+00  0
VecNorm              477 1.0 8.0256e-02 1.6 4.77e+07 1.0 0.0e+00 0.0e+00 4.8e+02  2  3  0  0 33   2  3  0  0 33 11887       0      0 0.00e+00    0 0.00e+00  0
VecCopy                5 1.0 3.6994e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet               955 1.0 2.0056e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             1899 1.0 1.0013e-01 1.0 1.90e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3 13  0  0  0   3 13  0  0  0 37932       0      0 0.00e+00    0 0.00e+00  0
VecAYPX              949 1.0 1.2571e-01 1.0 9.48e+07 1.0 0.0e+00 0.0e+00 0.0e+00  4  7  0  0  0   4  7  0  0  0 15082       0      0 0.00e+00    0 0.00e+00  0
VecWAXPY            1898 1.0 2.5075e-01 1.0 1.66e+08 1.0 0.0e+00 0.0e+00 0.0e+00  8 11  0  0  0   8 11  0  0  0 13245       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin      951 1.0 1.5440e-02 1.6 0.00e+00 0.0 3.6e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd        951 1.0 5.3185e-02 2.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.5940e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 2.4213e-0312.9 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin       951 1.0 1.3813e-02 1.6 0.00e+00 0.0 3.6e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd         951 1.0 5.2147e-02 2.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack               951 1.0 2.9718e-03 6.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack             951 1.0 5.4932e-04 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.8933e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.7550e+00 1.0 1.45e+09 1.0 3.6e+04 8.0e+03 1.4e+03 88100100100 99  88100100100 99 10489       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                2 1.0 2.4571e-02 1.3 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0   442       0      0 0.00e+00    0 0.00e+00  0
PCSetUpOnBlocks        1 1.0 1.1605e-02 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   935       0      0 0.00e+00    0 0.00e+00  0
PCApply              951 1.0 1.1010e+00 1.0 4.24e+08 1.0 0.0e+00 0.0e+00 0.0e+00 35 29  0  0  0  35 29  0  0  0  7701       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              4     11791508     0.
         Vec Scatter     1              1          816     0.
              Vector    16             16      4835264     0.
           Index Set     5              5       608512     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     2              2         2832     0.
      Preconditioner     2              2         1944     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.11e-08
Average time for MPI_Barrier(): 1.14552e-05
Average time for zero size MPI_Send(): 6.4698e-06
#PETSc Option Table entries:
-ksp_type tfqmr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type bjacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 3.10408e-07 iterations 1511
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8080 with 20 processors, by luciano.siqueira Wed Sep  9 17:36:23 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           5.893e+00     1.000   5.893e+00
Objects:              2.500e+01     1.000   2.500e+01
Flop:                 3.399e+09     1.002   3.399e+09  6.797e+10
Flop/sec:             5.769e+08     1.002   5.768e+08  1.154e+10
MPI Messages:         6.050e+03     2.000   5.748e+03  1.150e+05
MPI Message Lengths:  4.838e+07     2.000   7.996e+03  9.191e+08
MPI Reductions:       4.555e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 5.8929e+00 100.0%  6.7975e+10 100.0%  1.150e+05 100.0%  7.996e+03      100.0%  4.548e+03  99.8%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 5.2412e-0336.4 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 4.2485e-0390.0 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             3023 1.0 3.2279e+00 1.1 1.36e+09 1.0 1.1e+05 8.0e+03 0.0e+00 53 40100100  0  53 40100100  0  8421       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 4.3015e-0341.6 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6583e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot              3022 1.0 3.5678e-01 1.3 3.02e+08 1.0 0.0e+00 0.0e+00 3.0e+03  6  9  0  0 66   6  9  0  0 66 16940       0      0 0.00e+00    0 0.00e+00  0
VecNorm             1513 1.0 1.9141e-01 1.6 1.51e+08 1.0 0.0e+00 0.0e+00 1.5e+03  3  4  0  0 33   3  4  0  0 33 15809       0      0 0.00e+00    0 0.00e+00  0
VecCopy                5 1.0 3.1965e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 4 1.0 2.8006e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             6043 1.0 3.1004e-01 1.1 6.04e+08 1.0 0.0e+00 0.0e+00 0.0e+00  5 18  0  0  0   5 18  0  0  0 38983       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             3021 1.0 3.9179e-01 1.0 3.02e+08 1.0 0.0e+00 0.0e+00 0.0e+00  7  9  0  0  0   7  9  0  0  0 15416       0      0 0.00e+00    0 0.00e+00  0
VecWAXPY            6042 1.0 7.8690e-01 1.0 5.29e+08 1.0 0.0e+00 0.0e+00 0.0e+00 13 16  0  0  0  13 16  0  0  0 13436       0      0 0.00e+00    0 0.00e+00  0
VecPointwiseMult    3023 1.0 4.2570e-01 1.0 1.51e+08 1.0 0.0e+00 0.0e+00 0.0e+00  7  4  0  0  0   7  4  0  0  0  7101       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     3023 1.0 3.4455e-02 1.4 0.00e+00 0.0 1.1e+05 8.0e+03 0.0e+00  1  0100100  0   1  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       3023 1.0 2.6518e-01 3.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 1.2760e-05 2.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.4919e-03 3.3 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      3023 1.0 3.0905e-02 1.5 0.00e+00 0.0 1.1e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        3023 1.0 2.6209e-01 3.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              3023 1.0 7.9668e-03 8.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            3023 1.0 1.4615e-03 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 2.6883e-03 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 5.5294e+00 1.0 3.40e+09 1.0 1.1e+05 8.0e+03 4.5e+03 94100100100100  94100100100100 12291       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 1.1630e-06 3.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply             3023 1.0 4.2983e-01 1.0 1.51e+08 1.0 0.0e+00 0.0e+00 2.0e+00  7  4  0  0  0   7  4  0  0  0  7033       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      8013484     0.
         Vec Scatter     1              1          816     0.
              Vector    15             15      5233792     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1416     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.24e-08
Average time for MPI_Barrier(): 1.4345e-05
Average time for zero size MPI_Send(): 6.36595e-06
#PETSc Option Table entries:
-ksp_type tfqmr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type jacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.000233762 iterations 533
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8080 with 20 processors, by luciano.siqueira Wed Sep  9 17:36:29 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.889e+00     1.000   3.889e+00
Objects:              2.400e+01     1.000   2.400e+01
Flop:                 1.675e+09     1.001   1.675e+09  3.349e+10
Flop/sec:             4.307e+08     1.001   4.307e+08  8.613e+09
MPI Messages:         2.138e+03     2.000   2.031e+03  4.062e+04
MPI Message Lengths:  1.708e+07     2.000   7.989e+03  3.245e+08
MPI Reductions:       1.619e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.8887e+00 100.0%  3.3495e+10 100.0%  4.062e+04 100.0%  7.989e+03      100.0%  1.612e+03  99.6%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 4.2979e-0348.9 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 4.2588e-0383.1 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             1067 1.0 1.1174e+00 1.0 4.80e+08 1.0 4.1e+04 8.0e+03 0.0e+00 29 29100100  0  29 29100100  0  8586       0      0 0.00e+00    0 0.00e+00  0
MatSOR              1067 1.0 1.6255e+00 1.0 5.29e+08 1.0 0.0e+00 0.0e+00 0.0e+00 41 32  0  0  0  41 32  0  0  0  6509       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 4.3116e-0341.4 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6815e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot              1066 1.0 1.7775e-01 1.3 1.07e+08 1.0 0.0e+00 0.0e+00 1.1e+03  4  6  0  0 66   4  6  0  0 66 11994       0      0 0.00e+00    0 0.00e+00  0
VecNorm              535 1.0 1.0333e-01 1.8 5.35e+07 1.0 0.0e+00 0.0e+00 5.4e+02  2  3  0  0 33   2  3  0  0 33 10355       0      0 0.00e+00    0 0.00e+00  0
VecCopy                5 1.0 3.4466e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 4 1.0 2.6624e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             2131 1.0 1.2977e-01 1.1 2.13e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3 13  0  0  0   3 13  0  0  0 32843       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             1065 1.0 1.4900e-01 1.1 1.06e+08 1.0 0.0e+00 0.0e+00 0.0e+00  4  6  0  0  0   4  6  0  0  0 14282       0      0 0.00e+00    0 0.00e+00  0
VecWAXPY            2130 1.0 2.9675e-01 1.0 1.86e+08 1.0 0.0e+00 0.0e+00 0.0e+00  8 11  0  0  0   8 11  0  0  0 12560       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     1067 1.0 2.0291e-02 1.7 0.00e+00 0.0 4.1e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       1067 1.0 6.2958e-02 2.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.6860e-06 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.5904e-03 3.9 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      1067 1.0 1.8126e-02 1.8 0.00e+00 0.0 4.1e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        1067 1.0 6.1388e-02 2.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              1067 1.0 4.0234e-03 9.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            1067 1.0 6.4780e-04 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 1.8723e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 3.5174e+00 1.0 1.67e+09 1.0 4.1e+04 8.0e+03 1.6e+03 90100100100 99  90100100100 99  9519       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 3.5400e-07 2.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply             1067 1.0 1.6275e+00 1.0 5.29e+08 1.0 0.0e+00 0.0e+00 0.0e+00 41 32  0  0  0  41 32  0  0  0  6501       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    14             14      4832064     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1416     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.31e-08
Average time for MPI_Barrier(): 1.28714e-05
Average time for zero size MPI_Send(): 6.5563e-06
#PETSc Option Table entries:
-ksp_type tfqmr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type sor
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.0017322 iterations 281
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8080 with 20 processors, by luciano.siqueira Wed Sep  9 17:36:35 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           4.166e+00     1.000   4.166e+00
Objects:              4.700e+01     1.000   4.700e+01
Flop:                 1.665e+09     1.001   1.664e+09  3.329e+10
Flop/sec:             3.995e+08     1.001   3.995e+08  7.990e+09
MPI Messages:         2.276e+03     2.000   2.162e+03  4.324e+04
MPI Message Lengths:  1.818e+07     2.000   7.989e+03  3.455e+08
MPI Reductions:       8.860e+02     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 4.1664e+00 100.0%  3.3288e+10 100.0%  4.324e+04 100.0%  7.989e+03      100.0%  8.790e+02  99.2%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 6.1080e-0370.1 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 5.1686e-0394.6 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             1136 1.0 1.1979e+00 1.0 5.11e+08 1.0 4.3e+04 8.0e+03 0.0e+00 28 31100100  0  28 31100100  0  8527       0      0 0.00e+00    0 0.00e+00  0
MatSOR              1137 1.0 1.7756e+00 1.0 5.64e+08 1.0 0.0e+00 0.0e+00 0.0e+00 41 34  0  0  0  41 34  0  0  0  6350       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 5.2213e-0348.5 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6519e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  1   1  0  0  0  1     0       0      0 0.00e+00    0 0.00e+00  0
VecDot               562 1.0 1.0462e-01 1.5 5.62e+07 1.0 0.0e+00 0.0e+00 5.6e+02  2  3  0  0 63   2  3  0  0 64 10744       0      0 0.00e+00    0 0.00e+00  0
VecMDot               10 1.0 4.0698e-03 1.2 5.50e+06 1.0 0.0e+00 0.0e+00 1.0e+01  0  0  0  0  1   0  0  0  0  1 27028       0      0 0.00e+00    0 0.00e+00  0
VecNorm              294 1.0 7.2949e-02 2.2 2.94e+07 1.0 0.0e+00 0.0e+00 2.9e+02  1  2  0  0 33   1  2  0  0 33  8060       0      0 0.00e+00    0 0.00e+00  0
VecScale              11 1.0 2.5687e-04 1.1 5.50e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 42824       0      0 0.00e+00    0 0.00e+00  0
VecCopy             1132 1.0 5.6468e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet              1132 1.0 3.3131e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             1124 1.0 6.6946e-02 1.0 1.12e+08 1.0 0.0e+00 0.0e+00 0.0e+00  2  7  0  0  0   2  7  0  0  0 33580       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             1687 1.0 2.3251e-01 1.0 1.40e+08 1.0 0.0e+00 0.0e+00 0.0e+00  6  8  0  0  0   6  8  0  0  0 12081       0      0 0.00e+00    0 0.00e+00  0
VecAXPBYCZ           563 1.0 9.8560e-02 1.0 1.41e+08 1.0 0.0e+00 0.0e+00 0.0e+00  2  8  0  0  0   2  8  0  0  0 28561       0      0 0.00e+00    0 0.00e+00  0
VecWAXPY            1122 1.0 1.5950e-01 1.0 9.82e+07 1.0 0.0e+00 0.0e+00 0.0e+00  4  6  0  0  0   4  6  0  0  0 12307       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY              11 1.0 4.6857e-03 1.0 6.50e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 27744       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     1136 1.0 1.9246e-02 1.5 0.00e+00 0.0 4.3e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       1136 1.0 6.7817e-02 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecNormalize          11 1.0 7.2483e-03 1.1 1.65e+06 1.0 0.0e+00 0.0e+00 1.1e+01  0  0  0  0  1   0  0  0  0  1  4553       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.7500e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.0563e-03 2.7 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      1136 1.0 1.7517e-02 1.5 0.00e+00 0.0 4.3e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        1136 1.0 6.6412e-02 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              1136 1.0 3.8997e-03 8.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            1136 1.0 5.3929e-04 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.3975e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  3  1  1  1  3   3  1  1  1  3  3399       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 3.7919e+00 1.0 1.66e+09 1.0 4.3e+04 8.0e+03 8.7e+02 91100100100 98  91100100100 99  8775       0      0 0.00e+00    0 0.00e+00  0
KSPGMRESOrthog        10 1.0 1.2039e-02 1.1 1.10e+07 1.0 0.0e+00 0.0e+00 1.0e+01  0  1  0  0  1   0  1  0  0  1 18274       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 1.3815e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  3  1  1  1  3   3  1  1  1  3  3438       0      0 0.00e+00    0 0.00e+00  0
PCApply              574 1.0 2.6931e+00 1.0 1.04e+09 1.0 2.1e+04 8.0e+03 0.0e+00 63 63 49 50  0  63 63 49 50  0  7739       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    33             33     12464896     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     3              3        33432     0.
      Preconditioner     3              3         2936     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.79e-08
Average time for MPI_Barrier(): 1.07732e-05
Average time for zero size MPI_Send(): 6.36925e-06
#PETSc Option Table entries:
-ksp_type tfqmr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.00115167 iterations 573
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8080 with 20 processors, by luciano.siqueira Wed Sep  9 17:36:39 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           2.049e+00     1.000   2.049e+00
Objects:              2.900e+01     1.000   2.900e+01
Flop:                 9.164e+08     1.001   9.163e+08  1.833e+10
Flop/sec:             4.472e+08     1.001   4.471e+08  8.943e+09
MPI Messages:         1.154e+03     2.000   1.096e+03  2.193e+04
MPI Message Lengths:  9.208e+06     2.000   7.979e+03  1.750e+08
MPI Reductions:       1.166e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 2.0492e+00 100.0%  1.8325e+10 100.0%  2.193e+04 100.0%  7.979e+03      100.0%  1.159e+03  99.4%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 4.2940e-0330.3 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 4.1800e-0385.5 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult              575 1.0 6.1740e-01 1.0 2.59e+08 1.0 2.2e+04 8.0e+03 0.0e+00 30 28100100  0  30 28100100  0  8375       0      0 0.00e+00    0 0.00e+00  0
MatSolve             574 1.0 6.6883e-01 1.0 2.56e+08 1.0 0.0e+00 0.0e+00 0.0e+00 32 28  0  0  0  32 28  0  0  0  7652       0      0 0.00e+00    0 0.00e+00  0
MatLUFactorNum         1 1.0 7.6270e-03 1.0 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1423       0      0 0.00e+00    0 0.00e+00  0
MatILUFactorSym        1 1.0 3.8796e-03 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 4.2327e-0342.2 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6954e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetRowIJ            1 1.0 1.0060e-05 4.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetOrdering         1 1.0 7.0096e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot               573 1.0 8.2139e-02 1.7 5.73e+07 1.0 0.0e+00 0.0e+00 5.7e+02  3  6  0  0 49   3  6  0  0 49 13952       0      0 0.00e+00    0 0.00e+00  0
VecNorm                1 1.0 8.9596e-05 1.1 1.00e+05 1.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0 22322       0      0 0.00e+00    0 0.00e+00  0
VecCopy                3 1.0 2.3768e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet               577 1.0 1.0344e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             1147 1.0 5.9424e-02 1.1 1.15e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3 13  0  0  0   3 13  0  0  0 38604       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             1144 1.0 1.4906e-01 1.0 1.14e+08 1.0 0.0e+00 0.0e+00 0.0e+00  7 12  0  0  0   7 12  0  0  0 15350       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin      575 1.0 9.0699e-03 1.4 0.00e+00 0.0 2.2e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd        575 1.0 3.8054e-02 3.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecReduceArith      1148 1.0 8.2046e-02 1.1 1.15e+08 1.0 0.0e+00 0.0e+00 0.0e+00  4 13  0  0  0   4 13  0  0  0 27984       0      0 0.00e+00    0 0.00e+00  0
VecReduceComm        574 1.0 3.2241e-02 2.7 0.00e+00 0.0 0.0e+00 0.0e+00 5.7e+02  1  0  0  0 49   1  0  0  0 50     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.8830e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.4645e-03 3.4 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin       575 1.0 8.2650e-03 1.5 0.00e+00 0.0 2.2e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd         575 1.0 3.7436e-02 3.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack               575 1.0 1.7016e-03 7.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack             575 1.0 3.1961e-04 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.2770e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 1.6776e+00 1.0 9.16e+08 1.0 2.2e+04 8.0e+03 1.1e+03 82100 99100 98  82100 99100 99 10916       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                2 1.0 2.6401e-02 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0   411       0      0 0.00e+00    0 0.00e+00  0
PCSetUpOnBlocks        1 1.0 1.2092e-02 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0   898       0      0 0.00e+00    0 0.00e+00  0
PCApply              574 1.0 6.8233e-01 1.0 2.56e+08 1.0 0.0e+00 0.0e+00 0.0e+00 33 28  0  0  0  33 28  0  0  0  7500       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              4     11791508     0.
         Vec Scatter     1              1          816     0.
              Vector    13             13      3630080     0.
           Index Set     5              5       608512     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     2              2         2832     0.
      Preconditioner     2              2         1944     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.04e-08
Average time for MPI_Barrier(): 1.17794e-05
Average time for zero size MPI_Send(): 6.76555e-06
#PETSc Option Table entries:
-ksp_type cr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type bjacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.000525242 iterations 1635
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8080 with 20 processors, by luciano.siqueira Wed Sep  9 17:36:44 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.316e+00     1.000   3.316e+00
Objects:              2.200e+01     1.000   2.200e+01
Flop:                 1.963e+09     1.002   1.962e+09  3.925e+10
Flop/sec:             5.919e+08     1.002   5.918e+08  1.184e+10
MPI Messages:         3.278e+03     2.000   3.114e+03  6.228e+04
MPI Message Lengths:  2.620e+07     2.000   7.993e+03  4.978e+08
MPI Reductions:       3.292e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.3162e+00 100.0%  3.9250e+10 100.0%  6.228e+04 100.0%  7.993e+03      100.0%  3.285e+03  99.8%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 6.9946e-0371.5 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 6.9674e-03119.6 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             1637 1.0 1.7420e+00 1.0 7.36e+08 1.0 6.2e+04 8.0e+03 0.0e+00 52 38100100  0  52 38100100  0  8450       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 7.0203e-0364.2 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6980e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot              1635 1.0 1.7061e-01 1.3 1.63e+08 1.0 0.0e+00 0.0e+00 1.6e+03  5  8  0  0 50   5  8  0  0 50 19166       0      0 0.00e+00    0 0.00e+00  0
VecNorm                1 1.0 8.5077e-05 1.1 1.00e+05 1.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0 23508       0      0 0.00e+00    0 0.00e+00  0
VecCopy                3 1.0 2.1546e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 3 1.0 2.1952e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             3271 1.0 1.5622e-01 1.1 3.27e+08 1.0 0.0e+00 0.0e+00 0.0e+00  5 17  0  0  0   5 17  0  0  0 41877       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             3268 1.0 4.2591e-01 1.1 3.27e+08 1.0 0.0e+00 0.0e+00 0.0e+00 12 17  0  0  0  12 17  0  0  0 15346       0      0 0.00e+00    0 0.00e+00  0
VecPointwiseMult    1636 1.0 2.3643e-01 1.1 8.18e+07 1.0 0.0e+00 0.0e+00 0.0e+00  7  4  0  0  0   7  4  0  0  0  6920       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     1637 1.0 1.9075e-02 1.6 0.00e+00 0.0 6.2e+04 8.0e+03 0.0e+00  1  0100100  0   1  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       1637 1.0 1.4180e-01 4.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  3  0  0  0  0   3  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecReduceArith      3272 1.0 2.1864e-01 1.1 3.27e+08 1.0 0.0e+00 0.0e+00 0.0e+00  6 17  0  0  0   6 17  0  0  0 29930       0      0 0.00e+00    0 0.00e+00  0
VecReduceComm       1636 1.0 6.5388e-02 2.4 0.00e+00 0.0 0.0e+00 0.0e+00 1.6e+03  1  0  0  0 50   1  0  0  0 50     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.8260e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.4077e-03 3.6 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      1637 1.0 1.7228e-02 1.6 0.00e+00 0.0 6.2e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        1637 1.0 1.4020e-01 4.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  3  0  0  0  0   3  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              1637 1.0 4.0568e-03 8.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            1637 1.0 7.8730e-04 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 1.2499e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.9466e+00 1.0 1.96e+09 1.0 6.2e+04 8.0e+03 3.3e+03 89100100100 99  89100100100100 13316       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 9.9100e-07 3.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply             1636 1.0 2.4133e-01 1.1 8.18e+07 1.0 0.0e+00 0.0e+00 2.0e+00  7  4  0  0  0   7  4  0  0  0  6779       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      8013484     0.
         Vec Scatter     1              1          816     0.
              Vector    12             12      4028608     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1416     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3e-08
Average time for MPI_Barrier(): 1.13442e-05
Average time for zero size MPI_Send(): 6.6534e-06
#PETSc Option Table entries:
-ksp_type cr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type jacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.000702134 iterations 672
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8080 with 20 processors, by luciano.siqueira Wed Sep  9 17:36:49 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           2.674e+00     1.000   2.674e+00
Objects:              2.100e+01     1.000   2.100e+01
Flop:                 1.107e+09     1.001   1.107e+09  2.215e+10
Flop/sec:             4.142e+08     1.001   4.141e+08  8.282e+09
MPI Messages:         1.352e+03     2.000   1.284e+03  2.569e+04
MPI Message Lengths:  1.079e+07     2.000   7.982e+03  2.050e+08
MPI Reductions:       1.364e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 2.6740e+00 100.0%  2.2147e+10 100.0%  2.569e+04 100.0%  7.982e+03      100.0%  1.357e+03  99.5%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 5.0258e-0349.2 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 3.6008e-0380.4 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult              674 1.0 7.4119e-01 1.0 3.03e+08 1.0 2.6e+04 8.0e+03 0.0e+00 27 27100100  0  27 27100100  0  8177       0      0 0.00e+00    0 0.00e+00  0
MatSOR               673 1.0 1.0628e+00 1.0 3.34e+08 1.0 0.0e+00 0.0e+00 0.0e+00 39 30  0  0  0  39 30  0  0  0  6280       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 3.6509e-0338.1 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6537e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot               672 1.0 1.3621e-01 1.7 6.72e+07 1.0 0.0e+00 0.0e+00 6.7e+02  4  6  0  0 49   4  6  0  0 50  9867       0      0 0.00e+00    0 0.00e+00  0
VecNorm                1 1.0 9.7972e-05 1.2 1.00e+05 1.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0 20414       0      0 0.00e+00    0 0.00e+00  0
VecCopy                3 1.0 2.2537e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 3 1.0 2.1240e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             1345 1.0 1.0569e-01 1.1 1.34e+08 1.0 0.0e+00 0.0e+00 0.0e+00  4 12  0  0  0   4 12  0  0  0 25452       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             1342 1.0 1.8726e-01 1.1 1.34e+08 1.0 0.0e+00 0.0e+00 0.0e+00  7 12  0  0  0   7 12  0  0  0 14333       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin      674 1.0 1.5721e-02 1.7 0.00e+00 0.0 2.6e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd        674 1.0 5.6393e-02 3.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecReduceArith      1346 1.0 1.0808e-01 1.1 1.35e+08 1.0 0.0e+00 0.0e+00 0.0e+00  4 12  0  0  0   4 12  0  0  0 24907       0      0 0.00e+00    0 0.00e+00  0
VecReduceComm        673 1.0 5.9502e-02 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 6.7e+02  2  0  0  0 49   2  0  0  0 50     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.9810e-06 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.5433e-03 3.7 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin       674 1.0 1.3746e-02 1.8 0.00e+00 0.0 2.6e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd         674 1.0 5.5473e-02 4.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack               674 1.0 2.9349e-03 7.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack             674 1.0 4.6111e-04 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 1.2835e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.3128e+00 1.0 1.11e+09 1.0 2.6e+04 8.0e+03 1.3e+03 86100100100 99  86100100100 99  9570       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 3.8800e-07 2.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply              673 1.0 1.0645e+00 1.0 3.34e+08 1.0 0.0e+00 0.0e+00 0.0e+00 39 30  0  0  0  39 30  0  0  0  6270       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    11             11      3626880     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1416     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.97e-08
Average time for MPI_Barrier(): 1.50396e-05
Average time for zero size MPI_Send(): 6.29895e-06
#PETSc Option Table entries:
-ksp_type cr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type sor
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.000656407 iterations 374
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8080 with 20 processors, by luciano.siqueira Wed Sep  9 17:36:54 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.071e+00     1.000   3.071e+00
Objects:              4.400e+01     1.000   4.400e+01
Flop:                 1.145e+09     1.001   1.145e+09  2.290e+10
Flop/sec:             3.729e+08     1.001   3.729e+08  7.457e+09
MPI Messages:         1.526e+03     2.000   1.450e+03  2.899e+04
MPI Message Lengths:  1.218e+07     2.000   7.984e+03  2.315e+08
MPI Reductions:       7.910e+02     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.0714e+00 100.0%  2.2905e+10 100.0%  2.899e+04 100.0%  7.984e+03      100.0%  7.840e+02  99.1%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 3.5632e-0334.4 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 3.5199e-0375.2 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult              761 1.0 8.7948e-01 1.1 3.42e+08 1.0 2.9e+04 8.0e+03 0.0e+00 28 30100100  0  28 30100100  0  7781       0      0 0.00e+00    0 0.00e+00  0
MatSOR               761 1.0 1.2186e+00 1.1 3.77e+08 1.0 0.0e+00 0.0e+00 0.0e+00 38 33  0  0  0  38 33  0  0  0  6193       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 3.5739e-0335.8 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6214e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  1   1  0  0  0  1     0       0      0 0.00e+00    0 0.00e+00  0
VecDot               374 1.0 1.0656e-01 2.5 3.74e+07 1.0 0.0e+00 0.0e+00 3.7e+02  3  3  0  0 47   3  3  0  0 48  7020       0      0 0.00e+00    0 0.00e+00  0
VecMDot               10 1.0 5.5761e-03 1.5 5.50e+06 1.0 0.0e+00 0.0e+00 1.0e+01  0  0  0  0  1   0  0  0  0  1 19727       0      0 0.00e+00    0 0.00e+00  0
VecNorm               12 1.0 1.1121e-02 1.5 1.20e+06 1.0 0.0e+00 0.0e+00 1.2e+01  0  0  0  0  2   0  0  0  0  2  2158       0      0 0.00e+00    0 0.00e+00  0
VecScale              11 1.0 2.7608e-04 1.1 5.50e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 39843       0      0 0.00e+00    0 0.00e+00  0
VecCopy              754 1.0 3.6449e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet               755 1.0 2.0777e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY              750 1.0 7.4965e-02 1.1 7.50e+07 1.0 0.0e+00 0.0e+00 0.0e+00  2  7  0  0  0   2  7  0  0  0 20009       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             1496 1.0 2.1311e-01 1.1 1.31e+08 1.0 0.0e+00 0.0e+00 0.0e+00  7 11  0  0  0   7 11  0  0  0 12280       0      0 0.00e+00    0 0.00e+00  0
VecAXPBYCZ           375 1.0 6.7880e-02 1.1 9.38e+07 1.0 0.0e+00 0.0e+00 0.0e+00  2  8  0  0  0   2  8  0  0  0 27622       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY              11 1.0 5.0283e-03 1.1 6.50e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 25854       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin      761 1.0 1.7430e-02 1.7 0.00e+00 0.0 2.9e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd        761 1.0 1.1239e-01 4.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecReduceArith       750 1.0 5.7699e-02 1.2 7.50e+07 1.0 0.0e+00 0.0e+00 0.0e+00  2  7  0  0  0   2  7  0  0  0 25997       0      0 0.00e+00    0 0.00e+00  0
VecReduceComm        375 1.0 2.5511e-02 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 3.8e+02  1  0  0  0 47   1  0  0  0 48     0       0      0 0.00e+00    0 0.00e+00  0
VecNormalize          11 1.0 1.1324e-02 1.5 1.65e+06 1.0 0.0e+00 0.0e+00 1.1e+01  0  0  0  0  1   0  0  0  0  1  2914       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.5370e-06 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.4331e-03 3.7 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin       761 1.0 1.5417e-02 1.7 0.00e+00 0.0 2.9e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd         761 1.0 1.1144e-01 4.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack               761 1.0 3.0118e-03 8.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack             761 1.0 4.1747e-04 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.4870e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  5  2  1  1  3   5  2  1  1  3  3194       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.7027e+00 1.0 1.14e+09 1.0 2.9e+04 8.0e+03 7.7e+02 88100100100 98  88100100100 98  8470       0      0 0.00e+00    0 0.00e+00  0
KSPGMRESOrthog        10 1.0 1.7159e-02 1.4 1.10e+07 1.0 0.0e+00 0.0e+00 1.0e+01  0  1  0  0  1   0  1  0  0  1 12821       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 1.4771e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  5  2  1  1  3   5  2  1  1  3  3216       0      0 0.00e+00    0 0.00e+00  0
PCApply              386 1.0 1.8675e+00 1.0 6.96e+08 1.0 1.4e+04 8.0e+03 0.0e+00 60 61 49 49  0  60 61 49 49  0  7453       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    30             30     11259712     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     3              3        33432     0.
      Preconditioner     3              3         2936     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.93e-08
Average time for MPI_Barrier(): 1.229e-05
Average time for zero size MPI_Send(): 6.26005e-06
#PETSc Option Table entries:
-ksp_type cr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.0311837 iterations 7307
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8080 with 20 processors, by luciano.siqueira Wed Sep  9 17:37:34 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.849e+01     1.000   3.849e+01
Objects:              8.400e+01     1.000   8.400e+01
Flop:                 4.268e+10     1.000   4.268e+10  8.536e+11
Flop/sec:             1.109e+09     1.000   1.109e+09  2.218e+10
MPI Messages:         1.462e+04     2.000   1.389e+04  2.778e+05
MPI Message Lengths:  1.170e+08     2.000   7.998e+03  2.222e+09
MPI Reductions:       2.170e+04     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.8488e+01 100.0%  8.5362e+11 100.0%  2.778e+05 100.0%  7.998e+03      100.0%  2.169e+04 100.0%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 4.5163e-0352.3 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 3.8338e-0370.4 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             7309 1.0 7.4589e+00 1.0 3.29e+09 1.0 2.8e+05 8.0e+03 0.0e+00 19  8100100  0  19  8100100  0  8811       0      0 0.00e+00    0 0.00e+00  0
MatSolve            7307 1.0 8.0156e+00 1.0 3.26e+09 1.0 0.0e+00 0.0e+00 0.0e+00 21  8  0  0  0  21  8  0  0  0  8128       0      0 0.00e+00    0 0.00e+00  0
MatLUFactorNum         1 1.0 7.5566e-03 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1436       0      0 0.00e+00    0 0.00e+00  0
MatILUFactorSym        1 1.0 3.4534e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 3.8854e-0336.0 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6271e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetRowIJ            1 1.0 3.1350e-06 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetOrdering         1 1.0 7.0093e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDotNorm2         7307 1.0 1.5244e+00 1.1 1.46e+09 1.0 0.0e+00 0.0e+00 7.3e+03  4  3  0  0 34   4  3  0  0 34 19174       0      0 0.00e+00    0 0.00e+00  0
VecMDot             7063 1.0 5.6564e+00 1.0 1.06e+10 1.0 0.0e+00 0.0e+00 7.1e+03 14 25  0  0 33  14 25  0  0 33 37423       0      0 0.00e+00    0 0.00e+00  0
VecNorm             7309 1.0 5.9740e-01 1.1 7.31e+08 1.0 0.0e+00 0.0e+00 7.3e+03  2  2  0  0 34   2  2  0  0 34 24470       0      0 0.00e+00    0 0.00e+00  0
VecScale           14614 1.0 3.1722e-01 1.0 7.31e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1  2  0  0  0   1  2  0  0  0 46069       0      0 0.00e+00    0 0.00e+00  0
VecSet              7310 1.0 4.5431e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY            14615 1.0 7.0789e-01 1.1 1.46e+09 1.0 0.0e+00 0.0e+00 0.0e+00  2  3  0  0  0   2  3  0  0  0 41292       0      0 0.00e+00    0 0.00e+00  0
VecAYPX                1 1.0 1.7461e-04 1.2 5.00e+04 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  5727       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY           14126 1.0 1.3536e+01 1.0 2.12e+10 1.0 0.0e+00 0.0e+00 0.0e+00 35 50  0  0  0  35 50  0  0  0 31278       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     7309 1.0 1.0864e-01 1.5 0.00e+00 0.0 2.8e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       7309 1.0 2.6264e-01 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.6760e-06 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 2.1579e-03 5.7 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      7309 1.0 9.4642e-02 1.6 0.00e+00 0.0 2.8e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        7309 1.0 2.5262e-01 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              7309 1.0 2.5566e-02 8.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            7309 1.0 4.1080e-03 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.5053e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 3.8106e+01 1.0 4.27e+10 1.0 2.8e+05 8.0e+03 2.2e+04 99100100100100  99100100100100 22401       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                2 1.0 2.6288e-02 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   413       0      0 0.00e+00    0 0.00e+00  0
PCSetUpOnBlocks        1 1.0 1.1773e-02 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   922       0      0 0.00e+00    0 0.00e+00  0
PCApply             7307 1.0 8.5344e+00 1.0 3.26e+09 1.0 0.0e+00 0.0e+00 0.0e+00 22  8  0  0  0  22  8  0  0  0  7634       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              4     11791508     0.
         Vec Scatter     1              1          816     0.
              Vector    68             68     25725120     0.
           Index Set     5              5       608512     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     2              2         2896     0.
      Preconditioner     2              2         1944     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.85e-08
Average time for MPI_Barrier(): 1.42836e-05
Average time for zero size MPI_Send(): 6.4691e-06
#PETSc Option Table entries:
-ksp_type gcr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type bjacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 186.34 iterations 10000
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8080 with 20 processors, by luciano.siqueira Wed Sep  9 17:38:18 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           4.235e+01     1.000   4.235e+01
Objects:              7.700e+01     1.000   7.700e+01
Flop:                 5.447e+10     1.000   5.447e+10  1.089e+12
Flop/sec:             1.286e+09     1.000   1.286e+09  2.572e+10
MPI Messages:         2.001e+04     2.000   1.901e+04  3.802e+05
MPI Message Lengths:  1.600e+08     2.000   7.999e+03  3.041e+09
MPI Reductions:       2.969e+04     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 4.2349e+01 100.0%  1.0893e+12 100.0%  3.802e+05 100.0%  7.999e+03      100.0%  2.968e+04 100.0%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 3.2674e-03 8.0 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 2.5937e-0358.6 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult            10002 1.0 1.0269e+01 1.0 4.50e+09 1.0 3.8e+05 8.0e+03 0.0e+00 24  8100100  0  24  8100100  0  8758       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 2.6466e-0327.8 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6736e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDotNorm2        10000 1.0 2.1564e+00 1.1 2.00e+09 1.0 0.0e+00 0.0e+00 1.0e+04  5  4  0  0 34   5  4  0  0 34 18550       0      0 0.00e+00    0 0.00e+00  0
VecMDot             9666 1.0 7.4851e+00 1.0 1.45e+10 1.0 0.0e+00 0.0e+00 9.7e+03 17 27  0  0 33  17 27  0  0 33 38716       0      0 0.00e+00    0 0.00e+00  0
VecNorm            10002 1.0 8.2593e-01 1.1 1.00e+09 1.0 0.0e+00 0.0e+00 1.0e+04  2  2  0  0 34   2  2  0  0 34 24220       0      0 0.00e+00    0 0.00e+00  0
VecScale           20000 1.0 4.6217e-01 1.0 1.00e+09 1.0 0.0e+00 0.0e+00 0.0e+00  1  2  0  0  0   1  2  0  0  0 43274       0      0 0.00e+00    0 0.00e+00  0
VecSet                 3 1.0 2.1934e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY            20001 1.0 9.6867e-01 1.1 2.00e+09 1.0 0.0e+00 0.0e+00 0.0e+00  2  4  0  0  0   2  4  0  0  0 41296       0      0 0.00e+00    0 0.00e+00  0
VecAYPX                1 1.0 1.7005e-04 1.1 5.00e+04 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  5881       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY           19332 1.0 1.8688e+01 1.0 2.90e+10 1.0 0.0e+00 0.0e+00 0.0e+00 44 53  0  0  0  44 53  0  0  0 31015       0      0 0.00e+00    0 0.00e+00  0
VecPointwiseMult   10000 1.0 1.5136e+00 1.0 5.00e+08 1.0 0.0e+00 0.0e+00 0.0e+00  4  1  0  0  0   4  1  0  0  0  6607       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin    10002 1.0 1.4507e-01 1.4 0.00e+00 0.0 3.8e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd      10002 1.0 3.3124e-01 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.6840e-06 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.1724e-03 2.9 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin     10002 1.0 1.2879e-01 1.6 0.00e+00 0.0 3.8e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd       10002 1.0 3.1891e-01 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack             10002 1.0 3.8570e-02 8.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack           10002 1.0 5.3862e-03 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 1.7452e-02 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 4.1978e+01 1.0 5.45e+10 1.0 3.8e+05 8.0e+03 3.0e+04 99100100100100  99100100100100 25950       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 1.1840e-06 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply            10000 1.0 1.5263e+00 1.0 5.00e+08 1.0 0.0e+00 0.0e+00 2.0e+00  4  1  0  0  0   4  1  0  0  0  6552       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      8013484     0.
         Vec Scatter     1              1          816     0.
              Vector    67             67     26123648     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1480     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.76e-08
Average time for MPI_Barrier(): 1.04642e-05
Average time for zero size MPI_Send(): 6.38705e-06
#PETSc Option Table entries:
-ksp_type gcr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type jacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.0316741 iterations 6946
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8080 with 20 processors, by luciano.siqueira Wed Sep  9 17:39:00 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.980e+01     1.000   3.980e+01
Objects:              7.600e+01     1.000   7.600e+01
Flop:                 4.092e+10     1.000   4.092e+10  8.183e+11
Flop/sec:             1.028e+09     1.000   1.028e+09  2.056e+10
MPI Messages:         1.390e+04     2.000   1.320e+04  2.641e+05
MPI Message Lengths:  1.112e+08     2.000   7.998e+03  2.112e+09
MPI Reductions:       2.063e+04     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.9804e+01 100.0%  8.1834e+11 100.0%  2.641e+05 100.0%  7.998e+03      100.0%  2.062e+04 100.0%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 7.5815e-0369.3 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 6.3695e-0393.9 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             6948 1.0 7.1442e+00 1.0 3.13e+09 1.0 2.6e+05 8.0e+03 0.0e+00 18  8100100  0  18  8100100  0  8745       0      0 0.00e+00    0 0.00e+00  0
MatSOR              6946 1.0 1.0801e+01 1.0 3.44e+09 1.0 0.0e+00 0.0e+00 0.0e+00 27  8  0  0  0  27  8  0  0  0  6377       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 6.4212e-0353.3 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6440e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDotNorm2         6946 1.0 1.4979e+00 1.1 1.39e+09 1.0 0.0e+00 0.0e+00 6.9e+03  4  3  0  0 34   4  3  0  0 34 18548       0      0 0.00e+00    0 0.00e+00  0
VecMDot             6714 1.0 5.6975e+00 1.0 1.01e+10 1.0 0.0e+00 0.0e+00 6.7e+03 14 25  0  0 33  14 25  0  0 33 35315       0      0 0.00e+00    0 0.00e+00  0
VecNorm             6948 1.0 5.8777e-01 1.1 6.95e+08 1.0 0.0e+00 0.0e+00 6.9e+03  1  2  0  0 34   1  2  0  0 34 23642       0      0 0.00e+00    0 0.00e+00  0
VecScale           13892 1.0 3.2957e-01 1.1 6.95e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1  2  0  0  0   1  2  0  0  0 42152       0      0 0.00e+00    0 0.00e+00  0
VecSet                 3 1.0 2.1940e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY            13893 1.0 7.0185e-01 1.1 1.39e+09 1.0 0.0e+00 0.0e+00 0.0e+00  2  3  0  0  0   2  3  0  0  0 39590       0      0 0.00e+00    0 0.00e+00  0
VecAYPX                1 1.0 1.6773e-04 1.0 5.00e+04 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  5962       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY           13428 1.0 1.2981e+01 1.0 2.01e+10 1.0 0.0e+00 0.0e+00 0.0e+00 33 49  0  0  0  33 49  0  0  0 31000       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     6948 1.0 1.1134e-01 1.5 0.00e+00 0.0 2.6e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       6948 1.0 3.0829e-01 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.5640e-06 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.3876e-03 3.5 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      6948 1.0 9.7574e-02 1.6 0.00e+00 0.0 2.6e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        6948 1.0 3.0003e-01 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              6948 1.0 2.7989e-02 8.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            6948 1.0 3.3688e-03 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 1.4581e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 3.9431e+01 1.0 4.09e+10 1.0 2.6e+05 8.0e+03 2.1e+04 99100100100100  99100100100100 20753       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 6.8000e-07 3.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply             6946 1.0 1.0816e+01 1.0 3.44e+09 1.0 0.0e+00 0.0e+00 0.0e+00 27  8  0  0  0  27  8  0  0  0  6368       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    66             66     25721920     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1480     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.3e-08
Average time for MPI_Barrier(): 1.07902e-05
Average time for zero size MPI_Send(): 6.6614e-06
#PETSc Option Table entries:
-ksp_type gcr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type sor
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.0315697 iterations 3254
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8080 with 20 processors, by luciano.siqueira Wed Sep  9 17:39:31 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           2.925e+01     1.000   2.925e+01
Objects:              9.900e+01     1.000   9.900e+01
Flop:                 2.355e+10     1.001   2.355e+10  4.711e+11
Flop/sec:             8.052e+08     1.001   8.051e+08  1.610e+10
MPI Messages:         1.304e+04     2.000   1.239e+04  2.478e+05
MPI Message Lengths:  1.043e+08     2.000   7.998e+03  1.982e+09
MPI Reductions:       9.698e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 2.9253e+01 100.0%  4.7106e+11 100.0%  2.478e+05 100.0%  7.998e+03      100.0%  9.691e+03  99.9%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 5.0944e-0351.0 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 5.0627e-03104.0 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             6520 1.0 6.8111e+00 1.0 2.93e+09 1.0 2.5e+05 8.0e+03 0.0e+00 23 12100100  0  23 12100100  0  8608       0      0 0.00e+00    0 0.00e+00  0
MatSOR              6519 1.0 9.9896e+00 1.0 3.23e+09 1.0 0.0e+00 0.0e+00 0.0e+00 34 14  0  0  0  34 14  0  0  0  6471       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 5.1154e-0350.3 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6514e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDotNorm2         3254 1.0 7.3209e-01 1.1 6.51e+08 1.0 0.0e+00 0.0e+00 3.3e+03  2  3  0  0 34   2  3  0  0 34 17779       0      0 0.00e+00    0 0.00e+00  0
VecMDot             3155 1.0 2.8135e+00 1.1 4.71e+09 1.0 0.0e+00 0.0e+00 3.2e+03  9 20  0  0 33   9 20  0  0 33 33499       0      0 0.00e+00    0 0.00e+00  0
VecNorm             3267 1.0 2.9062e-01 1.1 3.27e+08 1.0 0.0e+00 0.0e+00 3.3e+03  1  1  0  0 34   1  1  0  0 34 22483       0      0 0.00e+00    0 0.00e+00  0
VecScale            6519 1.0 1.5553e-01 1.1 3.26e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1  1  0  0  0   1  1  0  0  0 41915       0      0 0.00e+00    0 0.00e+00  0
VecCopy             6509 1.0 2.8761e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet              6513 1.0 2.5438e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             6510 1.0 3.2688e-01 1.0 6.51e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1  3  0  0  0   1  3  0  0  0 39831       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             6509 1.0 8.7679e-01 1.0 4.88e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3  2  0  0  0   3  2  0  0  0 11135       0      0 0.00e+00    0 0.00e+00  0
VecAXPBYCZ          3254 1.0 5.5736e-01 1.0 8.14e+08 1.0 0.0e+00 0.0e+00 0.0e+00  2  3  0  0  0   2  3  0  0  0 29191       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY            6301 1.0 6.1106e+00 1.0 9.42e+09 1.0 0.0e+00 0.0e+00 0.0e+00 21 40  0  0  0  21 40  0  0  0 30834       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     6520 1.0 1.0556e-01 1.5 0.00e+00 0.0 2.5e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       6520 1.0 4.3208e-01 2.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecNormalize          11 1.0 7.8085e-03 1.2 1.65e+06 1.0 0.0e+00 0.0e+00 1.1e+01  0  0  0  0  0   0  0  0  0  0  4226       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.6730e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.2766e-03 3.1 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      6520 1.0 9.4193e-02 1.6 0.00e+00 0.0 2.5e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        6520 1.0 4.2376e-01 2.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              6520 1.0 2.1822e-02 7.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            6520 1.0 3.6019e-03 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.5767e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.5e+01  1  0  0  0  0   1  0  0  0  0  3013       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.8878e+01 1.0 2.36e+10 1.0 2.5e+05 8.0e+03 9.7e+03 99100100100100  99100100100100 16312       0      0 0.00e+00    0 0.00e+00  0
KSPGMRESOrthog        10 1.0 1.3668e-02 1.1 1.10e+07 1.0 0.0e+00 0.0e+00 1.0e+01  0  0  0  0  0   0  0  0  0  0 16096       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 1.4341e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  0  0  0  0  0   0  0  0  0  0  3312       0      0 0.00e+00    0 0.00e+00  0
PCApply             3265 1.0 1.5280e+01 1.0 6.00e+09 1.0 1.2e+05 8.0e+03 0.0e+00 52 25 50 50  0  52 25 50 50  0  7849       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    85             85     33354752     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     3              3        33496     0.
      Preconditioner     3              3         2936     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 4.9e-08
Average time for MPI_Barrier(): 1.09488e-05
Average time for zero size MPI_Send(): 6.18245e-06
#PETSc Option Table entries:
-ksp_type gcr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

