/tmp/slurmd/job659925/slurm_script: line 13: nodeset: comando nÃ£o encontrado
Norm of error 0.000101716 iterations 641
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8078 with 20 processors, by luciano.siqueira Wed Sep  9 17:31:12 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           2.187e+00     1.000   2.187e+00
Objects:              2.600e+01     1.000   2.600e+01
Flop:                 9.603e+08     1.001   9.602e+08  1.920e+10
Flop/sec:             4.390e+08     1.001   4.390e+08  8.780e+09
MPI Messages:         1.288e+03     2.000   1.224e+03  2.447e+04
MPI Message Lengths:  1.028e+07     2.000   7.981e+03  1.953e+08
MPI Reductions:       1.943e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 2.1873e+00 100.0%  1.9204e+10 100.0%  2.447e+04 100.0%  7.981e+03      100.0%  1.936e+03  99.6%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 1.6064e-03 6.0 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 1.3237e-0323.6 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult              642 1.0 6.9906e-01 1.0 2.89e+08 1.0 2.4e+04 8.0e+03 0.0e+00 32 30100100  0  32 30100100  0  8258       0      0 0.00e+00    0 0.00e+00  0
MatSolve             642 1.0 7.5647e-01 1.0 2.86e+08 1.0 0.0e+00 0.0e+00 0.0e+00 34 30  0  0  0  34 30  0  0  0  7567       0      0 0.00e+00    0 0.00e+00  0
MatLUFactorNum         1 1.0 7.6408e-03 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1420       0      0 0.00e+00    0 0.00e+00  0
MatILUFactorSym        1 1.0 3.4632e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 1.3750e-0312.7 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.8037e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetRowIJ            1 1.0 2.7260e-06 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetOrdering         1 1.0 7.0444e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecTDot             1282 1.0 1.2064e-01 1.1 1.28e+08 1.0 0.0e+00 0.0e+00 1.3e+03  5 13  0  0 66   5 13  0  0 66 21253       0      0 0.00e+00    0 0.00e+00  0
VecNorm              643 1.0 7.2160e-02 1.1 6.43e+07 1.0 0.0e+00 0.0e+00 6.4e+02  3  7  0  0 33   3  7  0  0 33 17822       0      0 0.00e+00    0 0.00e+00  0
VecCopy                2 1.0 1.5371e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet               645 1.0 5.0717e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             1283 1.0 5.9376e-02 1.1 1.28e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3 13  0  0  0   3 13  0  0  0 43216       0      0 0.00e+00    0 0.00e+00  0
VecAYPX              640 1.0 8.3935e-02 1.0 6.40e+07 1.0 0.0e+00 0.0e+00 0.0e+00  4  7  0  0  0   4  7  0  0  0 15250       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin      642 1.0 1.0218e-02 1.5 0.00e+00 0.0 2.4e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd        642 1.0 3.0007e-02 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.7820e-06 2.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 6.3289e-04 1.9 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin       642 1.0 9.2660e-03 1.6 0.00e+00 0.0 2.4e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd         642 1.0 2.9361e-02 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack               642 1.0 1.5459e-03 5.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack             642 1.0 3.6453e-04 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 6.5179e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 1.8038e+00 1.0 9.60e+08 1.0 2.4e+04 8.0e+03 1.9e+03 82100100100 99  82100100100 99 10639       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                2 1.0 2.7671e-02 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0   392       0      0 0.00e+00    0 0.00e+00  0
PCSetUpOnBlocks        1 1.0 1.1853e-02 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0   916       0      0 0.00e+00    0 0.00e+00  0
PCApply              642 1.0 7.6570e-01 1.0 2.86e+08 1.0 0.0e+00 0.0e+00 0.0e+00 35 30  0  0  0  35 30  0  0  0  7476       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              4     11791508     0.
         Vec Scatter     1              1          816     0.
              Vector    10             10      2424896     0.
           Index Set     5              5       608512     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     2              2         2904     0.
      Preconditioner     2              2         1944     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.79e-08
Average time for MPI_Barrier(): 9.8058e-06
Average time for zero size MPI_Send(): 6.30055e-06
#PETSc Option Table entries:
-ksp_type cg
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type bjacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 4.69193e-05 iterations 1715
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8078 with 20 processors, by luciano.siqueira Wed Sep  9 17:31:17 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.197e+00     1.000   3.197e+00
Objects:              1.900e+01     1.000   1.900e+01
Flop:                 1.887e+09     1.002   1.887e+09  3.773e+10
Flop/sec:             5.901e+08     1.002   5.900e+08  1.180e+10
MPI Messages:         3.436e+03     2.000   3.264e+03  6.528e+04
MPI Message Lengths:  2.746e+07     2.000   7.993e+03  5.218e+08
MPI Reductions:       5.167e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.1975e+00 100.0%  3.7730e+10 100.0%  6.528e+04 100.0%  7.993e+03      100.0%  5.160e+03  99.9%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 5.1165e-0311.3 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 5.0843e-03105.2 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             1716 1.0 1.8079e+00 1.0 7.72e+08 1.0 6.5e+04 8.0e+03 0.0e+00 56 41100100  0  56 41100100  0  8535       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 5.1359e-0350.9 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6996e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecTDot             3430 1.0 2.9555e-01 1.1 3.43e+08 1.0 0.0e+00 0.0e+00 3.4e+03  8 18  0  0 66   8 18  0  0 66 23211       0      0 0.00e+00    0 0.00e+00  0
VecNorm             1717 1.0 1.5152e-01 1.1 1.72e+08 1.0 0.0e+00 0.0e+00 1.7e+03  5  9  0  0 33   5  9  0  0 33 22664       0      0 0.00e+00    0 0.00e+00  0
VecCopy                2 1.0 9.9513e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 3 1.0 2.0818e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             3431 1.0 1.5511e-01 1.1 3.43e+08 1.0 0.0e+00 0.0e+00 0.0e+00  5 18  0  0  0   5 18  0  0  0 44241       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             1714 1.0 2.2030e-01 1.0 1.71e+08 1.0 0.0e+00 0.0e+00 0.0e+00  7  9  0  0  0   7  9  0  0  0 15561       0      0 0.00e+00    0 0.00e+00  0
VecPointwiseMult    1716 1.0 2.4171e-01 1.1 8.58e+07 1.0 0.0e+00 0.0e+00 0.0e+00  7  5  0  0  0   7  5  0  0  0  7099       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     1716 1.0 2.0665e-02 2.0 0.00e+00 0.0 6.5e+04 8.0e+03 0.0e+00  1  0100100  0   1  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       1716 1.0 1.2836e-01 5.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.9310e-06 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 2.6672e-0314.2 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      1716 1.0 1.8899e-02 2.2 0.00e+00 0.0 6.5e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        1716 1.0 1.2667e-01 5.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              1716 1.0 3.8281e-03 8.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            1716 1.0 8.6007e-04 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 6.4992e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.8208e+00 1.0 1.89e+09 1.0 6.5e+04 8.0e+03 5.1e+03 88100100100100  88100100100100 13371       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 5.8900e-07 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply             1716 1.0 2.4416e-01 1.1 8.58e+07 1.0 0.0e+00 0.0e+00 2.0e+00  7  5  0  0  0   7  5  0  0  0  7028       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      8013484     0.
         Vec Scatter     1              1          816     0.
              Vector     9              9      2823424     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1488     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.97e-08
Average time for MPI_Barrier(): 1.12916e-05
Average time for zero size MPI_Send(): 6.33615e-06
#PETSc Option Table entries:
-ksp_type cg
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type jacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.00061416 iterations 692
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8078 with 20 processors, by luciano.siqueira Wed Sep  9 17:31:21 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           2.551e+00     1.000   2.551e+00
Objects:              1.800e+01     1.000   1.800e+01
Flop:                 1.071e+09     1.001   1.071e+09  2.141e+10
Flop/sec:             4.197e+08     1.001   4.196e+08  8.392e+09
MPI Messages:         1.390e+03     2.000   1.320e+03  2.641e+04
MPI Message Lengths:  1.110e+07     2.000   7.983e+03  2.108e+08
MPI Reductions:       2.096e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 2.5514e+00 100.0%  2.1412e+10 100.0%  2.641e+04 100.0%  7.983e+03      100.0%  2.089e+03  99.7%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 6.5698e-0393.1 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 4.1233e-0375.6 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult              693 1.0 7.3829e-01 1.0 3.12e+08 1.0 2.6e+04 8.0e+03 0.0e+00 29 29100100  0  29 29100100  0  8440       0      0 0.00e+00    0 0.00e+00  0
MatSOR               693 1.0 1.0731e+00 1.0 3.44e+08 1.0 0.0e+00 0.0e+00 0.0e+00 41 32  0  0  0  41 32  0  0  0  6404       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 4.1729e-0338.5 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7017e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecTDot             1384 1.0 1.4700e-01 1.1 1.38e+08 1.0 0.0e+00 0.0e+00 1.4e+03  5 13  0  0 66   5 13  0  0 66 18830       0      0 0.00e+00    0 0.00e+00  0
VecNorm              694 1.0 1.1315e-01 1.5 6.94e+07 1.0 0.0e+00 0.0e+00 6.9e+02  4  6  0  0 33   4  6  0  0 33 12267       0      0 0.00e+00    0 0.00e+00  0
VecCopy                2 1.0 1.2102e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 3 1.0 2.1033e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             1385 1.0 7.2056e-02 1.1 1.38e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3 13  0  0  0   3 13  0  0  0 38442       0      0 0.00e+00    0 0.00e+00  0
VecAYPX              691 1.0 9.4922e-02 1.0 6.91e+07 1.0 0.0e+00 0.0e+00 0.0e+00  4  6  0  0  0   4  6  0  0  0 14559       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin      693 1.0 1.1560e-02 1.7 0.00e+00 0.0 2.6e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd        693 1.0 4.4844e-02 2.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.4970e-06 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 2.5760e-0314.0 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin       693 1.0 1.0334e-02 1.7 0.00e+00 0.0 2.6e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd         693 1.0 4.4013e-02 2.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack               693 1.0 2.4053e-03 9.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack             693 1.0 4.9422e-04 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 6.5032e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.1800e+00 1.0 1.07e+09 1.0 2.6e+04 8.0e+03 2.1e+03 85100100100 99  85100100100 99  9816       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 4.4500e-07 3.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply              693 1.0 1.0747e+00 1.0 3.44e+08 1.0 0.0e+00 0.0e+00 0.0e+00 41 32  0  0  0  41 32  0  0  0  6395       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector     8              8      2421696     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1488     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.03e-08
Average time for MPI_Barrier(): 1.21994e-05
Average time for zero size MPI_Send(): 6.225e-06
#PETSc Option Table entries:
-ksp_type cg
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type sor
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.000603072 iterations 383
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8078 with 20 processors, by luciano.siqueira Wed Sep  9 17:31:26 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.090e+00     1.000   3.090e+00
Objects:              4.100e+01     1.000   4.100e+01
Flop:                 1.134e+09     1.001   1.133e+09  2.267e+10
Flop/sec:             3.668e+08     1.001   3.668e+08  7.336e+09
MPI Messages:         1.560e+03     2.000   1.482e+03  2.964e+04
MPI Message Lengths:  1.246e+07     2.000   7.985e+03  2.367e+08
MPI Reductions:       1.192e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.0901e+00 100.0%  2.2668e+10 100.0%  2.964e+04 100.0%  7.985e+03      100.0%  1.185e+03  99.4%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 3.5934e-0333.5 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 3.4797e-0369.6 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult              778 1.0 8.8442e-01 1.0 3.50e+08 1.0 3.0e+04 8.0e+03 0.0e+00 28 31100100  0  28 31100100  0  7910       0      0 0.00e+00    0 0.00e+00  0
MatSOR               779 1.0 1.2777e+00 1.0 3.86e+08 1.0 0.0e+00 0.0e+00 0.0e+00 41 34  0  0  0  41 34  0  0  0  6047       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 3.5339e-0334.2 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7299e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecMDot               10 1.0 4.1171e-03 1.2 5.50e+06 1.0 0.0e+00 0.0e+00 1.0e+01  0  0  0  0  1   0  0  0  0  1 26717       0      0 0.00e+00    0 0.00e+00  0
VecTDot              766 1.0 8.5880e-02 1.1 7.66e+07 1.0 0.0e+00 0.0e+00 7.7e+02  3  7  0  0 64   3  7  0  0 65 17839       0      0 0.00e+00    0 0.00e+00  0
VecNorm              396 1.0 7.5968e-02 1.7 3.96e+07 1.0 0.0e+00 0.0e+00 4.0e+02  2  3  0  0 33   2  3  0  0 33 10425       0      0 0.00e+00    0 0.00e+00  0
VecScale              11 1.0 2.6831e-04 1.0 5.50e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 40998       0      0 0.00e+00    0 0.00e+00  0
VecCopy              771 1.0 3.4826e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet               773 1.0 6.4919e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY              768 1.0 4.8121e-02 1.1 7.68e+07 1.0 0.0e+00 0.0e+00 0.0e+00  1  7  0  0  0   1  7  0  0  0 31919       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             1150 1.0 1.6818e-01 1.0 9.58e+07 1.0 0.0e+00 0.0e+00 0.0e+00  5  8  0  0  0   5  8  0  0  0 11392       0      0 0.00e+00    0 0.00e+00  0
VecAXPBYCZ           384 1.0 7.2011e-02 1.1 9.60e+07 1.0 0.0e+00 0.0e+00 0.0e+00  2  8  0  0  0   2  8  0  0  0 26663       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY              11 1.0 5.0256e-03 1.0 6.50e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 25868       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin      778 1.0 1.4079e-02 1.5 0.00e+00 0.0 3.0e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd        778 1.0 5.5353e-02 3.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecNormalize          11 1.0 7.8701e-03 1.1 1.65e+06 1.0 0.0e+00 0.0e+00 1.1e+01  0  0  0  0  1   0  0  0  0  1  4193       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.8610e-06 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.2250e-03 3.1 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin       778 1.0 1.2780e-02 1.6 0.00e+00 0.0 3.0e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd         778 1.0 5.4327e-02 3.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack               778 1.0 2.7094e-03 9.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack             778 1.0 4.1589e-04 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.4457e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  5  2  1  1  2   5  2  1  1  2  3286       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.7083e+00 1.0 1.13e+09 1.0 3.0e+04 8.0e+03 1.2e+03 88100100100 98  88100100100 99  8365       0      0 0.00e+00    0 0.00e+00  0
KSPGMRESOrthog        10 1.0 1.2708e-02 1.1 1.10e+07 1.0 0.0e+00 0.0e+00 1.0e+01  0  1  0  0  1   0  1  0  0  1 17312       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 1.4417e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  5  2  1  1  2   5  2  1  1  2  3295       0      0 0.00e+00    0 0.00e+00  0
PCApply              395 1.0 1.9381e+00 1.0 7.13e+08 1.0 1.5e+04 8.0e+03 0.0e+00 62 63 49 49  0  62 63 49 49  0  7353       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    27             27     10054528     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     3              3        33504     0.
      Preconditioner     3              3         2936     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.03e-08
Average time for MPI_Barrier(): 1.18404e-05
Average time for zero size MPI_Send(): 6.22035e-06
#PETSc Option Table entries:
-ksp_type cg
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.0125906 iterations 6472
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8078 with 20 processors, by luciano.siqueira Wed Sep  9 17:31:56 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           2.733e+01     1.000   2.733e+01
Objects:              5.700e+01     1.000   5.700e+01
Flop:                 2.773e+10     1.000   2.773e+10  5.546e+11
Flop/sec:             1.015e+09     1.000   1.014e+09  2.029e+10
MPI Messages:         1.338e+04     2.000   1.271e+04  2.542e+05
MPI Message Lengths:  1.070e+08     2.000   7.998e+03  2.033e+09
MPI Reductions:       1.318e+04     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 2.7333e+01 100.0%  5.5456e+11 100.0%  2.542e+05 100.0%  7.998e+03      100.0%  1.317e+04  99.9%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 5.4706e-0354.6 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 5.4316e-03100.8 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             6688 1.0 7.1398e+00 1.1 3.01e+09 1.0 2.5e+05 8.0e+03 0.0e+00 25 11100100  0  25 11100100  0  8423       0      0 0.00e+00    0 0.00e+00  0
MatSolve            6688 1.0 7.4284e+00 1.0 2.98e+09 1.0 0.0e+00 0.0e+00 0.0e+00 27 11  0  0  0  27 11  0  0  0  8027       0      0 0.00e+00    0 0.00e+00  0
MatLUFactorNum         1 1.0 7.5360e-03 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1440       0      0 0.00e+00    0 0.00e+00  0
MatILUFactorSym        1 1.0 3.4471e-03 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 5.4801e-0349.8 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7493e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetRowIJ            1 1.0 2.6290e-06 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetOrdering         1 1.0 6.9465e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecMDot             6472 1.0 5.1196e+00 1.1 1.00e+10 1.0 0.0e+00 0.0e+00 6.5e+03 18 36  0  0 49  18 36  0  0 49 39154       0      0 0.00e+00    0 0.00e+00  0
VecNorm             6689 1.0 7.0857e-01 1.2 6.69e+08 1.0 0.0e+00 0.0e+00 6.7e+03  2  2  0  0 51   2  2  0  0 51 18880       0      0 0.00e+00    0 0.00e+00  0
VecScale            6688 1.0 1.3479e-01 1.0 3.34e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 49617       0      0 0.00e+00    0 0.00e+00  0
VecCopy              216 1.0 1.8663e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet              6907 1.0 4.1162e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY              432 1.0 2.0645e-02 1.0 4.32e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 41851       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY            6688 1.0 6.4350e+00 1.0 1.07e+10 1.0 0.0e+00 0.0e+00 0.0e+00 23 38  0  0  0  23 38  0  0  0 33162       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     6688 1.0 9.1550e-02 1.5 0.00e+00 0.0 2.5e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       6688 1.0 5.4741e-01 3.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecNormalize        6688 1.0 8.5176e-01 1.1 1.00e+09 1.0 0.0e+00 0.0e+00 6.7e+03  3  4  0  0 51   3  4  0  0 51 23556       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.4480e-06 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 2.1427e-03 5.6 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      6688 1.0 7.9724e-02 1.6 0.00e+00 0.0 2.5e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        6688 1.0 5.3946e-01 3.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              6688 1.0 2.1097e-02 6.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            6688 1.0 3.4191e-03 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.0440e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.6949e+01 1.0 2.77e+10 1.0 2.5e+05 8.0e+03 1.3e+04 99100100100100  99100100100100 20578       0      0 0.00e+00    0 0.00e+00  0
KSPGMRESOrthog      6472 1.0 1.1093e+01 1.0 2.00e+10 1.0 0.0e+00 0.0e+00 6.5e+03 40 72  0  0 49  40 72  0  0 49 36141       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                2 1.0 1.8396e-02 1.2 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   590       0      0 0.00e+00    0 0.00e+00  0
PCSetUpOnBlocks        1 1.0 1.1685e-02 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   929       0      0 0.00e+00    0 0.00e+00  0
PCApply             6688 1.0 7.8830e+00 1.0 2.98e+09 1.0 0.0e+00 0.0e+00 0.0e+00 29 11  0  0  0  29 11  0  0  0  7564       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              4     11791508     0.
         Vec Scatter     1              1          816     0.
              Vector    41             41     14878464     0.
           Index Set     5              5       608512     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     2              2        20072     0.
      Preconditioner     2              2         1944     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.93e-08
Average time for MPI_Barrier(): 1.3214e-05
Average time for zero size MPI_Send(): 6.17145e-06
#PETSc Option Table entries:
-ksp_type gmres
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type bjacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 186.34 iterations 10000
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8078 with 20 processors, by luciano.siqueira Wed Sep  9 17:32:31 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.331e+01     1.000   3.331e+01
Objects:              5.000e+01     1.000   5.000e+01
Flop:                 3.876e+10     1.001   3.876e+10  7.752e+11
Flop/sec:             1.164e+09     1.001   1.164e+09  2.328e+10
MPI Messages:         2.067e+04     2.000   1.964e+04  3.928e+05
MPI Message Lengths:  1.654e+08     2.000   7.999e+03  3.142e+09
MPI Reductions:       2.036e+04     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.3305e+01 100.0%  7.7519e+11 100.0%  3.928e+05 100.0%  7.999e+03      100.0%  2.035e+04 100.0%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 4.0023e-0325.4 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 3.7080e-0376.8 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult            10334 1.0 1.1107e+01 1.0 4.65e+09 1.0 3.9e+05 8.0e+03 0.0e+00 33 12100100  0  33 12100100  0  8366       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 3.7635e-0337.5 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7155e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecMDot            10000 1.0 8.2222e+00 1.0 1.55e+10 1.0 0.0e+00 0.0e+00 1.0e+04 24 40  0  0 49  24 40  0  0 49 37678       0      0 0.00e+00    0 0.00e+00  0
VecNorm            10335 1.0 1.5391e+00 1.8 1.03e+09 1.0 0.0e+00 0.0e+00 1.0e+04  4  3  0  0 51   4  3  0  0 51 13430       0      0 0.00e+00    0 0.00e+00  0
VecScale           10334 1.0 2.1823e-01 1.1 5.17e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1  1  0  0  0   1  1  0  0  0 47353       0      0 0.00e+00    0 0.00e+00  0
VecCopy              334 1.0 2.8904e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet               337 1.0 2.1140e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY              668 1.0 3.2497e-02 1.1 6.68e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 41111       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY           10334 1.0 1.1000e+01 1.1 1.65e+10 1.0 0.0e+00 0.0e+00 0.0e+00 32 43  0  0  0  32 43  0  0  0 29981       0      0 0.00e+00    0 0.00e+00  0
VecPointwiseMult   10334 1.0 1.6499e+00 1.1 5.17e+08 1.0 0.0e+00 0.0e+00 0.0e+00  5  1  0  0  0   5  1  0  0  0  6264       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin    10334 1.0 1.3623e-01 1.6 0.00e+00 0.0 3.9e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd      10334 1.0 9.6569e-01 5.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecNormalize       10334 1.0 1.7496e+00 1.6 1.55e+09 1.0 0.0e+00 0.0e+00 1.0e+04  4  4  0  0 51   4  4  0  0 51 17720       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.7280e-06 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 2.7268e-0314.0 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin     10334 1.0 1.2143e-01 1.7 0.00e+00 0.0 3.9e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd       10334 1.0 9.5268e-01 5.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack             10334 1.0 3.3181e-02 9.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack           10334 1.0 5.7864e-03 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 1.2188e-03 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 3.2936e+01 1.0 3.88e+10 1.0 3.9e+05 8.0e+03 2.0e+04 99100100100100  99100100100100 23536       0      0 0.00e+00    0 0.00e+00  0
KSPGMRESOrthog     10000 1.0 1.8568e+01 1.0 3.10e+10 1.0 0.0e+00 0.0e+00 1.0e+04 54 80  0  0 49  54 80  0  0 49 33368       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 8.4500e-07 3.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply            10334 1.0 1.6661e+00 1.1 5.17e+08 1.0 0.0e+00 0.0e+00 2.0e+00  5  1  0  0  0   5  1  0  0  0  6203       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      8013484     0.
         Vec Scatter     1              1          816     0.
              Vector    40             40     15276992     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1        18656     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3e-08
Average time for MPI_Barrier(): 1.23306e-05
Average time for zero size MPI_Send(): 6.38135e-06
#PETSc Option Table entries:
-ksp_type gmres
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type jacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.0487126 iterations 10000
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8078 with 20 processors, by luciano.siqueira Wed Sep  9 17:33:20 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           4.686e+01     1.000   4.686e+01
Objects:              4.900e+01     1.000   4.900e+01
Flop:                 4.337e+10     1.000   4.337e+10  8.673e+11
Flop/sec:             9.255e+08     1.000   9.255e+08  1.851e+10
MPI Messages:         2.067e+04     2.000   1.964e+04  3.928e+05
MPI Message Lengths:  1.654e+08     2.000   7.999e+03  3.142e+09
MPI Reductions:       2.035e+04     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 4.6859e+01 100.0%  8.6733e+11 100.0%  3.928e+05 100.0%  7.999e+03      100.0%  2.035e+04 100.0%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 4.1831e-0341.4 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 4.1510e-0378.1 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult            10334 1.0 1.0466e+01 1.0 4.65e+09 1.0 3.9e+05 8.0e+03 0.0e+00 22 11100100  0  22 11100100  0  8879       0      0 0.00e+00    0 0.00e+00  0
MatSOR             10334 1.0 1.5915e+01 1.0 5.12e+09 1.0 0.0e+00 0.0e+00 0.0e+00 33 12  0  0  0  33 12  0  0  0  6439       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 4.2032e-0339.5 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7372e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecMDot            10000 1.0 8.8862e+00 1.1 1.55e+10 1.0 0.0e+00 0.0e+00 1.0e+04 19 36  0  0 49  19 36  0  0 49 34863       0      0 0.00e+00    0 0.00e+00  0
VecNorm            10335 1.0 9.7866e-01 1.1 1.03e+09 1.0 0.0e+00 0.0e+00 1.0e+04  2  2  0  0 51   2  2  0  0 51 21121       0      0 0.00e+00    0 0.00e+00  0
VecScale           10334 1.0 2.1196e-01 1.1 5.17e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 48755       0      0 0.00e+00    0 0.00e+00  0
VecCopy              334 1.0 2.8808e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet               337 1.0 2.1091e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY              668 1.0 3.2402e-02 1.1 6.68e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 41233       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY           10334 1.0 1.0556e+01 1.0 1.65e+10 1.0 0.0e+00 0.0e+00 0.0e+00 22 38  0  0  0  22 38  0  0  0 31243       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin    10334 1.0 1.4514e-01 1.6 0.00e+00 0.0 3.9e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd      10334 1.0 3.1964e-01 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecNormalize       10334 1.0 1.1967e+00 1.1 1.55e+09 1.0 0.0e+00 0.0e+00 1.0e+04  2  4  0  0 51   2  4  0  0 51 25906       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 1.4666e-05 3.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.3374e-03 3.5 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin     10334 1.0 1.2749e-01 1.7 0.00e+00 0.0 3.9e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd       10334 1.0 3.0543e-01 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack             10334 1.0 3.7022e-02 8.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack           10334 1.0 5.8848e-03 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 1.0570e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 4.6484e+01 1.0 4.34e+10 1.0 3.9e+05 8.0e+03 2.0e+04 99100100100100  99100100100100 18659       0      0 0.00e+00    0 0.00e+00  0
KSPGMRESOrthog     10000 1.0 1.8802e+01 1.0 3.10e+10 1.0 0.0e+00 0.0e+00 1.0e+04 40 71  0  0 49  40 71  0  0 49 32953       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 2.9900e-07 2.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply            10334 1.0 1.5938e+01 1.0 5.12e+09 1.0 0.0e+00 0.0e+00 0.0e+00 33 12  0  0  0  33 12  0  0  0  6429       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    39             39     14875264     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1        18656     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.94e-08
Average time for MPI_Barrier(): 1.06118e-05
Average time for zero size MPI_Send(): 6.28875e-06
#PETSc Option Table entries:
-ksp_type gmres
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type sor
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.00984153 iterations 3132
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8078 with 20 processors, by luciano.siqueira Wed Sep  9 17:33:48 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           2.609e+01     1.000   2.609e+01
Objects:              7.200e+01     1.000   7.200e+01
Flop:                 1.795e+10     1.001   1.795e+10  3.589e+11
Flop/sec:             6.879e+08     1.001   6.879e+08  1.376e+10
MPI Messages:         1.297e+04     2.000   1.232e+04  2.465e+05
MPI Message Lengths:  1.038e+08     2.000   7.998e+03  1.971e+09
MPI Reductions:       6.411e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 2.6089e+01 100.0%  3.5893e+11 100.0%  2.465e+05 100.0%  7.998e+03      100.0%  6.404e+03  99.9%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 5.0247e-0365.9 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 4.9275e-0398.7 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             6484 1.0 7.2410e+00 1.1 2.92e+09 1.0 2.5e+05 8.0e+03 0.0e+00 27 16100100  0  27 16100100  0  8052       0      0 0.00e+00    0 0.00e+00  0
MatSOR              6485 1.0 1.0397e+01 1.1 3.22e+09 1.0 0.0e+00 0.0e+00 0.0e+00 38 18  0  0  0  38 18  0  0  0  6185       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 4.9802e-0347.0 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6761e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecMDot             3142 1.0 3.1692e+00 1.2 4.85e+09 1.0 0.0e+00 0.0e+00 3.1e+03 11 27  0  0 49  11 27  0  0 49 30603       0      0 0.00e+00    0 0.00e+00  0
VecNorm             3249 1.0 4.4849e-01 1.5 3.25e+08 1.0 0.0e+00 0.0e+00 3.2e+03  1  2  0  0 51   1  2  0  0 51 14489       0      0 0.00e+00    0 0.00e+00  0
VecScale            3248 1.0 6.8813e-02 1.1 1.62e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 47201       0      0 0.00e+00    0 0.00e+00  0
VecCopy             6580 1.0 3.5116e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet              6584 1.0 2.5974e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY              211 1.0 1.0183e-02 1.0 2.11e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 41440       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             6474 1.0 9.1986e-01 1.1 4.86e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3  3  0  0  0   3  3  0  0  0 10557       0      0 0.00e+00    0 0.00e+00  0
VecAXPBYCZ          3237 1.0 5.7267e-01 1.0 8.09e+08 1.0 0.0e+00 0.0e+00 0.0e+00  2  5  0  0  0   2  5  0  0  0 28262       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY            3248 1.0 3.2569e+00 1.0 5.16e+09 1.0 0.0e+00 0.0e+00 0.0e+00 12 29  0  0  0  12 29  0  0  0 31708       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     6484 1.0 9.5004e-02 1.4 0.00e+00 0.0 2.5e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       6484 1.0 8.3312e-01 5.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecNormalize        3248 1.0 5.1795e-01 1.4 4.87e+08 1.0 0.0e+00 0.0e+00 3.2e+03  2  3  0  0 51   2  3  0  0 51 18813       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.5360e-06 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 2.4635e-0311.2 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      6484 1.0 8.4052e-02 1.5 0.00e+00 0.0 2.5e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        6484 1.0 8.2448e-01 5.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              6484 1.0 2.1627e-02 7.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            6484 1.0 3.5493e-03 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.4610e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  1  0  0  0  0   1  0  0  0  0  3251       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.5718e+01 1.0 1.79e+10 1.0 2.5e+05 8.0e+03 6.4e+03 99100100100100  99100100100100 13956       0      0 0.00e+00    0 0.00e+00  0
KSPGMRESOrthog      3142 1.0 6.1261e+00 1.1 9.70e+09 1.0 0.0e+00 0.0e+00 3.1e+03 23 54  0  0 49  23 54  0  0 49 31663       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 1.4532e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  1  0  0  0  0   1  0  0  0  0  3269       0      0 0.00e+00    0 0.00e+00  0
PCApply             3248 1.0 1.5937e+01 1.0 5.97e+09 1.0 1.2e+05 8.0e+03 0.0e+00 60 33 50 50  0  60 33 50 50  0  7486       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    58             58     22508096     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     3              3        50672     0.
      Preconditioner     3              3         2936     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.9e-08
Average time for MPI_Barrier(): 1.19302e-05
Average time for zero size MPI_Send(): 6.29725e-06
#PETSc Option Table entries:
-ksp_type gmres
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.000101716 iterations 641
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8078 with 20 processors, by luciano.siqueira Wed Sep  9 17:33:53 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.324e+00     1.000   3.324e+00
Objects:              8.700e+01     1.000   8.700e+01
Flop:                 2.892e+09     1.000   2.892e+09  5.784e+10
Flop/sec:             8.701e+08     1.000   8.700e+08  1.740e+10
MPI Messages:         1.288e+03     2.000   1.224e+03  2.447e+04
MPI Message Lengths:  1.028e+07     2.000   7.981e+03  1.953e+08
MPI Reductions:       2.583e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.3243e+00 100.0%  5.7844e+10 100.0%  2.447e+04 100.0%  7.981e+03      100.0%  2.576e+03  99.7%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 4.4279e-0329.4 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 4.4005e-0393.7 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult              642 1.0 6.8270e-01 1.0 2.89e+08 1.0 2.4e+04 8.0e+03 0.0e+00 20 10100100  0  20 10100100  0  8456       0      0 0.00e+00    0 0.00e+00  0
MatSolve             642 1.0 7.4291e-01 1.0 2.86e+08 1.0 0.0e+00 0.0e+00 0.0e+00 22 10  0  0  0  22 10  0  0  0  7705       0      0 0.00e+00    0 0.00e+00  0
MatLUFactorNum         1 1.0 7.5450e-03 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1438       0      0 0.00e+00    0 0.00e+00  0
MatILUFactorSym        1 1.0 3.4491e-03 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 4.4509e-0344.2 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7906e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetRowIJ            1 1.0 3.1380e-06 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetOrdering         1 1.0 6.9767e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecTDot             1282 1.0 1.4457e-01 1.2 1.28e+08 1.0 0.0e+00 0.0e+00 1.3e+03  4  4  0  0 50   4  4  0  0 50 17735       0      0 0.00e+00    0 0.00e+00  0
VecMTDot             640 1.0 5.3958e-01 1.0 9.82e+08 1.0 0.0e+00 0.0e+00 6.4e+02 16 34  0  0 25  16 34  0  0 25 36398       0      0 0.00e+00    0 0.00e+00  0
VecNorm              643 1.0 9.2315e-02 1.7 6.43e+07 1.0 0.0e+00 0.0e+00 6.4e+02  2  2  0  0 25   2  2  0  0 25 13931       0      0 0.00e+00    0 0.00e+00  0
VecScale             640 1.0 2.3526e-02 1.1 3.20e+07 1.0 0.0e+00 0.0e+00 0.0e+00  1  1  0  0  0   1  1  0  0  0 27203       0      0 0.00e+00    0 0.00e+00  0
VecCopy              642 1.0 3.9144e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet               645 1.0 1.4382e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             1283 1.0 6.9046e-02 1.1 1.28e+08 1.0 0.0e+00 0.0e+00 0.0e+00  2  4  0  0  0   2  4  0  0  0 37164       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY             640 1.0 6.2741e-01 1.0 9.82e+08 1.0 0.0e+00 0.0e+00 0.0e+00 19 34  0  0  0  19 34  0  0  0 31303       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin      642 1.0 1.0934e-02 1.4 0.00e+00 0.0 2.4e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd        642 1.0 3.8167e-02 2.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 1.0133e-05 2.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.4193e-03 3.4 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin       642 1.0 9.8676e-03 1.5 0.00e+00 0.0 2.4e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd         642 1.0 3.7365e-02 3.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack               642 1.0 2.1410e-03 7.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack             642 1.0 3.4828e-04 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 4.5726e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.9540e+00 1.0 2.89e+09 1.0 2.4e+04 8.0e+03 2.6e+03 89100100100 99  89100100100100 19577       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                2 1.0 2.6222e-02 1.2 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0   414       0      0 0.00e+00    0 0.00e+00  0
PCSetUpOnBlocks        1 1.0 1.1735e-02 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   925       0      0 0.00e+00    0 0.00e+00  0
PCApply              642 1.0 7.6253e-01 1.0 2.86e+08 1.0 0.0e+00 0.0e+00 0.0e+00 22 10  0  0  0  22 10  0  0  0  7507       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              4     11791508     0.
         Vec Scatter     1              1          816     0.
              Vector    71             71     26930304     0.
           Index Set     5              5       608512     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     2              2         4080     0.
      Preconditioner     2              2         1944     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.85e-08
Average time for MPI_Barrier(): 1.30556e-05
Average time for zero size MPI_Send(): 6.3565e-06
#PETSc Option Table entries:
-ksp_type fcg
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type bjacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 4.69193e-05 iterations 1715
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8078 with 20 processors, by luciano.siqueira Wed Sep  9 17:34:01 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           6.365e+00     1.000   6.365e+00
Objects:              8.000e+01     1.000   8.000e+01
Flop:                 7.104e+09     1.000   7.104e+09  1.421e+11
Flop/sec:             1.116e+09     1.000   1.116e+09  2.232e+10
MPI Messages:         3.436e+03     2.000   3.264e+03  6.528e+04
MPI Message Lengths:  2.746e+07     2.000   7.993e+03  5.218e+08
MPI Reductions:       6.881e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 6.3651e+00 100.0%  1.4208e+11 100.0%  6.528e+04 100.0%  7.993e+03      100.0%  6.874e+03  99.9%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 2.8987e-0334.2 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 2.8625e-0353.6 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             1716 1.0 1.7988e+00 1.0 7.72e+08 1.0 6.5e+04 8.0e+03 0.0e+00 28 11100100  0  28 11100100  0  8578       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 2.9116e-0327.8 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6836e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecTDot             3430 1.0 3.5328e-01 1.2 3.43e+08 1.0 0.0e+00 0.0e+00 3.4e+03  5  5  0  0 50   5  5  0  0 50 19418       0      0 0.00e+00    0 0.00e+00  0
VecMTDot            1714 1.0 1.3858e+00 1.0 2.65e+09 1.0 0.0e+00 0.0e+00 1.7e+03 22 37  0  0 25  22 37  0  0 25 38267       0      0 0.00e+00    0 0.00e+00  0
VecNorm             1717 1.0 1.5070e-01 1.1 1.72e+08 1.0 0.0e+00 0.0e+00 1.7e+03  2  2  0  0 25   2  2  0  0 25 22786       0      0 0.00e+00    0 0.00e+00  0
VecScale            1714 1.0 5.0073e-02 1.1 8.57e+07 1.0 0.0e+00 0.0e+00 0.0e+00  1  1  0  0  0   1  1  0  0  0 34230       0      0 0.00e+00    0 0.00e+00  0
VecCopy             1716 1.0 1.1082e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 3 1.0 2.0836e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             3431 1.0 1.8586e-01 1.1 3.43e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3  5  0  0  0   3  5  0  0  0 36920       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY            1714 1.0 1.7643e+00 1.0 2.65e+09 1.0 0.0e+00 0.0e+00 0.0e+00 27 37  0  0  0  27 37  0  0  0 30057       0      0 0.00e+00    0 0.00e+00  0
VecPointwiseMult    1716 1.0 2.5189e-01 1.0 8.58e+07 1.0 0.0e+00 0.0e+00 0.0e+00  4  1  0  0  0   4  1  0  0  0  6813       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     1716 1.0 2.8082e-02 1.7 0.00e+00 0.0 6.5e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       1716 1.0 8.4934e-02 2.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.1660e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.2904e-03 3.4 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      1716 1.0 2.4821e-02 1.8 0.00e+00 0.0 6.5e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        1716 1.0 8.2635e-02 2.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              1716 1.0 5.7356e-03 9.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            1716 1.0 1.1990e-03 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 4.3995e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 5.9958e+00 1.0 7.10e+09 1.0 6.5e+04 8.0e+03 6.9e+03 94100100100100  94100100100100 23694       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 1.1400e-06 2.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply             1716 1.0 2.5540e-01 1.0 8.58e+07 1.0 0.0e+00 0.0e+00 2.0e+00  4  1  0  0  0   4  1  0  0  0  6719       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      8013484     0.
         Vec Scatter     1              1          816     0.
              Vector    70             70     27328832     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         2664     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.86e-08
Average time for MPI_Barrier(): 1.08184e-05
Average time for zero size MPI_Send(): 6.39835e-06
#PETSc Option Table entries:
-ksp_type fcg
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type jacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.000614161 iterations 692
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8078 with 20 processors, by luciano.siqueira Wed Sep  9 17:34:07 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           4.003e+00     1.000   4.003e+00
Objects:              7.900e+01     1.000   7.900e+01
Flop:                 3.175e+09     1.000   3.175e+09  6.351e+10
Flop/sec:             7.932e+08     1.000   7.932e+08  1.586e+10
MPI Messages:         1.390e+03     2.000   1.320e+03  2.641e+04
MPI Message Lengths:  1.110e+07     2.000   7.983e+03  2.108e+08
MPI Reductions:       2.787e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 4.0032e+00 100.0%  6.3505e+10 100.0%  2.641e+04 100.0%  7.983e+03      100.0%  2.780e+03  99.7%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 3.4490e-0343.2 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 2.1437e-0344.4 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult              693 1.0 7.3274e-01 1.0 3.12e+08 1.0 2.6e+04 8.0e+03 0.0e+00 18 10100100  0  18 10100100  0  8504       0      0 0.00e+00    0 0.00e+00  0
MatSOR               693 1.0 1.0889e+00 1.0 3.44e+08 1.0 0.0e+00 0.0e+00 0.0e+00 27 11  0  0  0  27 11  0  0  0  6312       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 2.1974e-0322.3 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7496e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecTDot             1384 1.0 1.6077e-01 1.2 1.38e+08 1.0 0.0e+00 0.0e+00 1.4e+03  4  4  0  0 50   4  4  0  0 50 17217       0      0 0.00e+00    0 0.00e+00  0
VecMTDot             691 1.0 6.8448e-01 1.0 1.07e+09 1.0 0.0e+00 0.0e+00 6.9e+02 17 34  0  0 25  17 34  0  0 25 31253       0      0 0.00e+00    0 0.00e+00  0
VecNorm              694 1.0 1.1487e-01 1.4 6.94e+07 1.0 0.0e+00 0.0e+00 6.9e+02  2  2  0  0 25   2  2  0  0 25 12083       0      0 0.00e+00    0 0.00e+00  0
VecScale             691 1.0 3.1701e-02 1.0 3.46e+07 1.0 0.0e+00 0.0e+00 0.0e+00  1  1  0  0  0   1  1  0  0  0 21797       0      0 0.00e+00    0 0.00e+00  0
VecCopy              693 1.0 4.4420e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 3 1.0 2.1094e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             1385 1.0 8.3703e-02 1.1 1.38e+08 1.0 0.0e+00 0.0e+00 0.0e+00  2  4  0  0  0   2  4  0  0  0 33093       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY             691 1.0 7.2151e-01 1.0 1.07e+09 1.0 0.0e+00 0.0e+00 0.0e+00 18 34  0  0  0  18 34  0  0  0 29649       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin      693 1.0 1.2458e-02 1.4 0.00e+00 0.0 2.6e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd        693 1.0 3.5327e-02 2.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 1.3967e-05 2.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 2.5818e-0311.8 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin       693 1.0 1.1183e-02 1.5 0.00e+00 0.0 2.6e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd         693 1.0 3.4352e-02 2.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack               693 1.0 2.4506e-03 8.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack             693 1.0 4.0138e-04 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 4.4534e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 3.6212e+00 1.0 3.17e+09 1.0 2.6e+04 8.0e+03 2.8e+03 90100100100 99  90100100100100 17534       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 5.4500e-07 3.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply              693 1.0 1.0905e+00 1.0 3.44e+08 1.0 0.0e+00 0.0e+00 0.0e+00 27 11  0  0  0  27 11  0  0  0  6303       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    69             69     26927104     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         2664     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.79e-08
Average time for MPI_Barrier(): 1.16502e-05
Average time for zero size MPI_Send(): 6.15825e-06
#PETSc Option Table entries:
-ksp_type fcg
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type sor
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.000603072 iterations 383
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8078 with 20 processors, by luciano.siqueira Wed Sep  9 17:34:13 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.870e+00     1.000   3.870e+00
Objects:              1.020e+02     1.000   1.020e+02
Flop:                 2.281e+09     1.001   2.281e+09  4.562e+10
Flop/sec:             5.895e+08     1.001   5.894e+08  1.179e+10
MPI Messages:         1.560e+03     2.000   1.482e+03  2.964e+04
MPI Message Lengths:  1.246e+07     2.000   7.985e+03  2.367e+08
MPI Reductions:       1.574e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.8698e+00 100.0%  4.5618e+10 100.0%  2.964e+04 100.0%  7.985e+03      100.0%  1.567e+03  99.6%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 6.7580e-0360.9 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 5.7190e-03105.6 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult              778 1.0 8.7672e-01 1.0 3.50e+08 1.0 3.0e+04 8.0e+03 0.0e+00 22 15100100  0  22 15100100  0  7980       0      0 0.00e+00    0 0.00e+00  0
MatSOR               779 1.0 1.2688e+00 1.0 3.86e+08 1.0 0.0e+00 0.0e+00 0.0e+00 32 17  0  0  0  32 17  0  0  0  6089       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 5.7715e-0353.0 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7203e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecMDot               10 1.0 3.8081e-03 1.1 5.50e+06 1.0 0.0e+00 0.0e+00 1.0e+01  0  0  0  0  1   0  0  0  0  1 28885       0      0 0.00e+00    0 0.00e+00  0
VecTDot              766 1.0 9.4076e-02 1.2 7.66e+07 1.0 0.0e+00 0.0e+00 7.7e+02  2  3  0  0 49   2  3  0  0 49 16285       0      0 0.00e+00    0 0.00e+00  0
VecMTDot             382 1.0 3.8570e-01 1.0 5.83e+08 1.0 0.0e+00 0.0e+00 3.8e+02 10 26  0  0 24  10 26  0  0 24 30246       0      0 0.00e+00    0 0.00e+00  0
VecNorm              396 1.0 7.3829e-02 1.6 3.96e+07 1.0 0.0e+00 0.0e+00 4.0e+02  2  2  0  0 25   2  2  0  0 25 10728       0      0 0.00e+00    0 0.00e+00  0
VecScale             393 1.0 2.1560e-02 1.0 1.96e+07 1.0 0.0e+00 0.0e+00 0.0e+00  1  1  0  0  0   1  1  0  0  0 18229       0      0 0.00e+00    0 0.00e+00  0
VecCopy             1153 1.0 6.2188e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet               773 1.0 1.8348e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY              768 1.0 4.7794e-02 1.0 7.68e+07 1.0 0.0e+00 0.0e+00 0.0e+00  1  3  0  0  0   1  3  0  0  0 32138       0      0 0.00e+00    0 0.00e+00  0
VecAYPX              768 1.0 1.1211e-01 1.0 5.76e+07 1.0 0.0e+00 0.0e+00 0.0e+00  3  3  0  0  0   3  3  0  0  0 10276       0      0 0.00e+00    0 0.00e+00  0
VecAXPBYCZ           384 1.0 7.0583e-02 1.0 9.60e+07 1.0 0.0e+00 0.0e+00 0.0e+00  2  4  0  0  0   2  4  0  0  0 27202       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY             393 1.0 3.9372e-01 1.0 5.90e+08 1.0 0.0e+00 0.0e+00 0.0e+00 10 26  0  0  0  10 26  0  0  0 29961       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin      778 1.0 1.4008e-02 1.4 0.00e+00 0.0 3.0e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd        778 1.0 5.1457e-02 2.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecNormalize          11 1.0 8.0207e-03 1.1 1.65e+06 1.0 0.0e+00 0.0e+00 1.1e+01  0  0  0  0  1   0  0  0  0  1  4114       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 1.0210e-05 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.5731e-03 3.9 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin       778 1.0 1.2759e-02 1.5 0.00e+00 0.0 3.0e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd         778 1.0 5.0333e-02 2.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack               778 1.0 2.6719e-03 8.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack             778 1.0 5.0937e-04 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.5171e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  4  1  1  1  1   4  1  1  1  1  3131       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 3.4903e+00 1.0 2.28e+09 1.0 3.0e+04 8.0e+03 1.6e+03 90100100100 99  90100100100 99 13066       0      0 0.00e+00    0 0.00e+00  0
KSPGMRESOrthog        10 1.0 1.2389e-02 1.0 1.10e+07 1.0 0.0e+00 0.0e+00 1.0e+01  0  0  0  0  1   0  0  0  0  1 17758       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 1.4763e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  4  1  1  1  1   4  1  1  1  1  3218       0      0 0.00e+00    0 0.00e+00  0
PCApply              395 1.0 1.9335e+00 1.0 7.13e+08 1.0 1.5e+04 8.0e+03 0.0e+00 50 31 49 49  0  50 31 49 49  0  7370       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    88             88     34559936     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     3              3        34680     0.
      Preconditioner     3              3         2936     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.03e-08
Average time for MPI_Barrier(): 1.18228e-05
Average time for zero size MPI_Send(): 6.84495e-06
#PETSc Option Table entries:
-ksp_type fcg
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 8.47251e-05 iterations 1671
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8078 with 20 processors, by luciano.siqueira Wed Sep  9 17:34:29 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           1.401e+01     1.000   1.401e+01
Objects:              4.000e+01     1.000   4.000e+01
Flop:                 7.332e+09     1.001   7.331e+09  1.466e+11
Flop/sec:             5.235e+08     1.001   5.234e+08  1.047e+10
MPI Messages:         1.003e+04     2.000   9.530e+03  1.906e+05
MPI Message Lengths:  8.023e+07     2.000   7.998e+03  1.524e+09
MPI Reductions:       8.375e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 1.4005e+01 100.0%  1.4662e+11 100.0%  1.906e+05 100.0%  7.998e+03      100.0%  8.368e+03  99.9%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 5.4809e-0362.2 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 5.4167e-0392.8 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             5014 1.0 5.1401e+00 1.0 2.26e+09 1.0 1.9e+05 8.0e+03 0.0e+00 36 31100100  0  36 31100100  0  8771       0      0 0.00e+00    0 0.00e+00  0
MatSolve            5014 1.0 5.5833e+00 1.0 2.24e+09 1.0 0.0e+00 0.0e+00 0.0e+00 39 30  0  0  0  39 30  0  0  0  8007       0      0 0.00e+00    0 0.00e+00  0
MatLUFactorNum         1 1.0 7.5630e-03 1.0 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1435       0      0 0.00e+00    0 0.00e+00  0
MatILUFactorSym        1 1.0 3.4555e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 5.4699e-0347.8 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.8215e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetRowIJ            1 1.0 2.5660e-06 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetOrdering         1 1.0 7.0038e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot              5013 1.0 5.2105e-01 1.2 5.01e+08 1.0 0.0e+00 0.0e+00 5.0e+03  3  7  0  0 60   3  7  0  0 60 19242       0      0 0.00e+00    0 0.00e+00  0
VecNorm             3344 1.0 3.9563e-01 1.4 3.34e+08 1.0 0.0e+00 0.0e+00 3.3e+03  2  5  0  0 40   2  5  0  0 40 16905       0      0 0.00e+00    0 0.00e+00  0
VecScale            6685 1.0 1.4133e-01 1.1 3.34e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1  5  0  0  0   1  5  0  0  0 47301       0      0 0.00e+00    0 0.00e+00  0
VecCopy            15043 1.0 7.0036e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  5  0  0  0  0   5  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet              5022 1.0 3.0372e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY            15037 1.0 8.2711e-01 1.0 1.50e+09 1.0 0.0e+00 0.0e+00 0.0e+00  6 21  0  0  0   6 21  0  0  0 36360       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             1671 1.0 2.2312e-01 1.0 1.67e+08 1.0 0.0e+00 0.0e+00 0.0e+00  2  2  0  0  0   2  2  0  0  0 14978       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     5014 1.0 8.0327e-02 1.7 0.00e+00 0.0 1.9e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       5014 1.0 2.2642e-01 2.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.6820e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.1730e-03 2.9 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      5014 1.0 7.1909e-02 1.8 0.00e+00 0.0 1.9e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        5014 1.0 2.2076e-01 2.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              5014 1.0 1.9568e-02 9.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            5014 1.0 2.5900e-03 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 3.5609e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 1.3623e+01 1.0 7.33e+09 1.0 1.9e+05 8.0e+03 8.4e+03 97100100100100  97100100100100 10762       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                2 1.0 2.7307e-02 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   397       0      0 0.00e+00    0 0.00e+00  0
PCSetUpOnBlocks        1 1.0 1.1781e-02 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   921       0      0 0.00e+00    0 0.00e+00  0
PCApply             5014 1.0 5.9218e+00 1.0 2.24e+09 1.0 0.0e+00 0.0e+00 0.0e+00 41 30  0  0  0  41 30  0  0  0  7549       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              4     11791508     0.
         Vec Scatter     1              1          816     0.
              Vector    24             24      8049088     0.
           Index Set     5              5       608512     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     2              2         2832     0.
      Preconditioner     2              2         1944     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.82e-08
Average time for MPI_Barrier(): 1.32208e-05
Average time for zero size MPI_Send(): 6.17755e-06
#PETSc Option Table entries:
-ksp_type tcqmr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type bjacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 2.62209e-05 iterations 6622
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8078 with 20 processors, by luciano.siqueira Wed Sep  9 17:35:04 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.346e+01     1.000   3.346e+01
Objects:              3.300e+01     1.000   3.300e+01
Flop:                 2.119e+10     1.002   2.118e+10  4.237e+11
Flop/sec:             6.332e+08     1.002   6.331e+08  1.266e+10
MPI Messages:         3.974e+04     2.000   3.775e+04  7.550e+05
MPI Message Lengths:  3.179e+08     2.000   7.999e+03  6.040e+09
MPI Reductions:       3.313e+04     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.3459e+01 100.0%  4.2366e+11 100.0%  7.550e+05 100.0%  7.999e+03      100.0%  3.312e+04 100.0%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 4.1535e-0358.6 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 3.8714e-0375.6 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult            19867 1.0 2.0063e+01 1.0 8.94e+09 1.0 7.5e+05 8.0e+03 0.0e+00 59 42100100  0  59 42100100  0  8904       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 3.9237e-0337.7 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7626e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot             19866 1.0 1.9155e+00 1.1 1.99e+09 1.0 0.0e+00 0.0e+00 2.0e+04  5  9  0  0 60   5  9  0  0 60 20742       0      0 0.00e+00    0 0.00e+00  0
VecNorm            13246 1.0 1.3335e+00 1.3 1.32e+09 1.0 0.0e+00 0.0e+00 1.3e+04  3  6  0  0 40   3  6  0  0 40 19866       0      0 0.00e+00    0 0.00e+00  0
VecScale           26489 1.0 4.9359e-01 1.0 1.32e+09 1.0 0.0e+00 0.0e+00 0.0e+00  1  6  0  0  0   1  6  0  0  0 53667       0      0 0.00e+00    0 0.00e+00  0
VecCopy            59602 1.0 2.7955e+00 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  8  0  0  0  0   8  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 8 1.0 5.3004e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY            59596 1.0 3.2804e+00 1.0 5.96e+09 1.0 0.0e+00 0.0e+00 0.0e+00 10 28  0  0  0  10 28  0  0  0 36334       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             6622 1.0 8.7308e-01 1.0 6.62e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3  3  0  0  0   3  3  0  0  0 15169       0      0 0.00e+00    0 0.00e+00  0
VecPointwiseMult   19867 1.0 2.9027e+00 1.0 9.93e+08 1.0 0.0e+00 0.0e+00 0.0e+00  9  5  0  0  0   9  5  0  0  0  6844       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin    19867 1.0 2.8000e-01 1.7 0.00e+00 0.0 7.5e+05 8.0e+03 0.0e+00  1  0100100  0   1  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd      19867 1.0 6.6495e-01 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 1.0024e-05 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.2978e-03 3.3 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin     19867 1.0 2.4828e-01 1.7 0.00e+00 0.0 7.5e+05 8.0e+03 0.0e+00  1  0100100  0   1  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd       19867 1.0 6.4548e-01 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack             19867 1.0 6.9103e-02 8.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack           19867 1.0 1.1576e-02 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 3.5341e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 3.3073e+01 1.0 2.12e+10 1.0 7.5e+05 8.0e+03 3.3e+04 99100100100100  99100100100100 12809       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 9.8600e-07 2.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply            19867 1.0 2.9258e+00 1.0 9.93e+08 1.0 0.0e+00 0.0e+00 2.0e+00  9  5  0  0  0   9  5  0  0  0  6790       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      8013484     0.
         Vec Scatter     1              1          816     0.
              Vector    23             23      8447616     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1416     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.96e-08
Average time for MPI_Barrier(): 1.26408e-05
Average time for zero size MPI_Send(): 6.26795e-06
#PETSc Option Table entries:
-ksp_type tcqmr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type jacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.000177372 iterations 1033
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8078 with 20 processors, by luciano.siqueira Wed Sep  9 17:35:17 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           1.046e+01     1.000   1.046e+01
Objects:              3.200e+01     1.000   3.200e+01
Flop:                 4.688e+09     1.001   4.687e+09  9.374e+10
Flop/sec:             4.482e+08     1.001   4.481e+08  8.962e+09
MPI Messages:         6.204e+03     2.000   5.894e+03  1.179e+05
MPI Message Lengths:  4.961e+07     2.000   7.996e+03  9.426e+08
MPI Reductions:       5.185e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 1.0459e+01 100.0%  9.3739e+10 100.0%  1.179e+05 100.0%  7.996e+03      100.0%  5.178e+03  99.9%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 5.2334e-0339.1 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 5.2121e-03108.1 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             3100 1.0 3.3901e+00 1.0 1.39e+09 1.0 1.2e+05 8.0e+03 0.0e+00 32 30100100  0  32 30100100  0  8223       0      0 0.00e+00    0 0.00e+00  0
MatSOR              3100 1.0 4.8985e+00 1.1 1.54e+09 1.0 0.0e+00 0.0e+00 0.0e+00 45 33  0  0  0  45 33  0  0  0  6275       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 5.2606e-0351.0 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7502e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot              3099 1.0 4.5080e-01 1.4 3.10e+08 1.0 0.0e+00 0.0e+00 3.1e+03  4  7  0  0 60   4  7  0  0 60 13749       0      0 0.00e+00    0 0.00e+00  0
VecNorm             2068 1.0 4.1425e-01 2.1 2.07e+08 1.0 0.0e+00 0.0e+00 2.1e+03  3  4  0  0 40   3  4  0  0 40  9984       0      0 0.00e+00    0 0.00e+00  0
VecScale            4133 1.0 9.7242e-02 1.1 2.07e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1  4  0  0  0   1  4  0  0  0 42502       0      0 0.00e+00    0 0.00e+00  0
VecCopy             9301 1.0 4.9193e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  5  0  0  0  0   5  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 8 1.0 5.2191e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             9295 1.0 5.5157e-01 1.0 9.30e+08 1.0 0.0e+00 0.0e+00 0.0e+00  5 20  0  0  0   5 20  0  0  0 33704       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             1033 1.0 1.4904e-01 1.1 1.03e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1  2  0  0  0   1  2  0  0  0 13862       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     3100 1.0 5.9457e-02 1.8 0.00e+00 0.0 1.2e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       3100 1.0 3.0450e-01 5.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.5800e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.3689e-03 3.5 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      3100 1.0 5.3555e-02 1.9 0.00e+00 0.0 1.2e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        3100 1.0 3.0101e-01 6.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              3100 1.0 1.4451e-0210.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            3100 1.0 1.7531e-03 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 3.5511e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 1.0082e+01 1.0 4.69e+09 1.0 1.2e+05 8.0e+03 5.2e+03 96100100100100  96100100100100  9297       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 3.6000e-07 2.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply             3100 1.0 4.9047e+00 1.1 1.54e+09 1.0 0.0e+00 0.0e+00 0.0e+00 45 33  0  0  0  45 33  0  0  0  6268       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    22             22      8045888     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1416     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.94e-08
Average time for MPI_Barrier(): 1.26066e-05
Average time for zero size MPI_Send(): 6.2813e-06
#PETSc Option Table entries:
-ksp_type tcqmr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type sor
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.000147819 iterations 474
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8078 with 20 processors, by luciano.siqueira Wed Sep  9 17:35:29 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           9.885e+00     1.000   9.885e+00
Objects:              5.500e+01     1.000   5.500e+01
Flop:                 4.090e+09     1.001   4.089e+09  8.179e+10
Flop/sec:             4.138e+08     1.001   4.137e+08  8.274e+09
MPI Messages:         5.716e+03     2.000   5.430e+03  1.086e+05
MPI Message Lengths:  4.570e+07     2.000   7.996e+03  8.684e+08
MPI Reductions:       2.413e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 9.8851e+00 100.0%  8.1788e+10 100.0%  1.086e+05 100.0%  7.996e+03      100.0%  2.406e+03  99.7%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 5.7072e-0376.2 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 5.6837e-03109.3 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             2856 1.0 3.1785e+00 1.1 1.28e+09 1.0 1.1e+05 8.0e+03 0.0e+00 31 31100100  0  31 31100100  0  8080       0      0 0.00e+00    0 0.00e+00  0
MatSOR              2857 1.0 4.5143e+00 1.0 1.42e+09 1.0 0.0e+00 0.0e+00 0.0e+00 45 35  0  0  0  45 35  0  0  0  6276       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 5.7360e-0355.8 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7528e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot              1422 1.0 2.2903e-01 1.6 1.42e+08 1.0 0.0e+00 0.0e+00 1.4e+03  2  3  0  0 59   2  3  0  0 59 12418       0      0 0.00e+00    0 0.00e+00  0
VecMDot               10 1.0 3.8483e-03 1.1 5.50e+06 1.0 0.0e+00 0.0e+00 1.0e+01  0  0  0  0  0   0  0  0  0  0 28584       0      0 0.00e+00    0 0.00e+00  0
VecNorm              961 1.0 2.3430e-01 2.5 9.61e+07 1.0 0.0e+00 0.0e+00 9.6e+02  2  2  0  0 40   2  2  0  0 40  8203       0      0 0.00e+00    0 0.00e+00  0
VecScale            1908 1.0 4.4007e-02 1.1 9.54e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  2  0  0  0   0  2  0  0  0 43357       0      0 0.00e+00    0 0.00e+00  0
VecCopy             7117 1.0 3.9138e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  4  0  0  0  0   4  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet              2856 1.0 1.1139e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             4265 1.0 2.5368e-01 1.0 4.26e+08 1.0 0.0e+00 0.0e+00 0.0e+00  2 10  0  0  0   2 10  0  0  0 33625       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             3320 1.0 4.7448e-01 1.0 2.61e+08 1.0 0.0e+00 0.0e+00 0.0e+00  5  6  0  0  0   5  6  0  0  0 10995       0      0 0.00e+00    0 0.00e+00  0
VecAXPBYCZ          1423 1.0 2.5258e-01 1.0 3.56e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3  9  0  0  0   3  9  0  0  0 28170       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY              11 1.0 5.0210e-03 1.0 6.50e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 25891       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     2856 1.0 4.8400e-02 1.6 0.00e+00 0.0 1.1e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       2856 1.0 2.9775e-01 5.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecNormalize          11 1.0 7.8583e-03 1.0 1.65e+06 1.0 0.0e+00 0.0e+00 1.1e+01  0  0  0  0  0   0  0  0  0  0  4199       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.7440e-06 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.2073e-03 3.1 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      2856 1.0 4.3248e-02 1.7 0.00e+00 0.0 1.1e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        2856 1.0 2.9405e-01 5.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              2856 1.0 1.1116e-02 8.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            2856 1.0 1.5221e-03 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.4929e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  1  1  0  0  1   1  1  0  0  1  3182       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 9.4982e+00 1.0 4.09e+09 1.0 1.1e+05 8.0e+03 2.4e+03 96100100100 99  96100100100100  8610       0      0 0.00e+00    0 0.00e+00  0
KSPGMRESOrthog        10 1.0 1.2330e-02 1.0 1.10e+07 1.0 0.0e+00 0.0e+00 1.0e+01  0  0  0  0  0   0  0  0  0  0 17843       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 1.4611e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  1  1  0  0  1   1  1  0  0  1  3251       0      0 0.00e+00    0 0.00e+00  0
PCApply             1434 1.0 6.9880e+00 1.0 2.63e+09 1.0 5.4e+04 8.0e+03 0.0e+00 70 64 50 50  0  70 64 50 50  0  7514       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    41             41     15678720     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     3              3        33432     0.
      Preconditioner     3              3         2936     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.8e-08
Average time for MPI_Barrier(): 1.08352e-05
Average time for zero size MPI_Send(): 6.1191e-06
#PETSc Option Table entries:
-ksp_type tcqmr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 7.08235e-05 iterations 490
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8078 with 20 processors, by luciano.siqueira Wed Sep  9 17:35:34 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.083e+00     1.000   3.083e+00
Objects:              3.000e+01     1.000   3.000e+01
Flop:                 1.345e+09     1.001   1.345e+09  2.689e+10
Flop/sec:             4.362e+08     1.001   4.362e+08  8.724e+09
MPI Messages:         1.966e+03     2.000   1.868e+03  3.735e+04
MPI Message Lengths:  1.570e+07     2.000   7.988e+03  2.984e+08
MPI Reductions:       1.491e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.0825e+00 100.0%  2.6891e+10 100.0%  3.735e+04 100.0%  7.988e+03      100.0%  1.484e+03  99.5%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 6.0032e-0387.9 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 5.9420e-03112.7 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult              981 1.0 1.0388e+00 1.0 4.41e+08 1.0 3.7e+04 8.0e+03 0.0e+00 33 33100100  0  33 33100100  0  8492       0      0 0.00e+00    0 0.00e+00  0
MatSolve             981 1.0 1.1290e+00 1.0 4.37e+08 1.0 0.0e+00 0.0e+00 0.0e+00 36 33  0  0  0  36 33  0  0  0  7747       0      0 0.00e+00    0 0.00e+00  0
MatLUFactorNum         1 1.0 7.5545e-03 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1437       0      0 0.00e+00    0 0.00e+00  0
MatILUFactorSym        1 1.0 3.4381e-03 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 5.9929e-0356.9 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7208e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetRowIJ            1 1.0 2.7500e-06 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetOrdering         1 1.0 6.9474e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot               981 1.0 1.3627e-01 1.3 9.81e+07 1.0 0.0e+00 0.0e+00 9.8e+02  4  7  0  0 66   4  7  0  0 66 14398       0      0 0.00e+00    0 0.00e+00  0
VecNorm              492 1.0 4.8496e-02 1.2 4.92e+07 1.0 0.0e+00 0.0e+00 4.9e+02  1  4  0  0 33   1  4  0  0 33 20290       0      0 0.00e+00    0 0.00e+00  0
VecCopy                4 1.0 2.9919e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet               984 1.0 1.7998e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             1470 1.0 8.7523e-02 1.1 1.47e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3 11  0  0  0   3 11  0  0  0 33591       0      0 0.00e+00    0 0.00e+00  0
VecWAXPY            1958 1.0 2.6392e-01 1.0 1.71e+08 1.0 0.0e+00 0.0e+00 0.0e+00  8 13  0  0  0   8 13  0  0  0 12981       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin      981 1.0 1.3616e-02 1.4 0.00e+00 0.0 3.7e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd        981 1.0 4.6890e-02 2.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.6100e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.1161e-03 2.9 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin       981 1.0 1.2240e-02 1.5 0.00e+00 0.0 3.7e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd         981 1.0 4.5912e-02 2.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack               981 1.0 2.7540e-03 8.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack             981 1.0 4.4857e-04 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.4569e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.7101e+00 1.0 1.34e+09 1.0 3.7e+04 8.0e+03 1.5e+03 88100100100 99  88100100100 99  9918       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                2 1.0 2.6325e-02 1.2 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0   412       0      0 0.00e+00    0 0.00e+00  0
PCSetUpOnBlocks        1 1.0 1.1708e-02 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   927       0      0 0.00e+00    0 0.00e+00  0
PCApply              981 1.0 1.1535e+00 1.0 4.37e+08 1.0 0.0e+00 0.0e+00 0.0e+00 37 33  0  0  0  37 33  0  0  0  7582       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              4     11791508     0.
         Vec Scatter     1              1          816     0.
              Vector    14             14      4031808     0.
           Index Set     5              5       608512     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     2              2         2832     0.
      Preconditioner     2              2         1944     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.07e-08
Average time for MPI_Barrier(): 1.52944e-05
Average time for zero size MPI_Send(): 6.5421e-06
#PETSc Option Table entries:
-ksp_type cgs
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type bjacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 8.26951e-08 iterations 1553
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8078 with 20 processors, by luciano.siqueira Wed Sep  9 17:35:41 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           5.508e+00     1.000   5.508e+00
Objects:              2.300e+01     1.000   2.300e+01
Flop:                 3.028e+09     1.002   3.028e+09  6.055e+10
Flop/sec:             5.498e+08     1.002   5.496e+08  1.099e+10
MPI Messages:         6.218e+03     2.000   5.907e+03  1.181e+05
MPI Message Lengths:  4.972e+07     2.000   7.996e+03  9.447e+08
MPI Reductions:       4.682e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 5.5084e+00 100.0%  6.0554e+10 100.0%  1.181e+05 100.0%  7.996e+03      100.0%  4.675e+03  99.9%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 2.8889e-0336.5 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 2.8277e-0355.4 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             3107 1.0 3.2211e+00 1.0 1.40e+09 1.0 1.2e+05 8.0e+03 0.0e+00 58 46100100  0  58 46100100  0  8674       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 2.8776e-0327.3 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7912e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot              3107 1.0 3.4372e-01 1.2 3.11e+08 1.0 0.0e+00 0.0e+00 3.1e+03  5 10  0  0 66   5 10  0  0 66 18079       0      0 0.00e+00    0 0.00e+00  0
VecNorm             1555 1.0 1.2494e-01 1.0 1.56e+08 1.0 0.0e+00 0.0e+00 1.6e+03  2  5  0  0 33   2  5  0  0 33 24891       0      0 0.00e+00    0 0.00e+00  0
VecCopy                4 1.0 2.2565e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 3 1.0 2.1794e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             4659 1.0 2.5641e-01 1.0 4.66e+08 1.0 0.0e+00 0.0e+00 0.0e+00  5 15  0  0  0   5 15  0  0  0 36341       0      0 0.00e+00    0 0.00e+00  0
VecWAXPY            6210 1.0 8.1742e-01 1.0 5.43e+08 1.0 0.0e+00 0.0e+00 0.0e+00 15 18  0  0  0  15 18  0  0  0 13294       0      0 0.00e+00    0 0.00e+00  0
VecPointwiseMult    3107 1.0 4.3947e-01 1.0 1.55e+08 1.0 0.0e+00 0.0e+00 0.0e+00  8  5  0  0  0   8  5  0  0  0  7070       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     3107 1.0 3.4061e-02 1.5 0.00e+00 0.0 1.2e+05 8.0e+03 0.0e+00  1  0100100  0   1  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       3107 1.0 1.3780e-01 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.8850e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.4592e-03 3.7 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      3107 1.0 3.0606e-02 1.5 0.00e+00 0.0 1.2e+05 8.0e+03 0.0e+00  1  0100100  0   1  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        3107 1.0 1.3479e-01 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              3107 1.0 7.5556e-03 7.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            3107 1.0 1.4871e-03 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 1.4469e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 5.1294e+00 1.0 3.03e+09 1.0 1.2e+05 8.0e+03 4.7e+03 93100100100100  93100100100100 11803       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 8.9600e-07 3.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply             3107 1.0 4.4390e-01 1.0 1.55e+08 1.0 0.0e+00 0.0e+00 2.0e+00  8  5  0  0  0   8  5  0  0  0  6999       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      8013484     0.
         Vec Scatter     1              1          816     0.
              Vector    13             13      4430336     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1416     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.05e-08
Average time for MPI_Barrier(): 1.157e-05
Average time for zero size MPI_Send(): 6.19875e-06
#PETSc Option Table entries:
-ksp_type cgs
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type jacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 8.535e-05 iterations 546
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8078 with 20 processors, by luciano.siqueira Wed Sep  9 17:35:47 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.772e+00     1.000   3.772e+00
Objects:              2.200e+01     1.000   2.200e+01
Flop:                 1.552e+09     1.001   1.552e+09  3.104e+10
Flop/sec:             4.115e+08     1.001   4.115e+08  8.230e+09
MPI Messages:         2.190e+03     2.000   2.080e+03  4.161e+04
MPI Message Lengths:  1.750e+07     2.000   7.989e+03  3.324e+08
MPI Reductions:       1.659e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.7721e+00 100.0%  3.1043e+10 100.0%  4.161e+04 100.0%  7.989e+03      100.0%  1.652e+03  99.6%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 3.5095e-0331.9 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 2.4678e-0344.7 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             1093 1.0 1.1768e+00 1.1 4.92e+08 1.0 4.2e+04 8.0e+03 0.0e+00 30 32100100  0  30 32100100  0  8352       0      0 0.00e+00    0 0.00e+00  0
MatSOR              1093 1.0 1.6414e+00 1.0 5.42e+08 1.0 0.0e+00 0.0e+00 0.0e+00 43 35  0  0  0  43 35  0  0  0  6604       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 2.5203e-0323.2 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7032e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot              1093 1.0 2.0094e-01 1.7 1.09e+08 1.0 0.0e+00 0.0e+00 1.1e+03  5  7  0  0 66   5  7  0  0 66 10879       0      0 0.00e+00    0 0.00e+00  0
VecNorm              548 1.0 5.4404e-02 1.1 5.48e+07 1.0 0.0e+00 0.0e+00 5.5e+02  1  4  0  0 33   1  4  0  0 33 20146       0      0 0.00e+00    0 0.00e+00  0
VecCopy                4 1.0 2.5185e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 3 1.0 2.1033e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             1638 1.0 1.0591e-01 1.1 1.64e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3 11  0  0  0   3 11  0  0  0 30931       0      0 0.00e+00    0 0.00e+00  0
VecWAXPY            2182 1.0 3.0481e-01 1.0 1.91e+08 1.0 0.0e+00 0.0e+00 0.0e+00  8 12  0  0  0   8 12  0  0  0 12526       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     1093 1.0 1.8791e-02 1.5 0.00e+00 0.0 4.2e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       1093 1.0 1.0612e-01 3.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.6810e-06 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.2170e-03 3.0 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      1093 1.0 1.6610e-02 1.6 0.00e+00 0.0 4.2e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        1093 1.0 1.0484e-01 3.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              1093 1.0 4.0338e-03 8.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            1093 1.0 5.7966e-04 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 1.4506e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 3.3942e+00 1.0 1.55e+09 1.0 4.1e+04 8.0e+03 1.6e+03 90100100100 99  90100100100 99  9142       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 3.4600e-07 2.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply             1093 1.0 1.6440e+00 1.0 5.42e+08 1.0 0.0e+00 0.0e+00 0.0e+00 43 35  0  0  0  43 35  0  0  0  6593       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    12             12      4028608     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1416     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.96e-08
Average time for MPI_Barrier(): 1.1319e-05
Average time for zero size MPI_Send(): 6.23555e-06
#PETSc Option Table entries:
-ksp_type cgs
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type sor
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.000193207 iterations 288
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8078 with 20 processors, by luciano.siqueira Wed Sep  9 17:35:53 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           4.443e+00     1.000   4.443e+00
Objects:              4.500e+01     1.000   4.500e+01
Flop:                 1.619e+09     1.001   1.619e+09  3.238e+10
Flop/sec:             3.645e+08     1.001   3.645e+08  7.289e+09
MPI Messages:         2.332e+03     2.000   2.215e+03  4.431e+04
MPI Message Lengths:  1.863e+07     2.000   7.990e+03  3.540e+08
MPI Reductions:       9.080e+02     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 4.4428e+00 100.0%  3.2385e+10 100.0%  4.431e+04 100.0%  7.990e+03      100.0%  9.010e+02  99.2%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 1.0599e-0275.7 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 1.0552e-02198.2 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             1164 1.0 1.4198e+00 1.1 5.24e+08 1.0 4.4e+04 8.0e+03 0.0e+00 31 32100100  0  31 32100100  0  7372       0      0 0.00e+00    0 0.00e+00  0
MatSOR              1165 1.0 1.9184e+00 1.1 5.78e+08 1.0 0.0e+00 0.0e+00 0.0e+00 41 36  0  0  0  41 36  0  0  0  6022       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 1.0601e-0297.1 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6995e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  1   1  0  0  0  1     0       0      0 0.00e+00    0 0.00e+00  0
VecDot               577 1.0 1.8028e-01 2.6 5.77e+07 1.0 0.0e+00 0.0e+00 5.8e+02  3  4  0  0 64   3  4  0  0 64  6401       0      0 0.00e+00    0 0.00e+00  0
VecMDot               10 1.0 4.6669e-03 1.4 5.50e+06 1.0 0.0e+00 0.0e+00 1.0e+01  0  0  0  0  1   0  0  0  0  1 23570       0      0 0.00e+00    0 0.00e+00  0
VecNorm              301 1.0 3.4157e-02 1.1 3.01e+07 1.0 0.0e+00 0.0e+00 3.0e+02  1  2  0  0 33   1  2  0  0 33 17624       0      0 0.00e+00    0 0.00e+00  0
VecScale              11 1.0 2.6754e-04 1.1 5.50e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 41115       0      0 0.00e+00    0 0.00e+00  0
VecCopy             1159 1.0 5.4242e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet              1159 1.0 3.4505e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY              865 1.0 6.0922e-02 1.1 8.65e+07 1.0 0.0e+00 0.0e+00 0.0e+00  1  5  0  0  0   1  5  0  0  0 28397       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             1154 1.0 1.7598e-01 1.1 8.66e+07 1.0 0.0e+00 0.0e+00 0.0e+00  4  5  0  0  0   4  5  0  0  0  9836       0      0 0.00e+00    0 0.00e+00  0
VecAXPBYCZ           577 1.0 1.1184e-01 1.1 1.44e+08 1.0 0.0e+00 0.0e+00 0.0e+00  2  9  0  0  0   2  9  0  0  0 25796       0      0 0.00e+00    0 0.00e+00  0
VecWAXPY            1150 1.0 1.7937e-01 1.1 1.01e+08 1.0 0.0e+00 0.0e+00 0.0e+00  4  6  0  0  0   4  6  0  0  0 11217       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY              11 1.0 5.0539e-03 1.1 6.50e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 25723       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     1164 1.0 2.1053e-02 1.4 0.00e+00 0.0 4.4e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       1164 1.0 2.5235e-0111.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  3  0  0  0  0   3  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecNormalize          11 1.0 7.9494e-03 1.1 1.65e+06 1.0 0.0e+00 0.0e+00 1.1e+01  0  0  0  0  1   0  0  0  0  1  4151       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 1.9853e-05 4.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.2061e-03 3.2 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      1164 1.0 1.9277e-02 1.5 0.00e+00 0.0 4.4e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        1164 1.0 2.5093e-0112.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  3  0  0  0  0   3  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              1164 1.0 4.8152e-03 9.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            1164 1.0 6.1115e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.4541e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  3  1  1  1  3   3  1  1  1  3  3267       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 4.0598e+00 1.0 1.62e+09 1.0 4.4e+04 8.0e+03 8.9e+02 91100100100 98  91100100100 99  7974       0      0 0.00e+00    0 0.00e+00  0
KSPGMRESOrthog        10 1.0 1.2788e-02 1.1 1.10e+07 1.0 0.0e+00 0.0e+00 1.0e+01  0  1  0  0  1   0  1  0  0  1 17204       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 1.4426e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  3  1  1  1  3   3  1  1  1  3  3293       0      0 0.00e+00    0 0.00e+00  0
PCApply              588 1.0 2.9478e+00 1.0 1.07e+09 1.0 2.2e+04 8.0e+03 0.0e+00 65 66 49 50  0  65 66 49 50  0  7245       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    31             31     11661440     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     3              3        33432     0.
      Preconditioner     3              3         2936     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.96e-08
Average time for MPI_Barrier(): 1.24784e-05
Average time for zero size MPI_Send(): 6.2078e-06
#PETSc Option Table entries:
-ksp_type cgs
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.00114039 iterations 479
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8078 with 20 processors, by luciano.siqueira Wed Sep  9 17:35:58 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.027e+00     1.000   3.027e+00
Objects:              2.900e+01     1.000   2.900e+01
Flop:                 1.387e+09     1.001   1.386e+09  2.773e+10
Flop/sec:             4.581e+08     1.001   4.581e+08  9.161e+09
MPI Messages:         1.922e+03     2.000   1.826e+03  3.652e+04
MPI Message Lengths:  1.535e+07     2.000   7.988e+03  2.917e+08
MPI Reductions:       1.936e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.0268e+00 100.0%  2.7729e+10 100.0%  3.652e+04 100.0%  7.988e+03      100.0%  1.929e+03  99.6%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 4.2337e-0358.3 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 4.0571e-03100.1 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult              959 1.0 1.0256e+00 1.1 4.31e+08 1.0 3.6e+04 8.0e+03 0.0e+00 33 31100100  0  33 31100100  0  8408       0      0 0.00e+00    0 0.00e+00  0
MatSolve             959 1.0 1.0705e+00 1.0 4.28e+08 1.0 0.0e+00 0.0e+00 0.0e+00 35 31  0  0  0  35 31  0  0  0  7987       0      0 0.00e+00    0 0.00e+00  0
MatLUFactorNum         1 1.0 7.5498e-03 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1438       0      0 0.00e+00    0 0.00e+00  0
MatILUFactorSym        1 1.0 3.4332e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 4.1069e-0344.0 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7535e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetRowIJ            1 1.0 6.0420e-06 3.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetOrdering         1 1.0 6.9134e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot               958 1.0 1.3090e-01 1.3 9.58e+07 1.0 0.0e+00 0.0e+00 9.6e+02  4  7  0  0 49   4  7  0  0 50 14637       0      0 0.00e+00    0 0.00e+00  0
VecDotNorm2          479 1.0 1.3019e-01 1.4 9.58e+07 1.0 0.0e+00 0.0e+00 4.8e+02  4  7  0  0 25   4  7  0  0 25 14717       0      0 0.00e+00    0 0.00e+00  0
VecNorm              481 1.0 5.0262e-02 1.2 4.81e+07 1.0 0.0e+00 0.0e+00 4.8e+02  1  3  0  0 25   1  3  0  0 25 19140       0      0 0.00e+00    0 0.00e+00  0
VecCopy                2 1.0 1.4881e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet               964 1.0 2.2591e-02 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY                1 1.0 8.6530e-05 1.1 1.00e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 23113       0      0 0.00e+00    0 0.00e+00  0
VecAXPBYCZ           958 1.0 1.6011e-01 1.0 1.92e+08 1.0 0.0e+00 0.0e+00 0.0e+00  5 14  0  0  0   5 14  0  0  0 23934       0      0 0.00e+00    0 0.00e+00  0
VecWAXPY             958 1.0 1.2462e-01 1.0 9.58e+07 1.0 0.0e+00 0.0e+00 0.0e+00  4  7  0  0  0   4  7  0  0  0 15375       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin      959 1.0 1.3090e-02 1.4 0.00e+00 0.0 3.6e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd        959 1.0 7.7793e-02 3.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.7200e-06 2.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 2.5396e-0312.0 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin       959 1.0 1.1733e-02 1.5 0.00e+00 0.0 3.6e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd         959 1.0 7.6548e-02 4.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack               959 1.0 2.7669e-03 6.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack             959 1.0 5.1089e-04 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.2351e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.6557e+00 1.0 1.39e+09 1.0 3.6e+04 8.0e+03 1.9e+03 88100100100 99  88100100100 99 10436       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                2 1.0 1.7274e-02 1.3 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0   628       0      0 0.00e+00    0 0.00e+00  0
PCSetUpOnBlocks        1 1.0 1.1668e-02 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   930       0      0 0.00e+00    0 0.00e+00  0
PCApply              959 1.0 1.0992e+00 1.0 4.28e+08 1.0 0.0e+00 0.0e+00 0.0e+00 36 31  0  0  0  36 31  0  0  0  7779       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              4     11791508     0.
         Vec Scatter     1              1          816     0.
              Vector    13             13      3630080     0.
           Index Set     5              5       608512     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     2              2         2840     0.
      Preconditioner     2              2         1944     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.87e-08
Average time for MPI_Barrier(): 1.05188e-05
Average time for zero size MPI_Send(): 6.22775e-06
#PETSc Option Table entries:
-ksp_type bcgs
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type bjacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.00878969 iterations 1368
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8078 with 20 processors, by luciano.siqueira Wed Sep  9 17:36:05 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           5.081e+00     1.000   5.081e+00
Objects:              2.200e+01     1.000   2.200e+01
Flop:                 2.873e+09     1.002   2.873e+09  5.745e+10
Flop/sec:             5.655e+08     1.002   5.654e+08  1.131e+10
MPI Messages:         5.478e+03     2.000   5.204e+03  1.041e+05
MPI Message Lengths:  4.380e+07     2.000   7.996e+03  8.322e+08
MPI Reductions:       5.494e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 5.0807e+00 100.0%  5.7450e+10 100.0%  1.041e+05 100.0%  7.996e+03      100.0%  5.487e+03  99.9%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 5.5670e-0364.8 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 4.5883e-0398.3 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             2737 1.0 2.8706e+00 1.0 1.23e+09 1.0 1.0e+05 8.0e+03 0.0e+00 56 43100100  0  56 43100100  0  8574       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 4.6404e-0345.5 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.8058e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot              2736 1.0 2.7095e-01 1.2 2.74e+08 1.0 0.0e+00 0.0e+00 2.7e+03  5 10  0  0 50   5 10  0  0 50 20196       0      0 0.00e+00    0 0.00e+00  0
VecDotNorm2         1368 1.0 2.8618e-01 1.1 2.74e+08 1.0 0.0e+00 0.0e+00 1.4e+03  5 10  0  0 25   5 10  0  0 25 19121       0      0 0.00e+00    0 0.00e+00  0
VecNorm             1370 1.0 1.2237e-01 1.0 1.37e+08 1.0 0.0e+00 0.0e+00 1.4e+03  2  5  0  0 25   2  5  0  0 25 22392       0      0 0.00e+00    0 0.00e+00  0
VecCopy                2 1.0 1.1408e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 5 1.0 3.2762e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY                1 1.0 7.9911e-05 1.1 1.00e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 25028       0      0 0.00e+00    0 0.00e+00  0
VecAXPBYCZ          2736 1.0 4.5720e-01 1.0 5.47e+08 1.0 0.0e+00 0.0e+00 0.0e+00  9 19  0  0  0   9 19  0  0  0 23937       0      0 0.00e+00    0 0.00e+00  0
VecWAXPY            2736 1.0 3.5677e-01 1.0 2.74e+08 1.0 0.0e+00 0.0e+00 0.0e+00  7 10  0  0  0   7 10  0  0  0 15337       0      0 0.00e+00    0 0.00e+00  0
VecPointwiseMult    2737 1.0 3.8587e-01 1.0 1.37e+08 1.0 0.0e+00 0.0e+00 0.0e+00  7  5  0  0  0   7  5  0  0  0  7093       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     2737 1.0 3.1036e-02 1.5 0.00e+00 0.0 1.0e+05 8.0e+03 0.0e+00  1  0100100  0   1  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       2737 1.0 1.3131e-01 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.8080e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.3453e-03 3.5 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      2737 1.0 2.7945e-02 1.6 0.00e+00 0.0 1.0e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        2737 1.0 1.2891e-01 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              2737 1.0 6.8602e-03 7.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            2737 1.0 1.5332e-03 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 1.2386e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 4.7025e+00 1.0 2.87e+09 1.0 1.0e+05 8.0e+03 5.5e+03 93100100100100  93100100100100 12214       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 7.3200e-07 2.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply             2737 1.0 3.8973e-01 1.0 1.37e+08 1.0 0.0e+00 0.0e+00 2.0e+00  8  5  0  0  0   8  5  0  0  0  7023       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      8013484     0.
         Vec Scatter     1              1          816     0.
              Vector    12             12      4028608     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1424     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.12e-08
Average time for MPI_Barrier(): 1.18502e-05
Average time for zero size MPI_Send(): 6.4131e-06
#PETSc Option Table entries:
-ksp_type bcgs
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type jacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.00987163 iterations 515
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8078 with 20 processors, by luciano.siqueira Wed Sep  9 17:36:11 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.659e+00     1.000   3.659e+00
Objects:              2.100e+01     1.000   2.100e+01
Flop:                 1.542e+09     1.001   1.542e+09  3.083e+10
Flop/sec:             4.214e+08     1.001   4.213e+08  8.426e+09
MPI Messages:         2.066e+03     2.000   1.963e+03  3.925e+04
MPI Message Lengths:  1.650e+07     2.000   7.988e+03  3.136e+08
MPI Reductions:       2.080e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.6589e+00 100.0%  3.0831e+10 100.0%  3.925e+04 100.0%  7.988e+03      100.0%  2.073e+03  99.7%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 3.3689e-0338.9 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 2.9994e-0368.1 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             1031 1.0 1.0765e+00 1.0 4.64e+08 1.0 3.9e+04 8.0e+03 0.0e+00 29 30100100  0  29 30100100  0  8612       0      0 0.00e+00    0 0.00e+00  0
MatSOR              1031 1.0 1.6040e+00 1.0 5.11e+08 1.0 0.0e+00 0.0e+00 0.0e+00 43 33  0  0  0  43 33  0  0  0  6374       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 3.0510e-0331.6 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7436e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot              1030 1.0 1.5549e-01 1.3 1.03e+08 1.0 0.0e+00 0.0e+00 1.0e+03  4  7  0  0 50   4  7  0  0 50 13249       0      0 0.00e+00    0 0.00e+00  0
VecDotNorm2          515 1.0 1.4996e-01 1.4 1.03e+08 1.0 0.0e+00 0.0e+00 5.2e+02  4  7  0  0 25   4  7  0  0 25 13737       0      0 0.00e+00    0 0.00e+00  0
VecNorm              517 1.0 5.8009e-02 1.2 5.17e+07 1.0 0.0e+00 0.0e+00 5.2e+02  1  3  0  0 25   1  3  0  0 25 17825       0      0 0.00e+00    0 0.00e+00  0
VecCopy                2 1.0 1.2559e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 5 1.0 3.3102e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY                1 1.0 8.3849e-05 1.1 1.00e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 23852       0      0 0.00e+00    0 0.00e+00  0
VecAXPBYCZ          1030 1.0 1.7669e-01 1.0 2.06e+08 1.0 0.0e+00 0.0e+00 0.0e+00  5 13  0  0  0   5 13  0  0  0 23318       0      0 0.00e+00    0 0.00e+00  0
VecWAXPY            1030 1.0 1.4788e-01 1.0 1.03e+08 1.0 0.0e+00 0.0e+00 0.0e+00  4  7  0  0  0   4  7  0  0  0 13930       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     1031 1.0 1.7948e-02 1.5 0.00e+00 0.0 3.9e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       1031 1.0 4.1151e-02 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.7420e-06 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.4530e-03 3.4 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      1031 1.0 1.6109e-02 1.6 0.00e+00 0.0 3.9e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        1031 1.0 3.9880e-02 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              1031 1.0 3.8402e-0310.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            1031 1.0 5.0940e-04 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 1.2504e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 3.2838e+00 1.0 1.54e+09 1.0 3.9e+04 8.0e+03 2.1e+03 90100100100 99  90100100100 99  9385       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 2.3600e-07 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply             1031 1.0 1.6060e+00 1.0 5.11e+08 1.0 0.0e+00 0.0e+00 0.0e+00 43 33  0  0  0  43 33  0  0  0  6366       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    11             11      3626880     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1424     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.98e-08
Average time for MPI_Barrier(): 1.39626e-05
Average time for zero size MPI_Send(): 6.2569e-06
#PETSc Option Table entries:
-ksp_type bcgs
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type sor
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.000785404 iterations 301
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8078 with 20 processors, by luciano.siqueira Wed Sep  9 17:36:17 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           4.456e+00     1.000   4.456e+00
Objects:              4.400e+01     1.000   4.400e+01
Flop:                 1.737e+09     1.001   1.737e+09  3.473e+10
Flop/sec:             3.898e+08     1.001   3.897e+08  7.794e+09
MPI Messages:         2.436e+03     2.000   2.314e+03  4.628e+04
MPI Message Lengths:  1.946e+07     2.000   7.990e+03  3.698e+08
MPI Reductions:       1.247e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 4.4558e+00 100.0%  3.4730e+10 100.0%  4.628e+04 100.0%  7.990e+03      100.0%  1.240e+03  99.4%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 3.1503e-0318.3 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 2.7786e-0356.1 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             1216 1.0 1.3308e+00 1.0 5.47e+08 1.0 4.6e+04 8.0e+03 0.0e+00 30 31100100  0  30 31100100  0  8216       0      0 0.00e+00    0 0.00e+00  0
MatSOR              1217 1.0 1.8773e+00 1.0 6.03e+08 1.0 0.0e+00 0.0e+00 0.0e+00 42 35  0  0  0  42 35  0  0  0  6429       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 2.8295e-0327.3 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6814e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot               602 1.0 9.1591e-02 1.1 6.02e+07 1.0 0.0e+00 0.0e+00 6.0e+02  2  3  0  0 48   2  3  0  0 49 13145       0      0 0.00e+00    0 0.00e+00  0
VecDotNorm2          301 1.0 8.8029e-02 1.1 6.02e+07 1.0 0.0e+00 0.0e+00 3.0e+02  2  3  0  0 24   2  3  0  0 24 13677       0      0 0.00e+00    0 0.00e+00  0
VecMDot               10 1.0 4.7050e-03 1.4 5.50e+06 1.0 0.0e+00 0.0e+00 1.0e+01  0  0  0  0  1   0  0  0  0  1 23379       0      0 0.00e+00    0 0.00e+00  0
VecNorm              314 1.0 3.8684e-02 1.1 3.14e+07 1.0 0.0e+00 0.0e+00 3.1e+02  1  2  0  0 25   1  2  0  0 25 16234       0      0 0.00e+00    0 0.00e+00  0
VecScale              11 1.0 2.6140e-04 1.1 5.50e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 42081       0      0 0.00e+00    0 0.00e+00  0
VecCopy             1209 1.0 5.7609e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet              1213 1.0 4.1133e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY                2 1.0 1.6339e-04 1.2 2.00e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 24481       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             1206 1.0 1.7096e-01 1.0 9.04e+07 1.0 0.0e+00 0.0e+00 0.0e+00  4  5  0  0  0   4  5  0  0  0 10581       0      0 0.00e+00    0 0.00e+00  0
VecAXPBYCZ          1205 1.0 2.1022e-01 1.0 2.71e+08 1.0 0.0e+00 0.0e+00 0.0e+00  5 16  0  0  0   5 16  0  0  0 25796       0      0 0.00e+00    0 0.00e+00  0
VecWAXPY             602 1.0 9.0174e-02 1.0 6.02e+07 1.0 0.0e+00 0.0e+00 0.0e+00  2  3  0  0  0   2  3  0  0  0 13352       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY              11 1.0 5.0512e-03 1.1 6.50e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 25737       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     1216 1.0 2.1284e-02 1.4 0.00e+00 0.0 4.6e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       1216 1.0 9.0454e-02 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecNormalize          11 1.0 7.7204e-03 1.1 1.65e+06 1.0 0.0e+00 0.0e+00 1.1e+01  0  0  0  0  1   0  0  0  0  1  4274       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.8500e-06 2.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 2.0145e-03 5.0 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      1216 1.0 1.8947e-02 1.5 0.00e+00 0.0 4.6e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        1216 1.0 8.9024e-02 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              1216 1.0 4.2963e-03 8.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            1216 1.0 7.5407e-04 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.4433e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  3  1  1  1  2   3  1  1  1  2  3291       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 4.0771e+00 1.0 1.74e+09 1.0 4.6e+04 8.0e+03 1.2e+03 91100100100 98  91100100100 99  8515       0      0 0.00e+00    0 0.00e+00  0
KSPGMRESOrthog        10 1.0 1.2921e-02 1.1 1.10e+07 1.0 0.0e+00 0.0e+00 1.0e+01  0  1  0  0  1   0  1  0  0  1 17026       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 1.4335e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  3  1  1  1  2   3  1  1  1  2  3313       0      0 0.00e+00    0 0.00e+00  0
PCApply              614 1.0 2.9225e+00 1.0 1.12e+09 1.0 2.3e+04 8.0e+03 0.0e+00 65 64 50 50  0  65 64 50 50  0  7636       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    30             30     11259712     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     3              3        33440     0.
      Preconditioner     3              3         2936     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.86e-08
Average time for MPI_Barrier(): 1.2658e-05
Average time for zero size MPI_Send(): 6.2358e-06
#PETSc Option Table entries:
-ksp_type bcgs
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 8.37695e-05 iterations 475
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8078 with 20 processors, by luciano.siqueira Wed Sep  9 17:36:22 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.183e+00     1.000   3.183e+00
Objects:              3.200e+01     1.000   3.200e+01
Flop:                 1.446e+09     1.001   1.446e+09  2.891e+10
Flop/sec:             4.542e+08     1.001   4.541e+08  9.083e+09
MPI Messages:         1.906e+03     2.000   1.811e+03  3.621e+04
MPI Message Lengths:  1.522e+07     2.000   7.987e+03  2.893e+08
MPI Reductions:       1.445e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.1830e+00 100.0%  2.8910e+10 100.0%  3.621e+04 100.0%  7.987e+03      100.0%  1.438e+03  99.5%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 7.6124e-0365.9 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 7.4958e-03137.0 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult              951 1.0 1.0219e+00 1.0 4.28e+08 1.0 3.6e+04 8.0e+03 0.0e+00 32 30100100  0  32 30100100  0  8368       0      0 0.00e+00    0 0.00e+00  0
MatSolve             951 1.0 1.0999e+00 1.0 4.24e+08 1.0 0.0e+00 0.0e+00 0.0e+00 34 29  0  0  0  34 29  0  0  0  7709       0      0 0.00e+00    0 0.00e+00  0
MatLUFactorNum         1 1.0 7.5193e-03 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1443       0      0 0.00e+00    0 0.00e+00  0
MatILUFactorSym        1 1.0 3.4506e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 7.5480e-0368.3 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7103e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetRowIJ            1 1.0 2.9530e-06 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetOrdering         1 1.0 6.9128e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot               950 1.0 1.2524e-01 1.2 9.50e+07 1.0 0.0e+00 0.0e+00 9.5e+02  4  7  0  0 66   4  7  0  0 66 15170       0      0 0.00e+00    0 0.00e+00  0
VecNorm              477 1.0 6.5256e-02 1.3 4.77e+07 1.0 0.0e+00 0.0e+00 4.8e+02  2  3  0  0 33   2  3  0  0 33 14619       0      0 0.00e+00    0 0.00e+00  0
VecCopy                5 1.0 3.5500e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet               955 1.0 2.0208e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             1899 1.0 1.0359e-01 1.1 1.90e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3 13  0  0  0   3 13  0  0  0 36664       0      0 0.00e+00    0 0.00e+00  0
VecAYPX              949 1.0 1.2855e-01 1.0 9.48e+07 1.0 0.0e+00 0.0e+00 0.0e+00  4  7  0  0  0   4  7  0  0  0 14749       0      0 0.00e+00    0 0.00e+00  0
VecWAXPY            1898 1.0 2.5678e-01 1.0 1.66e+08 1.0 0.0e+00 0.0e+00 0.0e+00  8 11  0  0  0   8 11  0  0  0 12933       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin      951 1.0 1.3421e-02 1.3 0.00e+00 0.0 3.6e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd        951 1.0 4.9934e-02 2.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.9310e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 2.6827e-0311.1 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin       951 1.0 1.2050e-02 1.3 0.00e+00 0.0 3.6e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd         951 1.0 4.8971e-02 2.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack               951 1.0 2.7132e-03 6.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack             951 1.0 4.5449e-04 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.8853e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.8054e+00 1.0 1.45e+09 1.0 3.6e+04 8.0e+03 1.4e+03 88100100100 99  88100100100 99 10301       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                2 1.0 2.5993e-02 1.3 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0   418       0      0 0.00e+00    0 0.00e+00  0
PCSetUpOnBlocks        1 1.0 1.1716e-02 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   926       0      0 0.00e+00    0 0.00e+00  0
PCApply              951 1.0 1.1245e+00 1.0 4.24e+08 1.0 0.0e+00 0.0e+00 0.0e+00 35 29  0  0  0  35 29  0  0  0  7540       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              4     11791508     0.
         Vec Scatter     1              1          816     0.
              Vector    16             16      4835264     0.
           Index Set     5              5       608512     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     2              2         2832     0.
      Preconditioner     2              2         1944     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.13e-08
Average time for MPI_Barrier(): 1.14908e-05
Average time for zero size MPI_Send(): 6.1721e-06
#PETSc Option Table entries:
-ksp_type tfqmr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type bjacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 3.10408e-07 iterations 1511
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8078 with 20 processors, by luciano.siqueira Wed Sep  9 17:36:30 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           5.805e+00     1.000   5.805e+00
Objects:              2.500e+01     1.000   2.500e+01
Flop:                 3.399e+09     1.002   3.399e+09  6.797e+10
Flop/sec:             5.856e+08     1.002   5.855e+08  1.171e+10
MPI Messages:         6.050e+03     2.000   5.748e+03  1.150e+05
MPI Message Lengths:  4.838e+07     2.000   7.996e+03  9.191e+08
MPI Reductions:       4.555e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 5.8049e+00 100.0%  6.7975e+10 100.0%  1.150e+05 100.0%  7.996e+03      100.0%  4.548e+03  99.8%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 6.2334e-0340.4 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 6.1635e-03100.4 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             3023 1.0 3.1204e+00 1.0 1.36e+09 1.0 1.1e+05 8.0e+03 0.0e+00 53 40100100  0  53 40100100  0  8711       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 6.2156e-0355.5 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6965e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot              3022 1.0 3.1521e-01 1.2 3.02e+08 1.0 0.0e+00 0.0e+00 3.0e+03  5  9  0  0 66   5  9  0  0 66 19174       0      0 0.00e+00    0 0.00e+00  0
VecNorm             1513 1.0 1.6384e-01 1.3 1.51e+08 1.0 0.0e+00 0.0e+00 1.5e+03  2  4  0  0 33   2  4  0  0 33 18470       0      0 0.00e+00    0 0.00e+00  0
VecCopy                5 1.0 3.1116e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 4 1.0 2.7361e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             6043 1.0 3.0517e-01 1.1 6.04e+08 1.0 0.0e+00 0.0e+00 0.0e+00  5 18  0  0  0   5 18  0  0  0 39604       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             3021 1.0 4.0028e-01 1.0 3.02e+08 1.0 0.0e+00 0.0e+00 0.0e+00  7  9  0  0  0   7  9  0  0  0 15089       0      0 0.00e+00    0 0.00e+00  0
VecWAXPY            6042 1.0 7.9486e-01 1.0 5.29e+08 1.0 0.0e+00 0.0e+00 0.0e+00 13 16  0  0  0  13 16  0  0  0 13302       0      0 0.00e+00    0 0.00e+00  0
VecPointwiseMult    3023 1.0 4.2292e-01 1.0 1.51e+08 1.0 0.0e+00 0.0e+00 0.0e+00  7  4  0  0  0   7  4  0  0  0  7148       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     3023 1.0 3.9007e-02 1.6 0.00e+00 0.0 1.1e+05 8.0e+03 0.0e+00  1  0100100  0   1  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       3023 1.0 1.1967e-01 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.5390e-06 2.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.3783e-03 3.4 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      3023 1.0 3.5052e-02 1.7 0.00e+00 0.0 1.1e+05 8.0e+03 0.0e+00  1  0100100  0   1  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        3023 1.0 1.1635e-01 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              3023 1.0 8.5260e-03 7.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            3023 1.0 1.5431e-03 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 1.8787e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 5.4346e+00 1.0 3.40e+09 1.0 1.1e+05 8.0e+03 4.5e+03 94100100100100  94100100100100 12505       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 1.0540e-06 3.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply             3023 1.0 4.2767e-01 1.0 1.51e+08 1.0 0.0e+00 0.0e+00 2.0e+00  7  4  0  0  0   7  4  0  0  0  7068       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      8013484     0.
         Vec Scatter     1              1          816     0.
              Vector    15             15      5233792     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1416     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.93e-08
Average time for MPI_Barrier(): 1.31522e-05
Average time for zero size MPI_Send(): 6.34875e-06
#PETSc Option Table entries:
-ksp_type tfqmr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type jacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.000233762 iterations 533
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8078 with 20 processors, by luciano.siqueira Wed Sep  9 17:36:36 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.972e+00     1.000   3.972e+00
Objects:              2.400e+01     1.000   2.400e+01
Flop:                 1.675e+09     1.001   1.675e+09  3.349e+10
Flop/sec:             4.217e+08     1.001   4.216e+08  8.432e+09
MPI Messages:         2.138e+03     2.000   2.031e+03  4.062e+04
MPI Message Lengths:  1.708e+07     2.000   7.989e+03  3.245e+08
MPI Reductions:       1.619e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.9724e+00 100.0%  3.3495e+10 100.0%  4.062e+04 100.0%  7.989e+03      100.0%  1.612e+03  99.6%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 4.8830e-0348.9 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 3.7933e-0380.2 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             1067 1.0 1.1542e+00 1.0 4.80e+08 1.0 4.1e+04 8.0e+03 0.0e+00 29 29100100  0  29 29100100  0  8313       0      0 0.00e+00    0 0.00e+00  0
MatSOR              1067 1.0 1.6660e+00 1.1 5.29e+08 1.0 0.0e+00 0.0e+00 0.0e+00 41 32  0  0  0  41 32  0  0  0  6351       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 3.8439e-0336.1 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7441e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot              1066 1.0 1.9457e-01 1.4 1.07e+08 1.0 0.0e+00 0.0e+00 1.1e+03  4  6  0  0 66   4  6  0  0 66 10957       0      0 0.00e+00    0 0.00e+00  0
VecNorm              535 1.0 1.1201e-01 2.0 5.35e+07 1.0 0.0e+00 0.0e+00 5.4e+02  2  3  0  0 33   2  3  0  0 33  9552       0      0 0.00e+00    0 0.00e+00  0
VecCopy                5 1.0 3.4928e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 4 1.0 2.7499e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             2131 1.0 1.2885e-01 1.1 2.13e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3 13  0  0  0   3 13  0  0  0 33077       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             1065 1.0 1.5163e-01 1.1 1.06e+08 1.0 0.0e+00 0.0e+00 0.0e+00  4  6  0  0  0   4  6  0  0  0 14034       0      0 0.00e+00    0 0.00e+00  0
VecWAXPY            2130 1.0 3.0430e-01 1.0 1.86e+08 1.0 0.0e+00 0.0e+00 0.0e+00  7 11  0  0  0   7 11  0  0  0 12248       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     1067 1.0 1.8354e-02 1.5 0.00e+00 0.0 4.1e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       1067 1.0 9.0548e-02 3.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.8350e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 2.5287e-03 6.2 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      1067 1.0 1.6484e-02 1.6 0.00e+00 0.0 4.1e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        1067 1.0 8.9175e-02 3.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              1067 1.0 3.9392e-03 8.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            1067 1.0 8.4076e-04 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 1.8535e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 3.5915e+00 1.0 1.67e+09 1.0 4.1e+04 8.0e+03 1.6e+03 90100100100 99  90100100100 99  9322       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 4.6400e-07 3.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply             1067 1.0 1.6683e+00 1.1 5.29e+08 1.0 0.0e+00 0.0e+00 0.0e+00 41 32  0  0  0  41 32  0  0  0  6342       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    14             14      4832064     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1416     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.01e-08
Average time for MPI_Barrier(): 1.1273e-05
Average time for zero size MPI_Send(): 6.31855e-06
#PETSc Option Table entries:
-ksp_type tfqmr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type sor
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.0017322 iterations 281
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8078 with 20 processors, by luciano.siqueira Wed Sep  9 17:36:42 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           4.463e+00     1.000   4.463e+00
Objects:              4.700e+01     1.000   4.700e+01
Flop:                 1.665e+09     1.001   1.664e+09  3.329e+10
Flop/sec:             3.730e+08     1.001   3.729e+08  7.458e+09
MPI Messages:         2.276e+03     2.000   2.162e+03  4.324e+04
MPI Message Lengths:  1.818e+07     2.000   7.989e+03  3.455e+08
MPI Reductions:       8.860e+02     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 4.4633e+00 100.0%  3.3288e+10 100.0%  4.324e+04 100.0%  7.989e+03      100.0%  8.790e+02  99.2%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 3.6350e-0347.4 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 2.6545e-0351.4 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             1136 1.0 1.3675e+00 1.1 5.11e+08 1.0 4.3e+04 8.0e+03 0.0e+00 29 31100100  0  29 31100100  0  7470       0      0 0.00e+00    0 0.00e+00  0
MatSOR              1137 1.0 1.9723e+00 1.2 5.64e+08 1.0 0.0e+00 0.0e+00 0.0e+00 40 34  0  0  0  40 34  0  0  0  5717       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 2.7071e-0325.9 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6507e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  1   1  0  0  0  1     0       0      0 0.00e+00    0 0.00e+00  0
VecDot               562 1.0 1.6372e-01 2.6 5.62e+07 1.0 0.0e+00 0.0e+00 5.6e+02  3  3  0  0 63   3  3  0  0 64  6865       0      0 0.00e+00    0 0.00e+00  0
VecMDot               10 1.0 5.3248e-03 1.6 5.50e+06 1.0 0.0e+00 0.0e+00 1.0e+01  0  0  0  0  1   0  0  0  0  1 20658       0      0 0.00e+00    0 0.00e+00  0
VecNorm              294 1.0 1.3070e-01 4.2 2.94e+07 1.0 0.0e+00 0.0e+00 2.9e+02  2  2  0  0 33   2  2  0  0 33  4499       0      0 0.00e+00    0 0.00e+00  0
VecScale              11 1.0 2.7012e-04 1.1 5.50e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 40723       0      0 0.00e+00    0 0.00e+00  0
VecCopy             1132 1.0 5.6301e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet              1132 1.0 3.3326e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             1124 1.0 6.8286e-02 1.1 1.12e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1  7  0  0  0   1  7  0  0  0 32920       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             1687 1.0 2.4783e-01 1.1 1.40e+08 1.0 0.0e+00 0.0e+00 0.0e+00  5  8  0  0  0   5  8  0  0  0 11334       0      0 0.00e+00    0 0.00e+00  0
VecAXPBYCZ           563 1.0 1.0356e-01 1.1 1.41e+08 1.0 0.0e+00 0.0e+00 0.0e+00  2  8  0  0  0   2  8  0  0  0 27183       0      0 0.00e+00    0 0.00e+00  0
VecWAXPY            1122 1.0 1.7031e-01 1.1 9.82e+07 1.0 0.0e+00 0.0e+00 0.0e+00  4  6  0  0  0   4  6  0  0  0 11526       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY              11 1.0 4.9859e-03 1.1 6.50e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 26074       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     1136 1.0 2.2609e-02 1.6 0.00e+00 0.0 4.3e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       1136 1.0 2.3791e-0112.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  3  0  0  0  0   3  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecNormalize          11 1.0 8.0888e-03 1.2 1.65e+06 1.0 0.0e+00 0.0e+00 1.1e+01  0  0  0  0  1   0  0  0  0  1  4080       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.9660e-06 2.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.2420e-03 3.2 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      1136 1.0 2.0287e-02 1.7 0.00e+00 0.0 4.3e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        1136 1.0 2.3650e-0113.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  3  0  0  0  0   3  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              1136 1.0 3.9593e-03 7.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            1136 1.0 7.0817e-04 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.4374e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  3  1  1  1  3   3  1  1  1  3  3305       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 4.0666e+00 1.0 1.66e+09 1.0 4.3e+04 8.0e+03 8.7e+02 91100100100 98  91100100100 99  8183       0      0 0.00e+00    0 0.00e+00  0
KSPGMRESOrthog        10 1.0 1.3340e-02 1.1 1.10e+07 1.0 0.0e+00 0.0e+00 1.0e+01  0  1  0  0  1   0  1  0  0  1 16492       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 1.4221e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  3  1  1  1  3   3  1  1  1  3  3340       0      0 0.00e+00    0 0.00e+00  0
PCApply              574 1.0 2.9281e+00 1.1 1.04e+09 1.0 2.1e+04 8.0e+03 0.0e+00 62 63 49 50  0  62 63 49 50  0  7118       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    33             33     12464896     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     3              3        33432     0.
      Preconditioner     3              3         2936     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.79e-08
Average time for MPI_Barrier(): 1.31552e-05
Average time for zero size MPI_Send(): 6.2514e-06
#PETSc Option Table entries:
-ksp_type tfqmr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.00115167 iterations 573
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8078 with 20 processors, by luciano.siqueira Wed Sep  9 17:36:46 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           2.037e+00     1.000   2.037e+00
Objects:              2.900e+01     1.000   2.900e+01
Flop:                 9.164e+08     1.001   9.163e+08  1.833e+10
Flop/sec:             4.498e+08     1.001   4.497e+08  8.994e+09
MPI Messages:         1.154e+03     2.000   1.096e+03  2.193e+04
MPI Message Lengths:  9.208e+06     2.000   7.979e+03  1.750e+08
MPI Reductions:       1.166e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 2.0374e+00 100.0%  1.8325e+10 100.0%  2.193e+04 100.0%  7.979e+03      100.0%  1.159e+03  99.4%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 5.8132e-0358.2 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 5.7503e-03110.6 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult              575 1.0 6.1171e-01 1.0 2.59e+08 1.0 2.2e+04 8.0e+03 0.0e+00 30 28100100  0  30 28100100  0  8452       0      0 0.00e+00    0 0.00e+00  0
MatSolve             574 1.0 6.6311e-01 1.0 2.56e+08 1.0 0.0e+00 0.0e+00 0.0e+00 32 28  0  0  0  32 28  0  0  0  7718       0      0 0.00e+00    0 0.00e+00  0
MatLUFactorNum         1 1.0 7.5040e-03 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1446       0      0 0.00e+00    0 0.00e+00  0
MatILUFactorSym        1 1.0 3.4275e-03 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 5.8007e-0355.9 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6863e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetRowIJ            1 1.0 2.9110e-06 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetOrdering         1 1.0 6.9650e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot               573 1.0 7.4533e-02 1.6 5.73e+07 1.0 0.0e+00 0.0e+00 5.7e+02  3  6  0  0 49   3  6  0  0 49 15376       0      0 0.00e+00    0 0.00e+00  0
VecNorm                1 1.0 9.1346e-05 1.2 1.00e+05 1.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0 21895       0      0 0.00e+00    0 0.00e+00  0
VecCopy                3 1.0 2.3101e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet               577 1.0 9.9381e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             1147 1.0 6.1332e-02 1.1 1.15e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3 13  0  0  0   3 13  0  0  0 37403       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             1144 1.0 1.4786e-01 1.0 1.14e+08 1.0 0.0e+00 0.0e+00 0.0e+00  7 12  0  0  0   7 12  0  0  0 15474       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin      575 1.0 9.0710e-03 1.4 0.00e+00 0.0 2.2e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd        575 1.0 3.5120e-02 3.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecReduceArith      1148 1.0 8.3817e-02 1.1 1.15e+08 1.0 0.0e+00 0.0e+00 0.0e+00  4 13  0  0  0   4 13  0  0  0 27393       0      0 0.00e+00    0 0.00e+00  0
VecReduceComm        574 1.0 2.9001e-02 2.5 0.00e+00 0.0 0.0e+00 0.0e+00 5.7e+02  1  0  0  0 49   1  0  0  0 50     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 1.3147e-05 2.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 2.5557e-0312.2 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin       575 1.0 8.2420e-03 1.5 0.00e+00 0.0 2.2e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd         575 1.0 3.4400e-02 3.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack               575 1.0 1.4611e-03 7.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack             575 1.0 3.7546e-04 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.2555e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 1.6630e+00 1.0 9.16e+08 1.0 2.2e+04 8.0e+03 1.1e+03 82100 99100 98  82100 99100 99 11011       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                2 1.0 2.6653e-02 1.2 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0   407       0      0 0.00e+00    0 0.00e+00  0
PCSetUpOnBlocks        1 1.0 1.1674e-02 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0   930       0      0 0.00e+00    0 0.00e+00  0
PCApply              574 1.0 6.7627e-01 1.0 2.56e+08 1.0 0.0e+00 0.0e+00 0.0e+00 33 28  0  0  0  33 28  0  0  0  7568       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              4     11791508     0.
         Vec Scatter     1              1          816     0.
              Vector    13             13      3630080     0.
           Index Set     5              5       608512     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     2              2         2832     0.
      Preconditioner     2              2         1944     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.86e-08
Average time for MPI_Barrier(): 1.12862e-05
Average time for zero size MPI_Send(): 6.2336e-06
#PETSc Option Table entries:
-ksp_type cr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type bjacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.000525242 iterations 1635
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8078 with 20 processors, by luciano.siqueira Wed Sep  9 17:36:51 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.236e+00     1.000   3.236e+00
Objects:              2.200e+01     1.000   2.200e+01
Flop:                 1.963e+09     1.002   1.962e+09  3.925e+10
Flop/sec:             6.065e+08     1.002   6.064e+08  1.213e+10
MPI Messages:         3.278e+03     2.000   3.114e+03  6.228e+04
MPI Message Lengths:  2.620e+07     2.000   7.993e+03  4.978e+08
MPI Reductions:       3.292e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.2363e+00 100.0%  3.9250e+10 100.0%  6.228e+04 100.0%  7.993e+03      100.0%  3.285e+03  99.8%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 4.0442e-0332.3 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 3.6369e-0360.5 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             1637 1.0 1.6923e+00 1.0 7.36e+08 1.0 6.2e+04 8.0e+03 0.0e+00 52 38100100  0  52 38100100  0  8698       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 3.6902e-0332.9 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7354e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot              1635 1.0 1.3559e-01 1.1 1.63e+08 1.0 0.0e+00 0.0e+00 1.6e+03  4  8  0  0 50   4  8  0  0 50 24116       0      0 0.00e+00    0 0.00e+00  0
VecNorm                1 1.0 8.5645e-05 1.1 1.00e+05 1.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0 23352       0      0 0.00e+00    0 0.00e+00  0
VecCopy                3 1.0 2.0744e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 3 1.0 2.1655e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             3271 1.0 1.5582e-01 1.1 3.27e+08 1.0 0.0e+00 0.0e+00 0.0e+00  5 17  0  0  0   5 17  0  0  0 41984       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             3268 1.0 4.1054e-01 1.0 3.27e+08 1.0 0.0e+00 0.0e+00 0.0e+00 13 17  0  0  0  13 17  0  0  0 15921       0      0 0.00e+00    0 0.00e+00  0
VecPointwiseMult    1636 1.0 2.3041e-01 1.0 8.18e+07 1.0 0.0e+00 0.0e+00 0.0e+00  7  4  0  0  0   7  4  0  0  0  7100       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     1637 1.0 1.9026e-02 1.7 0.00e+00 0.0 6.2e+04 8.0e+03 0.0e+00  1  0100100  0   1  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       1637 1.0 7.0231e-02 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecReduceArith      3272 1.0 2.1123e-01 1.0 3.27e+08 1.0 0.0e+00 0.0e+00 0.0e+00  6 17  0  0  0   6 17  0  0  0 30980       0      0 0.00e+00    0 0.00e+00  0
VecReduceComm       1636 1.0 4.9665e-02 2.2 0.00e+00 0.0 0.0e+00 0.0e+00 1.6e+03  1  0  0  0 50   1  0  0  0 50     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 1.0006e-05 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 2.7002e-0311.9 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      1637 1.0 1.6963e-02 1.7 0.00e+00 0.0 6.2e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        1637 1.0 6.8922e-02 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              1637 1.0 3.6292e-03 8.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            1637 1.0 8.9359e-04 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 1.2650e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.8520e+00 1.0 1.96e+09 1.0 6.2e+04 8.0e+03 3.3e+03 88100100100 99  88100100100100 13758       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 5.4600e-07 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply             1636 1.0 2.3304e-01 1.0 8.18e+07 1.0 0.0e+00 0.0e+00 2.0e+00  7  4  0  0  0   7  4  0  0  0  7020       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      8013484     0.
         Vec Scatter     1              1          816     0.
              Vector    12             12      4028608     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1416     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.06e-08
Average time for MPI_Barrier(): 1.38086e-05
Average time for zero size MPI_Send(): 6.3027e-06
#PETSc Option Table entries:
-ksp_type cr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type jacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.000702134 iterations 672
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8078 with 20 processors, by luciano.siqueira Wed Sep  9 17:36:56 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           2.715e+00     1.000   2.715e+00
Objects:              2.100e+01     1.000   2.100e+01
Flop:                 1.107e+09     1.001   1.107e+09  2.215e+10
Flop/sec:             4.079e+08     1.001   4.079e+08  8.157e+09
MPI Messages:         1.352e+03     2.000   1.284e+03  2.569e+04
MPI Message Lengths:  1.079e+07     2.000   7.982e+03  2.050e+08
MPI Reductions:       1.364e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 2.7151e+00 100.0%  2.2147e+10 100.0%  2.569e+04 100.0%  7.982e+03      100.0%  1.357e+03  99.5%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 3.9352e-0316.2 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 3.8084e-0379.5 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult              674 1.0 7.5655e-01 1.0 3.03e+08 1.0 2.6e+04 8.0e+03 0.0e+00 27 27100100  0  27 27100100  0  8011       0      0 0.00e+00    0 0.00e+00  0
MatSOR               673 1.0 1.0833e+00 1.0 3.34e+08 1.0 0.0e+00 0.0e+00 0.0e+00 39 30  0  0  0  39 30  0  0  0  6161       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 3.8583e-0339.0 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7581e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot               672 1.0 1.1740e-01 1.6 6.72e+07 1.0 0.0e+00 0.0e+00 6.7e+02  4  6  0  0 49   4  6  0  0 50 11448       0      0 0.00e+00    0 0.00e+00  0
VecNorm                1 1.0 9.1083e-05 1.2 1.00e+05 1.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0 21958       0      0 0.00e+00    0 0.00e+00  0
VecCopy                3 1.0 2.2163e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 3 1.0 2.2184e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             1345 1.0 1.1244e-01 1.1 1.34e+08 1.0 0.0e+00 0.0e+00 0.0e+00  4 12  0  0  0   4 12  0  0  0 23923       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             1342 1.0 1.9016e-01 1.1 1.34e+08 1.0 0.0e+00 0.0e+00 0.0e+00  7 12  0  0  0   7 12  0  0  0 14114       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin      674 1.0 1.4593e-02 1.6 0.00e+00 0.0 2.6e+04 8.0e+03 0.0e+00  1  0100100  0   1  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd        674 1.0 5.3859e-02 4.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecReduceArith      1346 1.0 1.0918e-01 1.2 1.35e+08 1.0 0.0e+00 0.0e+00 0.0e+00  4 12  0  0  0   4 12  0  0  0 24655       0      0 0.00e+00    0 0.00e+00  0
VecReduceComm        673 1.0 5.3664e-02 2.5 0.00e+00 0.0 0.0e+00 0.0e+00 6.7e+02  1  0  0  0 49   1  0  0  0 50     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.9270e-06 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.8704e-03 4.4 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin       674 1.0 1.3200e-02 1.7 0.00e+00 0.0 2.6e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd         674 1.0 5.2932e-02 4.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack               674 1.0 3.0851e-03 9.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack             674 1.0 4.3009e-04 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 1.2616e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.3373e+00 1.0 1.11e+09 1.0 2.6e+04 8.0e+03 1.3e+03 86100100100 99  86100100100 99  9470       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 2.7600e-07 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply              673 1.0 1.0847e+00 1.0 3.34e+08 1.0 0.0e+00 0.0e+00 0.0e+00 39 30  0  0  0  39 30  0  0  0  6153       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    11             11      3626880     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1416     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.09e-08
Average time for MPI_Barrier(): 1.3565e-05
Average time for zero size MPI_Send(): 6.17765e-06
#PETSc Option Table entries:
-ksp_type cr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type sor
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.000656407 iterations 374
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8078 with 20 processors, by luciano.siqueira Wed Sep  9 17:37:01 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.143e+00     1.000   3.143e+00
Objects:              4.400e+01     1.000   4.400e+01
Flop:                 1.145e+09     1.001   1.145e+09  2.290e+10
Flop/sec:             3.645e+08     1.001   3.644e+08  7.288e+09
MPI Messages:         1.526e+03     2.000   1.450e+03  2.899e+04
MPI Message Lengths:  1.218e+07     2.000   7.984e+03  2.315e+08
MPI Reductions:       7.910e+02     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.1427e+00 100.0%  2.2905e+10 100.0%  2.899e+04 100.0%  7.984e+03      100.0%  7.840e+02  99.1%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 7.0320e-0379.9 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 4.5409e-0382.2 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult              761 1.0 9.1661e-01 1.1 3.42e+08 1.0 2.9e+04 8.0e+03 0.0e+00 28 30100100  0  28 30100100  0  7465       0      0 0.00e+00    0 0.00e+00  0
MatSOR               761 1.0 1.2504e+00 1.1 3.77e+08 1.0 0.0e+00 0.0e+00 0.0e+00 38 33  0  0  0  38 33  0  0  0  6036       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 4.5973e-0341.2 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7611e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  1   1  0  0  0  1     0       0      0 0.00e+00    0 0.00e+00  0
VecDot               374 1.0 1.0556e-01 2.5 3.74e+07 1.0 0.0e+00 0.0e+00 3.7e+02  2  3  0  0 47   2  3  0  0 48  7086       0      0 0.00e+00    0 0.00e+00  0
VecMDot               10 1.0 4.8742e-03 1.4 5.50e+06 1.0 0.0e+00 0.0e+00 1.0e+01  0  0  0  0  1   0  0  0  0  1 22568       0      0 0.00e+00    0 0.00e+00  0
VecNorm               12 1.0 8.5725e-03 1.2 1.20e+06 1.0 0.0e+00 0.0e+00 1.2e+01  0  0  0  0  2   0  0  0  0  2  2800       0      0 0.00e+00    0 0.00e+00  0
VecScale              11 1.0 2.6541e-04 1.1 5.50e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 41446       0      0 0.00e+00    0 0.00e+00  0
VecCopy              754 1.0 3.5042e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet               755 1.0 2.0741e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY              750 1.0 7.4271e-02 1.1 7.50e+07 1.0 0.0e+00 0.0e+00 0.0e+00  2  7  0  0  0   2  7  0  0  0 20196       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             1496 1.0 2.1939e-01 1.1 1.31e+08 1.0 0.0e+00 0.0e+00 0.0e+00  7 11  0  0  0   7 11  0  0  0 11929       0      0 0.00e+00    0 0.00e+00  0
VecAXPBYCZ           375 1.0 7.1811e-02 1.1 9.38e+07 1.0 0.0e+00 0.0e+00 0.0e+00  2  8  0  0  0   2  8  0  0  0 26110       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY              11 1.0 5.0168e-03 1.1 6.50e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 25913       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin      761 1.0 1.8135e-02 1.6 0.00e+00 0.0 2.9e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd        761 1.0 1.2987e-01 8.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecReduceArith       750 1.0 5.9410e-02 1.2 7.50e+07 1.0 0.0e+00 0.0e+00 0.0e+00  2  7  0  0  0   2  7  0  0  0 25248       0      0 0.00e+00    0 0.00e+00  0
VecReduceComm        375 1.0 2.9112e-02 2.8 0.00e+00 0.0 0.0e+00 0.0e+00 3.8e+02  1  0  0  0 47   1  0  0  0 48     0       0      0 0.00e+00    0 0.00e+00  0
VecNormalize          11 1.0 8.7719e-03 1.2 1.65e+06 1.0 0.0e+00 0.0e+00 1.1e+01  0  0  0  0  1   0  0  0  0  1  3762       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 1.9029e-05 3.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 2.6051e-0313.1 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin       761 1.0 1.6025e-02 1.7 0.00e+00 0.0 2.9e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd         761 1.0 1.2871e-01 9.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack               761 1.0 2.9229e-03 9.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack             761 1.0 4.2345e-04 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.4729e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  5  2  1  1  3   5  2  1  1  3  3225       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.7643e+00 1.0 1.14e+09 1.0 2.9e+04 8.0e+03 7.7e+02 88100100100 98  88100100100 98  8281       0      0 0.00e+00    0 0.00e+00  0
KSPGMRESOrthog        10 1.0 1.3141e-02 1.1 1.10e+07 1.0 0.0e+00 0.0e+00 1.0e+01  0  1  0  0  1   0  1  0  0  1 16742       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 1.4633e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  5  2  1  1  3   5  2  1  1  3  3246       0      0 0.00e+00    0 0.00e+00  0
PCApply              386 1.0 1.9139e+00 1.0 6.96e+08 1.0 1.4e+04 8.0e+03 0.0e+00 60 61 49 49  0  60 61 49 49  0  7272       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    30             30     11259712     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     3              3        33432     0.
      Preconditioner     3              3         2936     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.97e-08
Average time for MPI_Barrier(): 1.11952e-05
Average time for zero size MPI_Send(): 6.3498e-06
#PETSc Option Table entries:
-ksp_type cr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.0311837 iterations 7307
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8078 with 20 processors, by luciano.siqueira Wed Sep  9 17:37:42 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.902e+01     1.000   3.902e+01
Objects:              8.400e+01     1.000   8.400e+01
Flop:                 4.268e+10     1.000   4.268e+10  8.536e+11
Flop/sec:             1.094e+09     1.000   1.094e+09  2.188e+10
MPI Messages:         1.462e+04     2.000   1.389e+04  2.778e+05
MPI Message Lengths:  1.170e+08     2.000   7.998e+03  2.222e+09
MPI Reductions:       2.170e+04     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.9021e+01 100.0%  8.5362e+11 100.0%  2.778e+05 100.0%  7.998e+03      100.0%  2.169e+04 100.0%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 2.6900e-0328.9 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 2.2774e-0341.3 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             7309 1.0 7.7291e+00 1.0 3.29e+09 1.0 2.8e+05 8.0e+03 0.0e+00 19  8100100  0  19  8100100  0  8503       0      0 0.00e+00    0 0.00e+00  0
MatSolve            7307 1.0 8.1261e+00 1.0 3.26e+09 1.0 0.0e+00 0.0e+00 0.0e+00 21  8  0  0  0  21  8  0  0  0  8017       0      0 0.00e+00    0 0.00e+00  0
MatLUFactorNum         1 1.0 7.5384e-03 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1440       0      0 0.00e+00    0 0.00e+00  0
MatILUFactorSym        1 1.0 3.4480e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 2.3316e-0321.4 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.8307e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetRowIJ            1 1.0 2.6830e-06 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetOrdering         1 1.0 6.9614e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDotNorm2         7307 1.0 1.7255e+00 1.2 1.46e+09 1.0 0.0e+00 0.0e+00 7.3e+03  4  3  0  0 34   4  3  0  0 34 16938       0      0 0.00e+00    0 0.00e+00  0
VecMDot             7063 1.0 5.6841e+00 1.0 1.06e+10 1.0 0.0e+00 0.0e+00 7.1e+03 14 25  0  0 33  14 25  0  0 33 37241       0      0 0.00e+00    0 0.00e+00  0
VecNorm             7309 1.0 6.1660e-01 1.1 7.31e+08 1.0 0.0e+00 0.0e+00 7.3e+03  1  2  0  0 34   1  2  0  0 34 23707       0      0 0.00e+00    0 0.00e+00  0
VecScale           14614 1.0 3.2515e-01 1.1 7.31e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1  2  0  0  0   1  2  0  0  0 44945       0      0 0.00e+00    0 0.00e+00  0
VecSet              7310 1.0 4.5517e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY            14615 1.0 7.0604e-01 1.1 1.46e+09 1.0 0.0e+00 0.0e+00 0.0e+00  2  3  0  0  0   2  3  0  0  0 41400       0      0 0.00e+00    0 0.00e+00  0
VecAYPX                1 1.0 1.6837e-04 1.1 5.00e+04 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  5939       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY           14126 1.0 1.3780e+01 1.0 2.12e+10 1.0 0.0e+00 0.0e+00 0.0e+00 35 50  0  0  0  35 50  0  0  0 30723       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     7309 1.0 1.3170e-01 1.8 0.00e+00 0.0 2.8e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       7309 1.0 5.4478e-01 3.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.6340e-06 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.4583e-03 3.6 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      7309 1.0 1.1423e-01 1.9 0.00e+00 0.0 2.8e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        7309 1.0 5.3438e-01 3.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              7309 1.0 2.7177e-02 7.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            7309 1.0 3.8553e-03 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.5445e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 3.8640e+01 1.0 4.27e+10 1.0 2.8e+05 8.0e+03 2.2e+04 99100100100100  99100100100100 22091       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                2 1.0 2.7229e-02 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   399       0      0 0.00e+00    0 0.00e+00  0
PCSetUpOnBlocks        1 1.0 1.1737e-02 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   925       0      0 0.00e+00    0 0.00e+00  0
PCApply             7307 1.0 8.6434e+00 1.0 3.26e+09 1.0 0.0e+00 0.0e+00 0.0e+00 22  8  0  0  0  22  8  0  0  0  7537       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              4     11791508     0.
         Vec Scatter     1              1          816     0.
              Vector    68             68     25725120     0.
           Index Set     5              5       608512     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     2              2         2896     0.
      Preconditioner     2              2         1944     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.78e-08
Average time for MPI_Barrier(): 1.136e-05
Average time for zero size MPI_Send(): 6.3211e-06
#PETSc Option Table entries:
-ksp_type gcr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type bjacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 186.34 iterations 10000
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8078 with 20 processors, by luciano.siqueira Wed Sep  9 17:38:26 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           4.215e+01     1.000   4.215e+01
Objects:              7.700e+01     1.000   7.700e+01
Flop:                 5.447e+10     1.000   5.447e+10  1.089e+12
Flop/sec:             1.292e+09     1.000   1.292e+09  2.585e+10
MPI Messages:         2.001e+04     2.000   1.901e+04  3.802e+05
MPI Message Lengths:  1.600e+08     2.000   7.999e+03  3.041e+09
MPI Reductions:       2.969e+04     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 4.2147e+01 100.0%  1.0893e+12 100.0%  3.802e+05 100.0%  7.999e+03      100.0%  2.968e+04 100.0%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 4.6144e-0346.1 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 4.5641e-0381.6 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult            10002 1.0 1.0210e+01 1.0 4.50e+09 1.0 3.8e+05 8.0e+03 0.0e+00 24  8100100  0  24  8100100  0  8809       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 4.6132e-0341.6 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7968e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDotNorm2        10000 1.0 2.0736e+00 1.0 2.00e+09 1.0 0.0e+00 0.0e+00 1.0e+04  5  4  0  0 34   5  4  0  0 34 19290       0      0 0.00e+00    0 0.00e+00  0
VecMDot             9666 1.0 7.4241e+00 1.0 1.45e+10 1.0 0.0e+00 0.0e+00 9.7e+03 17 27  0  0 33  17 27  0  0 33 39035       0      0 0.00e+00    0 0.00e+00  0
VecNorm            10002 1.0 8.1209e-01 1.1 1.00e+09 1.0 0.0e+00 0.0e+00 1.0e+04  2  2  0  0 34   2  2  0  0 34 24633       0      0 0.00e+00    0 0.00e+00  0
VecScale           20000 1.0 4.6858e-01 1.1 1.00e+09 1.0 0.0e+00 0.0e+00 0.0e+00  1  2  0  0  0   1  2  0  0  0 42682       0      0 0.00e+00    0 0.00e+00  0
VecSet                 3 1.0 2.1665e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY            20001 1.0 9.5494e-01 1.0 2.00e+09 1.0 0.0e+00 0.0e+00 0.0e+00  2  4  0  0  0   2  4  0  0  0 41889       0      0 0.00e+00    0 0.00e+00  0
VecAYPX                1 1.0 1.6622e-04 1.1 5.00e+04 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  6016       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY           19332 1.0 1.8563e+01 1.0 2.90e+10 1.0 0.0e+00 0.0e+00 0.0e+00 44 53  0  0  0  44 53  0  0  0 31223       0      0 0.00e+00    0 0.00e+00  0
VecPointwiseMult   10000 1.0 1.4970e+00 1.0 5.00e+08 1.0 0.0e+00 0.0e+00 0.0e+00  4  1  0  0  0   4  1  0  0  0  6680       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin    10002 1.0 1.4669e-01 1.5 0.00e+00 0.0 3.8e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd      10002 1.0 3.2170e-01 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 1.4647e-05 3.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.3065e-03 3.2 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin     10002 1.0 1.2989e-01 1.6 0.00e+00 0.0 3.8e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd       10002 1.0 3.0954e-01 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack             10002 1.0 3.6546e-02 8.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack           10002 1.0 6.2870e-03 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 1.4334e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 4.1766e+01 1.0 5.45e+10 1.0 3.8e+05 8.0e+03 3.0e+04 99100100100100  99100100100100 26082       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 1.2160e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply            10000 1.0 1.5118e+00 1.0 5.00e+08 1.0 0.0e+00 0.0e+00 2.0e+00  4  1  0  0  0   4  1  0  0  0  6615       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      8013484     0.
         Vec Scatter     1              1          816     0.
              Vector    67             67     26123648     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1480     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.93e-08
Average time for MPI_Barrier(): 1.07672e-05
Average time for zero size MPI_Send(): 6.16545e-06
#PETSc Option Table entries:
-ksp_type gcr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type jacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.0316741 iterations 6946
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8078 with 20 processors, by luciano.siqueira Wed Sep  9 17:39:08 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           4.010e+01     1.000   4.010e+01
Objects:              7.600e+01     1.000   7.600e+01
Flop:                 4.092e+10     1.000   4.092e+10  8.183e+11
Flop/sec:             1.020e+09     1.000   1.020e+09  2.041e+10
MPI Messages:         1.390e+04     2.000   1.320e+04  2.641e+05
MPI Message Lengths:  1.112e+08     2.000   7.998e+03  2.112e+09
MPI Reductions:       2.063e+04     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 4.0099e+01 100.0%  8.1834e+11 100.0%  2.641e+05 100.0%  7.998e+03      100.0%  2.062e+04 100.0%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 3.7383e-0333.1 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 2.9622e-0346.1 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             6948 1.0 7.1578e+00 1.0 3.13e+09 1.0 2.6e+05 8.0e+03 0.0e+00 18  8100100  0  18  8100100  0  8728       0      0 0.00e+00    0 0.00e+00  0
MatSOR              6946 1.0 1.0728e+01 1.0 3.44e+09 1.0 0.0e+00 0.0e+00 0.0e+00 27  8  0  0  0  27  8  0  0  0  6420       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 3.0191e-0325.7 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6886e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDotNorm2         6946 1.0 1.5837e+00 1.1 1.39e+09 1.0 0.0e+00 0.0e+00 6.9e+03  4  3  0  0 34   4  3  0  0 34 17544       0      0 0.00e+00    0 0.00e+00  0
VecMDot             6714 1.0 5.9038e+00 1.0 1.01e+10 1.0 0.0e+00 0.0e+00 6.7e+03 14 25  0  0 33  14 25  0  0 33 34081       0      0 0.00e+00    0 0.00e+00  0
VecNorm             6948 1.0 5.9482e-01 1.1 6.95e+08 1.0 0.0e+00 0.0e+00 6.9e+03  1  2  0  0 34   1  2  0  0 34 23361       0      0 0.00e+00    0 0.00e+00  0
VecScale           13892 1.0 3.2774e-01 1.1 6.95e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1  2  0  0  0   1  2  0  0  0 42387       0      0 0.00e+00    0 0.00e+00  0
VecSet                 3 1.0 2.1946e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY            13893 1.0 7.1566e-01 1.1 1.39e+09 1.0 0.0e+00 0.0e+00 0.0e+00  2  3  0  0  0   2  3  0  0  0 38826       0      0 0.00e+00    0 0.00e+00  0
VecAYPX                1 1.0 1.6663e-04 1.1 5.00e+04 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  6001       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY           13428 1.0 1.3076e+01 1.0 2.01e+10 1.0 0.0e+00 0.0e+00 0.0e+00 32 49  0  0  0  32 49  0  0  0 30775       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     6948 1.0 1.2301e-01 1.7 0.00e+00 0.0 2.6e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       6948 1.0 3.2799e-01 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.5880e-06 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.2954e-03 3.3 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      6948 1.0 1.0627e-01 1.8 0.00e+00 0.0 2.6e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        6948 1.0 3.1889e-01 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              6948 1.0 2.8025e-02 8.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            6948 1.0 4.0214e-03 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 1.4287e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 3.9716e+01 1.0 4.09e+10 1.0 2.6e+05 8.0e+03 2.1e+04 99100100100100  99100100100100 20604       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 7.1800e-07 3.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply             6946 1.0 1.0745e+01 1.0 3.44e+09 1.0 0.0e+00 0.0e+00 0.0e+00 27  8  0  0  0  27  8  0  0  0  6410       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    66             66     25721920     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1480     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.89e-08
Average time for MPI_Barrier(): 1.09626e-05
Average time for zero size MPI_Send(): 6.19835e-06
#PETSc Option Table entries:
-ksp_type gcr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type sor
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.0315697 iterations 3254
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8078 with 20 processors, by luciano.siqueira Wed Sep  9 17:39:40 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.020e+01     1.000   3.020e+01
Objects:              9.900e+01     1.000   9.900e+01
Flop:                 2.355e+10     1.001   2.355e+10  4.711e+11
Flop/sec:             7.799e+08     1.001   7.798e+08  1.560e+10
MPI Messages:         1.304e+04     2.000   1.239e+04  2.478e+05
MPI Message Lengths:  1.043e+08     2.000   7.998e+03  1.982e+09
MPI Reductions:       9.698e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.0202e+01 100.0%  4.7106e+11 100.0%  2.478e+05 100.0%  7.998e+03      100.0%  9.691e+03  99.9%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 6.9216e-0395.2 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 6.8364e-03141.0 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             6520 1.0 7.3683e+00 1.1 2.93e+09 1.0 2.5e+05 8.0e+03 0.0e+00 23 12100100  0  23 12100100  0  7957       0      0 0.00e+00    0 0.00e+00  0
MatSOR              6519 1.0 1.0251e+01 1.0 3.23e+09 1.0 0.0e+00 0.0e+00 0.0e+00 33 14  0  0  0  33 14  0  0  0  6306       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 6.8880e-0368.4 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7041e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDotNorm2         3254 1.0 9.0244e-01 1.3 6.51e+08 1.0 0.0e+00 0.0e+00 3.3e+03  3  3  0  0 34   3  3  0  0 34 14423       0      0 0.00e+00    0 0.00e+00  0
VecMDot             3155 1.0 2.8393e+00 1.1 4.71e+09 1.0 0.0e+00 0.0e+00 3.2e+03  9 20  0  0 33   9 20  0  0 33 33196       0      0 0.00e+00    0 0.00e+00  0
VecNorm             3267 1.0 2.9214e-01 1.1 3.27e+08 1.0 0.0e+00 0.0e+00 3.3e+03  1  1  0  0 34   1  1  0  0 34 22366       0      0 0.00e+00    0 0.00e+00  0
VecScale            6519 1.0 1.5889e-01 1.1 3.26e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1  1  0  0  0   1  1  0  0  0 41029       0      0 0.00e+00    0 0.00e+00  0
VecCopy             6509 1.0 2.9654e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet              6513 1.0 2.5635e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             6510 1.0 3.3444e-01 1.1 6.51e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1  3  0  0  0   1  3  0  0  0 38931       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             6509 1.0 9.1071e-01 1.1 4.88e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3  2  0  0  0   3  2  0  0  0 10720       0      0 0.00e+00    0 0.00e+00  0
VecAXPBYCZ          3254 1.0 5.7732e-01 1.1 8.14e+08 1.0 0.0e+00 0.0e+00 0.0e+00  2  3  0  0  0   2  3  0  0  0 28182       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY            6301 1.0 6.3187e+00 1.0 9.42e+09 1.0 0.0e+00 0.0e+00 0.0e+00 21 40  0  0  0  21 40  0  0  0 29819       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     6520 1.0 1.1690e-01 1.6 0.00e+00 0.0 2.5e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       6520 1.0 9.2995e-01 5.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecNormalize          11 1.0 7.4692e-03 1.1 1.65e+06 1.0 0.0e+00 0.0e+00 1.1e+01  0  0  0  0  0   0  0  0  0  0  4418       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.8950e-06 2.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 2.5974e-0313.5 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      6520 1.0 1.0238e-01 1.6 0.00e+00 0.0 2.5e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        6520 1.0 9.2198e-01 5.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              6520 1.0 2.3750e-02 7.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            6520 1.0 4.2648e-03 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.5743e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.5e+01  1  0  0  0  0   1  0  0  0  0  3017       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.9825e+01 1.0 2.36e+10 1.0 2.5e+05 8.0e+03 9.7e+03 99100100100100  99100100100100 15794       0      0 0.00e+00    0 0.00e+00  0
KSPGMRESOrthog        10 1.0 1.2765e-02 1.1 1.10e+07 1.0 0.0e+00 0.0e+00 1.0e+01  0  0  0  0  0   0  0  0  0  0 17234       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 1.4145e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  0  0  0  0  0   0  0  0  0  0  3358       0      0 0.00e+00    0 0.00e+00  0
PCApply             3265 1.0 1.5758e+01 1.0 6.00e+09 1.0 1.2e+05 8.0e+03 0.0e+00 52 25 50 50  0  52 25 50 50  0  7611       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    85             85     33354752     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     3              3        33496     0.
      Preconditioner     3              3         2936     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.86e-08
Average time for MPI_Barrier(): 1.10294e-05
Average time for zero size MPI_Send(): 6.50885e-06
#PETSc Option Table entries:
-ksp_type gcr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

