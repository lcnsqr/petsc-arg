sdumont8075
Norm of error 0.000101716 iterations 641
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8075 with 20 processors, by luciano.siqueira Wed Sep  9 17:31:12 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           2.145e+00     1.000   2.145e+00
Objects:              2.600e+01     1.000   2.600e+01
Flop:                 9.603e+08     1.001   9.602e+08  1.920e+10
Flop/sec:             4.476e+08     1.001   4.475e+08  8.951e+09
MPI Messages:         1.288e+03     2.000   1.224e+03  2.447e+04
MPI Message Lengths:  1.028e+07     2.000   7.981e+03  1.953e+08
MPI Reductions:       1.943e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 2.1454e+00 100.0%  1.9204e+10 100.0%  2.447e+04 100.0%  7.981e+03      100.0%  1.936e+03  99.6%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 1.8863e-0311.4 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 1.7986e-0327.6 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult              642 1.0 6.8997e-01 1.0 2.89e+08 1.0 2.4e+04 8.0e+03 0.0e+00 32 30100100  0  32 30100100  0  8367       0      0 0.00e+00    0 0.00e+00  0
MatSolve             642 1.0 7.5145e-01 1.0 2.86e+08 1.0 0.0e+00 0.0e+00 0.0e+00 34 30  0  0  0  34 30  0  0  0  7617       0      0 0.00e+00    0 0.00e+00  0
MatLUFactorNum         1 1.0 7.4056e-03 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1466       0      0 0.00e+00    0 0.00e+00  0
MatILUFactorSym        1 1.0 3.4549e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 1.8520e-0315.4 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6991e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetRowIJ            1 1.0 2.8660e-06 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetOrdering         1 1.0 7.0258e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecTDot             1282 1.0 1.1548e-01 1.1 1.28e+08 1.0 0.0e+00 0.0e+00 1.3e+03  5 13  0  0 66   5 13  0  0 66 22202       0      0 0.00e+00    0 0.00e+00  0
VecNorm              643 1.0 8.4386e-02 1.5 6.43e+07 1.0 0.0e+00 0.0e+00 6.4e+02  3  7  0  0 33   3  7  0  0 33 15239       0      0 0.00e+00    0 0.00e+00  0
VecCopy                2 1.0 1.6126e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet               645 1.0 4.9097e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             1283 1.0 5.8478e-02 1.1 1.28e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3 13  0  0  0   3 13  0  0  0 43880       0      0 0.00e+00    0 0.00e+00  0
VecAYPX              640 1.0 8.4218e-02 1.0 6.40e+07 1.0 0.0e+00 0.0e+00 0.0e+00  4  7  0  0  0   4  7  0  0  0 15199       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin      642 1.0 9.0523e-03 1.4 0.00e+00 0.0 2.4e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd        642 1.0 3.5274e-02 3.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.7640e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 5.4258e-04 4.2 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin       642 1.0 8.3177e-03 1.5 0.00e+00 0.0 2.4e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd         642 1.0 3.4637e-02 3.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack               642 1.0 1.4823e-03 6.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack             642 1.0 3.3120e-04 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 6.7131e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 1.7769e+00 1.0 9.60e+08 1.0 2.4e+04 8.0e+03 1.9e+03 83100100100 99  83100100100 99 10800       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                2 1.0 2.6842e-02 1.0 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0   404       0      0 0.00e+00    0 0.00e+00  0
PCSetUpOnBlocks        1 1.0 1.1624e-02 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0   934       0      0 0.00e+00    0 0.00e+00  0
PCApply              642 1.0 7.6037e-01 1.0 2.86e+08 1.0 0.0e+00 0.0e+00 0.0e+00 35 30  0  0  0  35 30  0  0  0  7528       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              4     11791508     0.
         Vec Scatter     1              1          816     0.
              Vector    10             10      2424896     0.
           Index Set     5              5       608512     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     2              2         2904     0.
      Preconditioner     2              2         1944     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.24e-08
Average time for MPI_Barrier(): 1.37316e-05
Average time for zero size MPI_Send(): 6.36815e-06
#PETSc Option Table entries:
-ksp_type cg
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type bjacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 4.69193e-05 iterations 1715
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8075 with 20 processors, by luciano.siqueira Wed Sep  9 17:31:17 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.217e+00     1.000   3.217e+00
Objects:              1.900e+01     1.000   1.900e+01
Flop:                 1.887e+09     1.002   1.887e+09  3.773e+10
Flop/sec:             5.865e+08     1.002   5.863e+08  1.173e+10
MPI Messages:         3.436e+03     2.000   3.264e+03  6.528e+04
MPI Message Lengths:  2.746e+07     2.000   7.993e+03  5.218e+08
MPI Reductions:       5.167e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.2174e+00 100.0%  3.7730e+10 100.0%  6.528e+04 100.0%  7.993e+03      100.0%  5.160e+03  99.9%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 7.8805e-0375.2 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 7.8527e-03157.2 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             1716 1.0 1.8262e+00 1.0 7.72e+08 1.0 6.5e+04 8.0e+03 0.0e+00 56 41100100  0  56 41100100  0  8449       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 7.9033e-0377.3 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6817e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecTDot             3430 1.0 2.8968e-01 1.1 3.43e+08 1.0 0.0e+00 0.0e+00 3.4e+03  8 18  0  0 66   8 18  0  0 66 23681       0      0 0.00e+00    0 0.00e+00  0
VecNorm             1717 1.0 1.6418e-01 1.2 1.72e+08 1.0 0.0e+00 0.0e+00 1.7e+03  5  9  0  0 33   5  9  0  0 33 20916       0      0 0.00e+00    0 0.00e+00  0
VecCopy                2 1.0 9.9536e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 3 1.0 2.1541e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             3431 1.0 1.5512e-01 1.1 3.43e+08 1.0 0.0e+00 0.0e+00 0.0e+00  5 18  0  0  0   5 18  0  0  0 44236       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             1714 1.0 2.2324e-01 1.1 1.71e+08 1.0 0.0e+00 0.0e+00 0.0e+00  7  9  0  0  0   7  9  0  0  0 15356       0      0 0.00e+00    0 0.00e+00  0
VecPointwiseMult    1716 1.0 2.4417e-01 1.1 8.58e+07 1.0 0.0e+00 0.0e+00 0.0e+00  7  5  0  0  0   7  5  0  0  0  7028       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     1716 1.0 2.0736e-02 1.8 0.00e+00 0.0 6.5e+04 8.0e+03 0.0e+00  1  0100100  0   1  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       1716 1.0 1.5484e-01 5.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  3  0  0  0  0   3  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.6260e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.4698e-03 3.5 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      1716 1.0 1.8801e-02 2.0 0.00e+00 0.0 6.5e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        1716 1.0 1.5343e-01 5.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  3  0  0  0  0   3  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              1716 1.0 3.7931e-03 6.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            1716 1.0 9.3170e-04 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 6.3922e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.8456e+00 1.0 1.89e+09 1.0 6.5e+04 8.0e+03 5.1e+03 88100100100100  88100100100100 13255       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 8.1300e-07 2.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply             1716 1.0 2.4994e-01 1.1 8.58e+07 1.0 0.0e+00 0.0e+00 2.0e+00  7  5  0  0  0   7  5  0  0  0  6866       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      8013484     0.
         Vec Scatter     1              1          816     0.
              Vector     9              9      2823424     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1488     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.97e-08
Average time for MPI_Barrier(): 1.1511e-05
Average time for zero size MPI_Send(): 6.24735e-06
#PETSc Option Table entries:
-ksp_type cg
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type jacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.00061416 iterations 692
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8075 with 20 processors, by luciano.siqueira Wed Sep  9 17:31:22 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           2.692e+00     1.000   2.692e+00
Objects:              1.800e+01     1.000   1.800e+01
Flop:                 1.071e+09     1.001   1.071e+09  2.141e+10
Flop/sec:             3.977e+08     1.001   3.977e+08  7.953e+09
MPI Messages:         1.390e+03     2.000   1.320e+03  2.641e+04
MPI Message Lengths:  1.110e+07     2.000   7.983e+03  2.108e+08
MPI Reductions:       2.096e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 2.6922e+00 100.0%  2.1412e+10 100.0%  2.641e+04 100.0%  7.983e+03      100.0%  2.089e+03  99.7%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 6.1853e-0368.6 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 6.0008e-03106.8 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult              693 1.0 7.9599e-01 1.0 3.12e+08 1.0 2.6e+04 8.0e+03 0.0e+00 29 29100100  0  29 29100100  0  7829       0      0 0.00e+00    0 0.00e+00  0
MatSOR               693 1.0 1.1430e+00 1.1 3.44e+08 1.0 0.0e+00 0.0e+00 0.0e+00 41 32  0  0  0  41 32  0  0  0  6013       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 6.0533e-0353.2 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 3.0852e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecTDot             1384 1.0 1.6152e-01 1.2 1.38e+08 1.0 0.0e+00 0.0e+00 1.4e+03  5 13  0  0 66   5 13  0  0 66 17137       0      0 0.00e+00    0 0.00e+00  0
VecNorm              694 1.0 1.7153e-01 2.4 6.94e+07 1.0 0.0e+00 0.0e+00 6.9e+02  5  6  0  0 33   5  6  0  0 33  8092       0      0 0.00e+00    0 0.00e+00  0
VecCopy                2 1.0 1.1731e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 3 1.0 2.1709e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             1385 1.0 7.5703e-02 1.1 1.38e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3 13  0  0  0   3 13  0  0  0 36590       0      0 0.00e+00    0 0.00e+00  0
VecAYPX              691 1.0 1.0011e-01 1.1 6.91e+07 1.0 0.0e+00 0.0e+00 0.0e+00  4  6  0  0  0   4  6  0  0  0 13805       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin      693 1.0 1.2639e-02 1.9 0.00e+00 0.0 2.6e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd        693 1.0 9.4207e-02 6.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 1.0010e-05 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.9772e-03 4.9 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin       693 1.0 1.1404e-02 2.0 0.00e+00 0.0 2.6e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd         693 1.0 9.3304e-02 6.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack               693 1.0 2.6357e-0310.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack             693 1.0 4.0551e-04 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 6.4449e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.3168e+00 1.0 1.07e+09 1.0 2.6e+04 8.0e+03 2.1e+03 86100100100 99  86100100100 99  9236       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 4.0500e-07 2.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply              693 1.0 1.1443e+00 1.1 3.44e+08 1.0 0.0e+00 0.0e+00 0.0e+00 41 32  0  0  0  41 32  0  0  0  6006       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector     8              8      2421696     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1488     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.23e-08
Average time for MPI_Barrier(): 1.10036e-05
Average time for zero size MPI_Send(): 6.3428e-06
#PETSc Option Table entries:
-ksp_type cg
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type sor
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.000603072 iterations 383
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8075 with 20 processors, by luciano.siqueira Wed Sep  9 17:31:27 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.125e+00     1.000   3.125e+00
Objects:              4.100e+01     1.000   4.100e+01
Flop:                 1.134e+09     1.001   1.133e+09  2.267e+10
Flop/sec:             3.628e+08     1.001   3.627e+08  7.254e+09
MPI Messages:         1.560e+03     2.000   1.482e+03  2.964e+04
MPI Message Lengths:  1.246e+07     2.000   7.985e+03  2.367e+08
MPI Reductions:       1.192e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.1248e+00 100.0%  2.2668e+10 100.0%  2.964e+04 100.0%  7.985e+03      100.0%  1.185e+03  99.4%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 3.4475e-0333.2 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 3.3990e-0368.9 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult              778 1.0 9.6106e-01 1.1 3.50e+08 1.0 3.0e+04 8.0e+03 0.0e+00 29 31100100  0  29 31100100  0  7279       0      0 0.00e+00    0 0.00e+00  0
MatSOR               779 1.0 1.2939e+00 1.1 3.86e+08 1.0 0.0e+00 0.0e+00 0.0e+00 40 34  0  0  0  40 34  0  0  0  5971       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 3.4541e-0333.9 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7021e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecMDot               10 1.0 4.0601e-03 1.2 5.50e+06 1.0 0.0e+00 0.0e+00 1.0e+01  0  0  0  0  1   0  0  0  0  1 27093       0      0 0.00e+00    0 0.00e+00  0
VecTDot              766 1.0 8.9812e-02 1.2 7.66e+07 1.0 0.0e+00 0.0e+00 7.7e+02  3  7  0  0 64   3  7  0  0 65 17058       0      0 0.00e+00    0 0.00e+00  0
VecNorm              396 1.0 1.2807e-01 2.8 3.96e+07 1.0 0.0e+00 0.0e+00 4.0e+02  3  3  0  0 33   3  3  0  0 33  6184       0      0 0.00e+00    0 0.00e+00  0
VecScale              11 1.0 2.6701e-04 1.1 5.50e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 41197       0      0 0.00e+00    0 0.00e+00  0
VecCopy              771 1.0 3.6027e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet               773 1.0 6.5175e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY              768 1.0 4.6839e-02 1.1 7.68e+07 1.0 0.0e+00 0.0e+00 0.0e+00  1  7  0  0  0   1  7  0  0  0 32793       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             1150 1.0 1.7132e-01 1.1 9.58e+07 1.0 0.0e+00 0.0e+00 0.0e+00  5  8  0  0  0   5  8  0  0  0 11184       0      0 0.00e+00    0 0.00e+00  0
VecAXPBYCZ           384 1.0 7.2681e-02 1.1 9.60e+07 1.0 0.0e+00 0.0e+00 0.0e+00  2  8  0  0  0   2  8  0  0  0 26417       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY              11 1.0 4.9914e-03 1.0 6.50e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 26045       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin      778 1.0 1.6810e-02 1.6 0.00e+00 0.0 3.0e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd        778 1.0 1.7660e-0112.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  3  0  0  0  0   3  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecNormalize          11 1.0 7.9885e-03 1.1 1.65e+06 1.0 0.0e+00 0.0e+00 1.1e+01  0  0  0  0  1   0  0  0  0  1  4131       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.8810e-06 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.4475e-03 3.5 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin       778 1.0 1.5090e-02 1.7 0.00e+00 0.0 3.0e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd         778 1.0 1.7553e-0113.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  3  0  0  0  0   3  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack               778 1.0 3.2898e-0310.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack             778 1.0 4.1938e-04 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.4133e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  5  2  1  1  2   5  2  1  1  2  3361       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.7487e+00 1.0 1.13e+09 1.0 3.0e+04 8.0e+03 1.2e+03 88100100100 98  88100100100 99  8242       0      0 0.00e+00    0 0.00e+00  0
KSPGMRESOrthog        10 1.0 1.2328e-02 1.0 1.10e+07 1.0 0.0e+00 0.0e+00 1.0e+01  0  1  0  0  1   0  1  0  0  1 17846       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 1.4093e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  4  2  1  1  2   4  2  1  1  2  3370       0      0 0.00e+00    0 0.00e+00  0
PCApply              395 1.0 1.9684e+00 1.0 7.13e+08 1.0 1.5e+04 8.0e+03 0.0e+00 62 63 49 49  0  62 63 49 49  0  7240       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    27             27     10054528     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     3              3        33504     0.
      Preconditioner     3              3         2936     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.9e-08
Average time for MPI_Barrier(): 1.13608e-05
Average time for zero size MPI_Send(): 6.3166e-06
#PETSc Option Table entries:
-ksp_type cg
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.0125906 iterations 6472
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8075 with 20 processors, by luciano.siqueira Wed Sep  9 17:31:57 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           2.853e+01     1.000   2.853e+01
Objects:              5.700e+01     1.000   5.700e+01
Flop:                 2.773e+10     1.000   2.773e+10  5.546e+11
Flop/sec:             9.718e+08     1.000   9.718e+08  1.944e+10
MPI Messages:         1.338e+04     2.000   1.271e+04  2.542e+05
MPI Message Lengths:  1.070e+08     2.000   7.998e+03  2.033e+09
MPI Reductions:       1.318e+04     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 2.8534e+01 100.0%  5.5456e+11 100.0%  2.542e+05 100.0%  7.998e+03      100.0%  1.317e+04  99.9%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 6.4883e-0377.1 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 5.0628e-03101.9 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             6688 1.0 7.2605e+00 1.0 3.01e+09 1.0 2.5e+05 8.0e+03 0.0e+00 25 11100100  0  25 11100100  0  8283       0      0 0.00e+00    0 0.00e+00  0
MatSolve            6688 1.0 7.7249e+00 1.0 2.98e+09 1.0 0.0e+00 0.0e+00 0.0e+00 27 11  0  0  0  27 11  0  0  0  7719       0      0 0.00e+00    0 0.00e+00  0
MatLUFactorNum         1 1.0 7.5354e-03 1.0 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1440       0      0 0.00e+00    0 0.00e+00  0
MatILUFactorSym        1 1.0 3.4354e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 5.1140e-0348.3 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7109e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetRowIJ            1 1.0 2.7650e-06 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetOrdering         1 1.0 7.0233e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecMDot             6472 1.0 5.3334e+00 1.1 1.00e+10 1.0 0.0e+00 0.0e+00 6.5e+03 18 36  0  0 49  18 36  0  0 49 37584       0      0 0.00e+00    0 0.00e+00  0
VecNorm             6689 1.0 8.8668e-01 1.3 6.69e+08 1.0 0.0e+00 0.0e+00 6.7e+03  3  2  0  0 51   3  2  0  0 51 15088       0      0 0.00e+00    0 0.00e+00  0
VecScale            6688 1.0 1.4036e-01 1.1 3.34e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 47649       0      0 0.00e+00    0 0.00e+00  0
VecCopy              216 1.0 1.8847e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet              6907 1.0 4.2379e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY              432 1.0 2.0841e-02 1.0 4.32e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 41457       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY            6688 1.0 6.7074e+00 1.0 1.07e+10 1.0 0.0e+00 0.0e+00 0.0e+00 23 38  0  0  0  23 38  0  0  0 31816       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     6688 1.0 8.9673e-02 1.5 0.00e+00 0.0 2.5e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       6688 1.0 4.7179e-01 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecNormalize        6688 1.0 1.0339e+00 1.2 1.00e+09 1.0 0.0e+00 0.0e+00 6.7e+03  3  4  0  0 51   3  4  0  0 51 19405       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.9020e-06 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.5465e-03 3.9 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      6688 1.0 7.8832e-02 1.5 0.00e+00 0.0 2.5e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        6688 1.0 4.6263e-01 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              6688 1.0 2.1162e-02 6.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            6688 1.0 3.6125e-03 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.0878e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.8162e+01 1.0 2.77e+10 1.0 2.5e+05 8.0e+03 1.3e+04 99100100100100  99100100100100 19691       0      0 0.00e+00    0 0.00e+00  0
KSPGMRESOrthog      6472 1.0 1.1471e+01 1.0 2.00e+10 1.0 0.0e+00 0.0e+00 6.5e+03 40 72  0  0 49  40 72  0  0 49 34949       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                2 1.0 1.8264e-02 1.2 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   594       0      0 0.00e+00    0 0.00e+00  0
PCSetUpOnBlocks        1 1.0 1.1718e-02 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   926       0      0 0.00e+00    0 0.00e+00  0
PCApply             6688 1.0 8.1944e+00 1.0 2.98e+09 1.0 0.0e+00 0.0e+00 0.0e+00 28 11  0  0  0  28 11  0  0  0  7277       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              4     11791508     0.
         Vec Scatter     1              1          816     0.
              Vector    41             41     14878464     0.
           Index Set     5              5       608512     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     2              2        20072     0.
      Preconditioner     2              2         1944     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.11e-08
Average time for MPI_Barrier(): 1.50518e-05
Average time for zero size MPI_Send(): 6.1403e-06
#PETSc Option Table entries:
-ksp_type gmres
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type bjacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 186.34 iterations 10000
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8075 with 20 processors, by luciano.siqueira Wed Sep  9 17:32:33 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.316e+01     1.000   3.316e+01
Objects:              5.000e+01     1.000   5.000e+01
Flop:                 3.876e+10     1.001   3.876e+10  7.752e+11
Flop/sec:             1.169e+09     1.001   1.169e+09  2.337e+10
MPI Messages:         2.067e+04     2.000   1.964e+04  3.928e+05
MPI Message Lengths:  1.654e+08     2.000   7.999e+03  3.142e+09
MPI Reductions:       2.036e+04     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.3164e+01 100.0%  7.7519e+11 100.0%  3.928e+05 100.0%  7.999e+03      100.0%  2.035e+04 100.0%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 3.7628e-0323.9 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 3.7364e-0363.4 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult            10334 1.0 1.1114e+01 1.0 4.65e+09 1.0 3.9e+05 8.0e+03 0.0e+00 33 12100100  0  33 12100100  0  8361       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 3.7884e-0333.2 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7121e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecMDot            10000 1.0 8.0338e+00 1.0 1.55e+10 1.0 0.0e+00 0.0e+00 1.0e+04 24 40  0  0 49  24 40  0  0 49 38562       0      0 0.00e+00    0 0.00e+00  0
VecNorm            10335 1.0 1.1034e+00 1.1 1.03e+09 1.0 0.0e+00 0.0e+00 1.0e+04  3  3  0  0 51   3  3  0  0 51 18732       0      0 0.00e+00    0 0.00e+00  0
VecScale           10334 1.0 2.1550e-01 1.0 5.17e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1  1  0  0  0   1  1  0  0  0 47953       0      0 0.00e+00    0 0.00e+00  0
VecCopy              334 1.0 2.9039e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet               337 1.0 2.1201e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY              668 1.0 3.2554e-02 1.0 6.68e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 41040       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY           10334 1.0 1.0801e+01 1.0 1.65e+10 1.0 0.0e+00 0.0e+00 0.0e+00 32 43  0  0  0  32 43  0  0  0 30535       0      0 0.00e+00    0 0.00e+00  0
VecPointwiseMult   10334 1.0 1.6292e+00 1.0 5.17e+08 1.0 0.0e+00 0.0e+00 0.0e+00  5  1  0  0  0   5  1  0  0  0  6343       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin    10334 1.0 1.3507e-01 1.6 0.00e+00 0.0 3.9e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd      10334 1.0 4.7586e-01 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecNormalize       10334 1.0 1.3250e+00 1.1 1.55e+09 1.0 0.0e+00 0.0e+00 1.0e+04  4  4  0  0 51   4  4  0  0 51 23397       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.7830e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.2721e-03 3.3 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin     10334 1.0 1.2066e-01 1.7 0.00e+00 0.0 3.9e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd       10334 1.0 4.6244e-01 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack             10334 1.0 3.3097e-02 8.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack           10334 1.0 5.9955e-03 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 1.0583e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 3.2792e+01 1.0 3.88e+10 1.0 3.9e+05 8.0e+03 2.0e+04 99100100100100  99100100100100 23639       0      0 0.00e+00    0 0.00e+00  0
KSPGMRESOrthog     10000 1.0 1.8203e+01 1.0 3.10e+10 1.0 0.0e+00 0.0e+00 1.0e+04 54 80  0  0 49  54 80  0  0 49 34039       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 6.0400e-07 2.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply            10334 1.0 1.6430e+00 1.0 5.17e+08 1.0 0.0e+00 0.0e+00 2.0e+00  5  1  0  0  0   5  1  0  0  0  6290       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      8013484     0.
         Vec Scatter     1              1          816     0.
              Vector    40             40     15276992     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1        18656     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.34e-08
Average time for MPI_Barrier(): 1.12566e-05
Average time for zero size MPI_Send(): 6.45895e-06
#PETSc Option Table entries:
-ksp_type gmres
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type jacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.0487126 iterations 10000
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8075 with 20 processors, by luciano.siqueira Wed Sep  9 17:33:24 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           4.901e+01     1.000   4.901e+01
Objects:              4.900e+01     1.000   4.900e+01
Flop:                 4.337e+10     1.000   4.337e+10  8.673e+11
Flop/sec:             8.849e+08     1.000   8.848e+08  1.770e+10
MPI Messages:         2.067e+04     2.000   1.964e+04  3.928e+05
MPI Message Lengths:  1.654e+08     2.000   7.999e+03  3.142e+09
MPI Reductions:       2.035e+04     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 4.9012e+01 100.0%  8.6733e+11 100.0%  3.928e+05 100.0%  7.999e+03      100.0%  2.035e+04 100.0%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 4.1537e-0345.0 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 4.0739e-0373.8 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult            10334 1.0 1.1093e+01 1.0 4.65e+09 1.0 3.9e+05 8.0e+03 0.0e+00 23 11100100  0  23 11100100  0  8377       0      0 0.00e+00    0 0.00e+00  0
MatSOR             10334 1.0 1.6398e+01 1.0 5.12e+09 1.0 0.0e+00 0.0e+00 0.0e+00 33 12  0  0  0  33 12  0  0  0  6249       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 4.1258e-0337.5 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6262e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecMDot            10000 1.0 9.2390e+00 1.1 1.55e+10 1.0 0.0e+00 0.0e+00 1.0e+04 19 36  0  0 49  19 36  0  0 49 33532       0      0 0.00e+00    0 0.00e+00  0
VecNorm            10335 1.0 1.2014e+00 1.1 1.03e+09 1.0 0.0e+00 0.0e+00 1.0e+04  2  2  0  0 51   2  2  0  0 51 17205       0      0 0.00e+00    0 0.00e+00  0
VecScale           10334 1.0 2.1704e-01 1.0 5.17e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 47613       0      0 0.00e+00    0 0.00e+00  0
VecCopy              334 1.0 2.8890e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet               337 1.0 2.1119e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY              668 1.0 3.2731e-02 1.1 6.68e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 40818       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY           10334 1.0 1.0946e+01 1.0 1.65e+10 1.0 0.0e+00 0.0e+00 0.0e+00 22 38  0  0  0  22 38  0  0  0 30131       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin    10334 1.0 1.4509e-01 1.6 0.00e+00 0.0 3.9e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd      10334 1.0 5.4243e-01 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecNormalize       10334 1.0 1.4291e+00 1.1 1.55e+09 1.0 0.0e+00 0.0e+00 1.0e+04  3  4  0  0 51   3  4  0  0 51 21694       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.9910e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 2.5180e-0312.8 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin     10334 1.0 1.2688e-01 1.7 0.00e+00 0.0 3.9e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd       10334 1.0 5.2844e-01 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack             10334 1.0 3.7055e-02 9.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack           10334 1.0 6.5262e-03 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 1.0523e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 4.8651e+01 1.0 4.34e+10 1.0 3.9e+05 8.0e+03 2.0e+04 99100100100100  99100100100100 17827       0      0 0.00e+00    0 0.00e+00  0
KSPGMRESOrthog     10000 1.0 1.9547e+01 1.0 3.10e+10 1.0 0.0e+00 0.0e+00 1.0e+04 39 71  0  0 49  39 71  0  0 49 31698       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 4.1900e-07 2.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply            10334 1.0 1.6416e+01 1.0 5.12e+09 1.0 0.0e+00 0.0e+00 0.0e+00 33 12  0  0  0  33 12  0  0  0  6242       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    39             39     14875264     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1        18656     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.3e-08
Average time for MPI_Barrier(): 1.6507e-05
Average time for zero size MPI_Send(): 6.41055e-06
#PETSc Option Table entries:
-ksp_type gmres
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type sor
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.00984153 iterations 3132
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8075 with 20 processors, by luciano.siqueira Wed Sep  9 17:33:53 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           2.681e+01     1.000   2.681e+01
Objects:              7.200e+01     1.000   7.200e+01
Flop:                 1.795e+10     1.001   1.795e+10  3.589e+11
Flop/sec:             6.694e+08     1.001   6.694e+08  1.339e+10
MPI Messages:         1.297e+04     2.000   1.232e+04  2.465e+05
MPI Message Lengths:  1.038e+08     2.000   7.998e+03  1.971e+09
MPI Reductions:       6.411e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 2.6810e+01 100.0%  3.5893e+11 100.0%  2.465e+05 100.0%  7.998e+03      100.0%  6.404e+03  99.9%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 3.9233e-0345.7 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 3.4852e-0363.3 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             6484 1.0 7.4836e+00 1.1 2.92e+09 1.0 2.5e+05 8.0e+03 0.0e+00 27 16100100  0  27 16100100  0  7791       0      0 0.00e+00    0 0.00e+00  0
MatSOR              6485 1.0 1.0488e+01 1.1 3.22e+09 1.0 0.0e+00 0.0e+00 0.0e+00 38 18  0  0  0  38 18  0  0  0  6132       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 3.5396e-0333.4 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6873e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecMDot             3142 1.0 3.0939e+00 1.1 4.85e+09 1.0 0.0e+00 0.0e+00 3.1e+03 11 27  0  0 49  11 27  0  0 49 31347       0      0 0.00e+00    0 0.00e+00  0
VecNorm             3249 1.0 4.9687e-01 1.6 3.25e+08 1.0 0.0e+00 0.0e+00 3.2e+03  2  2  0  0 51   2  2  0  0 51 13078       0      0 0.00e+00    0 0.00e+00  0
VecScale            3248 1.0 7.3306e-02 1.1 1.62e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 44308       0      0 0.00e+00    0 0.00e+00  0
VecCopy             6580 1.0 3.4981e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet              6584 1.0 2.5991e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY              211 1.0 1.0465e-02 1.1 2.11e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 40324       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             6474 1.0 9.4817e-01 1.1 4.86e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3  3  0  0  0   3  3  0  0  0 10242       0      0 0.00e+00    0 0.00e+00  0
VecAXPBYCZ          3237 1.0 5.9281e-01 1.1 8.09e+08 1.0 0.0e+00 0.0e+00 0.0e+00  2  5  0  0  0   2  5  0  0  0 27302       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY            3248 1.0 3.3666e+00 1.1 5.16e+09 1.0 0.0e+00 0.0e+00 0.0e+00 12 29  0  0  0  12 29  0  0  0 30675       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     6484 1.0 1.1697e-01 1.7 0.00e+00 0.0 2.5e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       6484 1.0 9.1980e-01 4.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecNormalize        3248 1.0 5.6909e-01 1.5 4.87e+08 1.0 0.0e+00 0.0e+00 3.2e+03  2  3  0  0 51   2  3  0  0 51 17122       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.9110e-06 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.1601e-03 2.9 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      6484 1.0 1.0307e-01 1.8 0.00e+00 0.0 2.5e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        6484 1.0 9.1099e-01 4.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              6484 1.0 2.3501e-02 8.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            6484 1.0 4.0717e-03 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.4062e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  1  0  0  0  0   1  0  0  0  0  3378       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.6441e+01 1.0 1.79e+10 1.0 2.5e+05 8.0e+03 6.4e+03 99100100100100  99100100100100 13574       0      0 0.00e+00    0 0.00e+00  0
KSPGMRESOrthog      3142 1.0 6.1072e+00 1.0 9.70e+09 1.0 0.0e+00 0.0e+00 3.1e+03 22 54  0  0 49  22 54  0  0 49 31761       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 1.3984e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  1  0  0  0  0   1  0  0  0  0  3397       0      0 0.00e+00    0 0.00e+00  0
PCApply             3248 1.0 1.6239e+01 1.0 5.97e+09 1.0 1.2e+05 8.0e+03 0.0e+00 60 33 50 50  0  60 33 50 50  0  7347       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    58             58     22508096     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     3              3        50672     0.
      Preconditioner     3              3         2936     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.84e-08
Average time for MPI_Barrier(): 1.23438e-05
Average time for zero size MPI_Send(): 7.48145e-06
#PETSc Option Table entries:
-ksp_type gmres
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.000101716 iterations 641
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8075 with 20 processors, by luciano.siqueira Wed Sep  9 17:33:58 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.380e+00     1.000   3.380e+00
Objects:              8.700e+01     1.000   8.700e+01
Flop:                 2.892e+09     1.000   2.892e+09  5.784e+10
Flop/sec:             8.558e+08     1.000   8.557e+08  1.711e+10
MPI Messages:         1.288e+03     2.000   1.224e+03  2.447e+04
MPI Message Lengths:  1.028e+07     2.000   7.981e+03  1.953e+08
MPI Reductions:       2.583e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.3798e+00 100.0%  5.7844e+10 100.0%  2.447e+04 100.0%  7.981e+03      100.0%  2.576e+03  99.7%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 5.7323e-0345.9 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 4.5729e-0374.9 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult              642 1.0 7.0377e-01 1.0 2.89e+08 1.0 2.4e+04 8.0e+03 0.0e+00 21 10100100  0  21 10100100  0  8203       0      0 0.00e+00    0 0.00e+00  0
MatSolve             642 1.0 7.6412e-01 1.1 2.86e+08 1.0 0.0e+00 0.0e+00 0.0e+00 22 10  0  0  0  22 10  0  0  0  7491       0      0 0.00e+00    0 0.00e+00  0
MatLUFactorNum         1 1.0 7.5299e-03 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1441       0      0 0.00e+00    0 0.00e+00  0
MatILUFactorSym        1 1.0 3.4476e-03 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 4.6258e-0341.1 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6629e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetRowIJ            1 1.0 2.8170e-06 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetOrdering         1 1.0 7.0126e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecTDot             1282 1.0 1.7159e-01 1.5 1.28e+08 1.0 0.0e+00 0.0e+00 1.3e+03  4  4  0  0 50   4  4  0  0 50 14942       0      0 0.00e+00    0 0.00e+00  0
VecMTDot             640 1.0 5.4457e-01 1.0 9.82e+08 1.0 0.0e+00 0.0e+00 6.4e+02 16 34  0  0 25  16 34  0  0 25 36065       0      0 0.00e+00    0 0.00e+00  0
VecNorm              643 1.0 1.3338e-01 2.4 6.43e+07 1.0 0.0e+00 0.0e+00 6.4e+02  3  2  0  0 25   3  2  0  0 25  9641       0      0 0.00e+00    0 0.00e+00  0
VecScale             640 1.0 2.3570e-02 1.1 3.20e+07 1.0 0.0e+00 0.0e+00 0.0e+00  1  1  0  0  0   1  1  0  0  0 27153       0      0 0.00e+00    0 0.00e+00  0
VecCopy              642 1.0 3.9652e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet               645 1.0 1.4249e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             1283 1.0 6.8089e-02 1.0 1.28e+08 1.0 0.0e+00 0.0e+00 0.0e+00  2  4  0  0  0   2  4  0  0  0 37686       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY             640 1.0 6.4648e-01 1.1 9.82e+08 1.0 0.0e+00 0.0e+00 0.0e+00 18 34  0  0  0  18 34  0  0  0 30380       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin      642 1.0 1.2304e-02 1.6 0.00e+00 0.0 2.4e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd        642 1.0 6.7217e-02 6.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.7120e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.5204e-03 3.8 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin       642 1.0 1.0901e-02 1.6 0.00e+00 0.0 2.4e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd         642 1.0 6.6503e-02 6.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack               642 1.0 2.1533e-03 9.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack             642 1.0 7.4598e-04 3.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 4.5937e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 3.0171e+00 1.0 2.89e+09 1.0 2.4e+04 8.0e+03 2.6e+03 89100100100 99  89100100100100 19167       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                2 1.0 2.5034e-02 1.5 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0   434       0      0 0.00e+00    0 0.00e+00  0
PCSetUpOnBlocks        1 1.0 1.1733e-02 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   925       0      0 0.00e+00    0 0.00e+00  0
PCApply              642 1.0 7.8377e-01 1.1 2.86e+08 1.0 0.0e+00 0.0e+00 0.0e+00 22 10  0  0  0  22 10  0  0  0  7303       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              4     11791508     0.
         Vec Scatter     1              1          816     0.
              Vector    71             71     26930304     0.
           Index Set     5              5       608512     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     2              2         4080     0.
      Preconditioner     2              2         1944     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.98e-08
Average time for MPI_Barrier(): 1.06392e-05
Average time for zero size MPI_Send(): 6.59025e-06
#PETSc Option Table entries:
-ksp_type fcg
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type bjacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 4.69193e-05 iterations 1715
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8075 with 20 processors, by luciano.siqueira Wed Sep  9 17:34:07 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           6.601e+00     1.000   6.601e+00
Objects:              8.000e+01     1.000   8.000e+01
Flop:                 7.104e+09     1.000   7.104e+09  1.421e+11
Flop/sec:             1.076e+09     1.000   1.076e+09  2.152e+10
MPI Messages:         3.436e+03     2.000   3.264e+03  6.528e+04
MPI Message Lengths:  2.746e+07     2.000   7.993e+03  5.218e+08
MPI Reductions:       6.881e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 6.6010e+00 100.0%  1.4208e+11 100.0%  6.528e+04 100.0%  7.993e+03      100.0%  6.874e+03  99.9%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 3.3395e-0338.1 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 3.3074e-0362.1 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             1716 1.0 1.8850e+00 1.0 7.72e+08 1.0 6.5e+04 8.0e+03 0.0e+00 28 11100100  0  28 11100100  0  8186       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 3.3600e-0332.6 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6988e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecTDot             3430 1.0 3.9973e-01 1.3 3.43e+08 1.0 0.0e+00 0.0e+00 3.4e+03  5  5  0  0 50   5  5  0  0 50 17162       0      0 0.00e+00    0 0.00e+00  0
VecMTDot            1714 1.0 1.4218e+00 1.0 2.65e+09 1.0 0.0e+00 0.0e+00 1.7e+03 21 37  0  0 25  21 37  0  0 25 37299       0      0 0.00e+00    0 0.00e+00  0
VecNorm             1717 1.0 1.6474e-01 1.2 1.72e+08 1.0 0.0e+00 0.0e+00 1.7e+03  2  2  0  0 25   2  2  0  0 25 20845       0      0 0.00e+00    0 0.00e+00  0
VecScale            1714 1.0 5.2088e-02 1.1 8.57e+07 1.0 0.0e+00 0.0e+00 0.0e+00  1  1  0  0  0   1  1  0  0  0 32906       0      0 0.00e+00    0 0.00e+00  0
VecCopy             1716 1.0 1.1121e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 3 1.0 2.1003e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             3431 1.0 1.8774e-01 1.0 3.43e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3  5  0  0  0   3  5  0  0  0 36550       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY            1714 1.0 1.8301e+00 1.0 2.65e+09 1.0 0.0e+00 0.0e+00 0.0e+00 27 37  0  0  0  27 37  0  0  0 28976       0      0 0.00e+00    0 0.00e+00  0
VecPointwiseMult    1716 1.0 2.6032e-01 1.1 8.58e+07 1.0 0.0e+00 0.0e+00 0.0e+00  4  1  0  0  0   4  1  0  0  0  6592       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     1716 1.0 2.3751e-02 1.5 0.00e+00 0.0 6.5e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       1716 1.0 1.1944e-01 2.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 1.2870e-05 2.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.3812e-03 3.5 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      1716 1.0 2.0986e-02 1.6 0.00e+00 0.0 6.5e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        1716 1.0 1.1746e-01 2.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              1716 1.0 5.3799e-03 9.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            1716 1.0 9.2730e-04 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 4.5904e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 6.2365e+00 1.0 7.10e+09 1.0 6.5e+04 8.0e+03 6.9e+03 94100100100100  94100100100100 22779       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 1.2900e-06 3.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply             1716 1.0 2.6483e-01 1.1 8.58e+07 1.0 0.0e+00 0.0e+00 2.0e+00  4  1  0  0  0   4  1  0  0  0  6480       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      8013484     0.
         Vec Scatter     1              1          816     0.
              Vector    70             70     27328832     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         2664     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.96e-08
Average time for MPI_Barrier(): 1.13756e-05
Average time for zero size MPI_Send(): 6.5866e-06
#PETSc Option Table entries:
-ksp_type fcg
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type jacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.000614161 iterations 692
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8075 with 20 processors, by luciano.siqueira Wed Sep  9 17:34:13 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           4.081e+00     1.000   4.081e+00
Objects:              7.900e+01     1.000   7.900e+01
Flop:                 3.175e+09     1.000   3.175e+09  6.351e+10
Flop/sec:             7.780e+08     1.000   7.780e+08  1.556e+10
MPI Messages:         1.390e+03     2.000   1.320e+03  2.641e+04
MPI Message Lengths:  1.110e+07     2.000   7.983e+03  2.108e+08
MPI Reductions:       2.787e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 4.0812e+00 100.0%  6.3505e+10 100.0%  2.641e+04 100.0%  7.983e+03      100.0%  2.780e+03  99.7%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 4.0777e-0345.4 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 4.0043e-0373.1 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult              693 1.0 7.5420e-01 1.0 3.12e+08 1.0 2.6e+04 8.0e+03 0.0e+00 18 10100100  0  18 10100100  0  8262       0      0 0.00e+00    0 0.00e+00  0
MatSOR               693 1.0 1.1077e+00 1.0 3.44e+08 1.0 0.0e+00 0.0e+00 0.0e+00 27 11  0  0  0  27 11  0  0  0  6205       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 4.0553e-0337.6 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6822e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecTDot             1384 1.0 1.6896e-01 1.1 1.38e+08 1.0 0.0e+00 0.0e+00 1.4e+03  4  4  0  0 50   4  4  0  0 50 16382       0      0 0.00e+00    0 0.00e+00  0
VecMTDot             691 1.0 6.9986e-01 1.0 1.07e+09 1.0 0.0e+00 0.0e+00 6.9e+02 17 34  0  0 25  17 34  0  0 25 30566       0      0 0.00e+00    0 0.00e+00  0
VecNorm              694 1.0 1.0406e-01 1.2 6.94e+07 1.0 0.0e+00 0.0e+00 6.9e+02  2  2  0  0 25   2  2  0  0 25 13338       0      0 0.00e+00    0 0.00e+00  0
VecScale             691 1.0 3.1962e-02 1.0 3.46e+07 1.0 0.0e+00 0.0e+00 0.0e+00  1  1  0  0  0   1  1  0  0  0 21619       0      0 0.00e+00    0 0.00e+00  0
VecCopy              693 1.0 4.4313e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 3 1.0 2.1937e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             1385 1.0 8.2897e-02 1.0 1.38e+08 1.0 0.0e+00 0.0e+00 0.0e+00  2  4  0  0  0   2  4  0  0  0 33415       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY             691 1.0 7.3539e-01 1.0 1.07e+09 1.0 0.0e+00 0.0e+00 0.0e+00 18 34  0  0  0  18 34  0  0  0 29089       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin      693 1.0 1.4151e-02 1.7 0.00e+00 0.0 2.6e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd        693 1.0 3.1256e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.6300e-06 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.2917e-03 3.4 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin       693 1.0 1.2525e-02 1.8 0.00e+00 0.0 2.6e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd         693 1.0 3.0241e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack               693 1.0 2.7514e-0311.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack             693 1.0 4.3580e-04 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 4.5649e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 3.7112e+00 1.0 3.17e+09 1.0 2.6e+04 8.0e+03 2.8e+03 91100100100 99  91100100100100 17108       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 6.8200e-07 4.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply              693 1.0 1.1095e+00 1.0 3.44e+08 1.0 0.0e+00 0.0e+00 0.0e+00 27 11  0  0  0  27 11  0  0  0  6195       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    69             69     26927104     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         2664     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.39e-08
Average time for MPI_Barrier(): 1.17546e-05
Average time for zero size MPI_Send(): 6.77905e-06
#PETSc Option Table entries:
-ksp_type fcg
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type sor
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.000603072 iterations 383
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8075 with 20 processors, by luciano.siqueira Wed Sep  9 17:34:19 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.824e+00     1.000   3.824e+00
Objects:              1.020e+02     1.000   1.020e+02
Flop:                 2.281e+09     1.001   2.281e+09  4.562e+10
Flop/sec:             5.966e+08     1.001   5.965e+08  1.193e+10
MPI Messages:         1.560e+03     2.000   1.482e+03  2.964e+04
MPI Message Lengths:  1.246e+07     2.000   7.985e+03  2.367e+08
MPI Reductions:       1.574e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.8236e+00 100.0%  4.5618e+10 100.0%  2.964e+04 100.0%  7.985e+03      100.0%  1.567e+03  99.6%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 5.2533e-0360.2 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 5.1700e-0393.9 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult              778 1.0 8.8386e-01 1.1 3.50e+08 1.0 3.0e+04 8.0e+03 0.0e+00 23 15100100  0  23 15100100  0  7915       0      0 0.00e+00    0 0.00e+00  0
MatSOR               779 1.0 1.2469e+00 1.0 3.86e+08 1.0 0.0e+00 0.0e+00 0.0e+00 32 17  0  0  0  32 17  0  0  0  6196       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 5.2254e-0347.3 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6622e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecMDot               10 1.0 4.6446e-03 1.4 5.50e+06 1.0 0.0e+00 0.0e+00 1.0e+01  0  0  0  0  1   0  0  0  0  1 23683       0      0 0.00e+00    0 0.00e+00  0
VecTDot              766 1.0 1.0187e-01 1.3 7.66e+07 1.0 0.0e+00 0.0e+00 7.7e+02  2  3  0  0 49   2  3  0  0 49 15039       0      0 0.00e+00    0 0.00e+00  0
VecMTDot             382 1.0 3.8511e-01 1.0 5.83e+08 1.0 0.0e+00 0.0e+00 3.8e+02 10 26  0  0 24  10 26  0  0 24 30292       0      0 0.00e+00    0 0.00e+00  0
VecNorm              396 1.0 9.3252e-02 1.9 3.96e+07 1.0 0.0e+00 0.0e+00 4.0e+02  2  2  0  0 25   2  2  0  0 25  8493       0      0 0.00e+00    0 0.00e+00  0
VecScale             393 1.0 2.1816e-02 1.1 1.96e+07 1.0 0.0e+00 0.0e+00 0.0e+00  1  1  0  0  0   1  1  0  0  0 18015       0      0 0.00e+00    0 0.00e+00  0
VecCopy             1153 1.0 6.3527e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet               773 1.0 1.8402e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY              768 1.0 4.7269e-02 1.0 7.68e+07 1.0 0.0e+00 0.0e+00 0.0e+00  1  3  0  0  0   1  3  0  0  0 32495       0      0 0.00e+00    0 0.00e+00  0
VecAYPX              768 1.0 1.1067e-01 1.1 5.76e+07 1.0 0.0e+00 0.0e+00 0.0e+00  3  3  0  0  0   3  3  0  0  0 10409       0      0 0.00e+00    0 0.00e+00  0
VecAXPBYCZ           384 1.0 6.9780e-02 1.1 9.60e+07 1.0 0.0e+00 0.0e+00 0.0e+00  2  4  0  0  0   2  4  0  0  0 27515       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY             393 1.0 3.8817e-01 1.0 5.90e+08 1.0 0.0e+00 0.0e+00 0.0e+00 10 26  0  0  0  10 26  0  0  0 30388       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin      778 1.0 1.6175e-02 1.6 0.00e+00 0.0 3.0e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd        778 1.0 9.0565e-02 3.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecNormalize          11 1.0 7.6717e-03 1.1 1.65e+06 1.0 0.0e+00 0.0e+00 1.1e+01  0  0  0  0  1   0  0  0  0  1  4302       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.7080e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 2.3025e-03 6.0 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin       778 1.0 1.4261e-02 1.6 0.00e+00 0.0 3.0e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd         778 1.0 8.9537e-02 3.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack               778 1.0 2.7153e-03 6.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack             778 1.0 4.9845e-04 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.4504e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  4  1  1  1  1   4  1  1  1  1  3275       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 3.4559e+00 1.0 2.28e+09 1.0 3.0e+04 8.0e+03 1.6e+03 90100100100 99  90100100100 99 13196       0      0 0.00e+00    0 0.00e+00  0
KSPGMRESOrthog        10 1.0 1.2714e-02 1.1 1.10e+07 1.0 0.0e+00 0.0e+00 1.0e+01  0  0  0  0  1   0  0  0  0  1 17303       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 1.4088e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  4  1  1  1  1   4  1  1  1  1  3372       0      0 0.00e+00    0 0.00e+00  0
PCApply              395 1.0 1.9132e+00 1.0 7.13e+08 1.0 1.5e+04 8.0e+03 0.0e+00 49 31 49 49  0  49 31 49 49  0  7449       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    88             88     34559936     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     3              3        34680     0.
      Preconditioner     3              3         2936     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.87e-08
Average time for MPI_Barrier(): 1.37388e-05
Average time for zero size MPI_Send(): 6.42275e-06
#PETSc Option Table entries:
-ksp_type fcg
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 8.47251e-05 iterations 1671
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8075 with 20 processors, by luciano.siqueira Wed Sep  9 17:34:35 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           1.437e+01     1.000   1.437e+01
Objects:              4.000e+01     1.000   4.000e+01
Flop:                 7.332e+09     1.001   7.331e+09  1.466e+11
Flop/sec:             5.102e+08     1.001   5.101e+08  1.020e+10
MPI Messages:         1.003e+04     2.000   9.530e+03  1.906e+05
MPI Message Lengths:  8.023e+07     2.000   7.998e+03  1.524e+09
MPI Reductions:       8.375e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 1.4370e+01 100.0%  1.4662e+11 100.0%  1.906e+05 100.0%  7.998e+03      100.0%  8.368e+03  99.9%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 1.7858e-0314.9 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 1.7197e-0331.9 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             5014 1.0 5.2932e+00 1.0 2.26e+09 1.0 1.9e+05 8.0e+03 0.0e+00 37 31100100  0  37 31100100  0  8518       0      0 0.00e+00    0 0.00e+00  0
MatSolve            5014 1.0 5.6415e+00 1.0 2.24e+09 1.0 0.0e+00 0.0e+00 0.0e+00 39 30  0  0  0  39 30  0  0  0  7924       0      0 0.00e+00    0 0.00e+00  0
MatLUFactorNum         1 1.0 7.5881e-03 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1430       0      0 0.00e+00    0 0.00e+00  0
MatILUFactorSym        1 1.0 3.4290e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 1.7737e-0316.2 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6516e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetRowIJ            1 1.0 7.6640e-06 3.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetOrdering         1 1.0 7.0188e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot              5013 1.0 5.3656e-01 1.1 5.01e+08 1.0 0.0e+00 0.0e+00 5.0e+03  3  7  0  0 60   3  7  0  0 60 18686       0      0 0.00e+00    0 0.00e+00  0
VecNorm             3344 1.0 4.2920e-01 1.4 3.34e+08 1.0 0.0e+00 0.0e+00 3.3e+03  3  5  0  0 40   3  5  0  0 40 15583       0      0 0.00e+00    0 0.00e+00  0
VecScale            6685 1.0 1.4585e-01 1.1 3.34e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1  5  0  0  0   1  5  0  0  0 45834       0      0 0.00e+00    0 0.00e+00  0
VecCopy            15043 1.0 6.9979e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  5  0  0  0  0   5  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet              5022 1.0 3.0629e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY            15037 1.0 8.2986e-01 1.0 1.50e+09 1.0 0.0e+00 0.0e+00 0.0e+00  6 21  0  0  0   6 21  0  0  0 36240       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             1671 1.0 2.2793e-01 1.0 1.67e+08 1.0 0.0e+00 0.0e+00 0.0e+00  2  2  0  0  0   2  2  0  0  0 14663       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     5014 1.0 6.9160e-02 1.4 0.00e+00 0.0 1.9e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       5014 1.0 2.5122e-01 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.6030e-06 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.2800e-03 3.2 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      5014 1.0 6.1906e-02 1.5 0.00e+00 0.0 1.9e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        5014 1.0 2.4625e-01 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              5014 1.0 1.8414e-02 8.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            5014 1.0 2.5671e-03 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 3.5437e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 1.3998e+01 1.0 7.33e+09 1.0 1.9e+05 8.0e+03 8.4e+03 97100100100100  97100100100100 10473       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                2 1.0 2.6693e-02 1.2 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   407       0      0 0.00e+00    0 0.00e+00  0
PCSetUpOnBlocks        1 1.0 1.1728e-02 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   925       0      0 0.00e+00    0 0.00e+00  0
PCApply             5014 1.0 5.9845e+00 1.0 2.24e+09 1.0 0.0e+00 0.0e+00 0.0e+00 41 30  0  0  0  41 30  0  0  0  7470       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              4     11791508     0.
         Vec Scatter     1              1          816     0.
              Vector    24             24      8049088     0.
           Index Set     5              5       608512     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     2              2         2832     0.
      Preconditioner     2              2         1944     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.96e-08
Average time for MPI_Barrier(): 1.2297e-05
Average time for zero size MPI_Send(): 6.3057e-06
#PETSc Option Table entries:
-ksp_type tcqmr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type bjacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 2.62209e-05 iterations 6622
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8075 with 20 processors, by luciano.siqueira Wed Sep  9 17:35:12 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.511e+01     1.000   3.511e+01
Objects:              3.300e+01     1.000   3.300e+01
Flop:                 2.119e+10     1.002   2.118e+10  4.237e+11
Flop/sec:             6.034e+08     1.002   6.033e+08  1.207e+10
MPI Messages:         3.974e+04     2.000   3.775e+04  7.550e+05
MPI Message Lengths:  3.179e+08     2.000   7.999e+03  6.040e+09
MPI Reductions:       3.313e+04     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.5115e+01 100.0%  4.2366e+11 100.0%  7.550e+05 100.0%  7.999e+03      100.0%  3.312e+04 100.0%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 3.8939e-0333.5 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 3.6862e-0361.6 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult            19867 1.0 2.1360e+01 1.0 8.94e+09 1.0 7.5e+05 8.0e+03 0.0e+00 60 42100100  0  60 42100100  0  8364       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 3.7404e-0332.3 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6235e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot             19866 1.0 1.8726e+00 1.1 1.99e+09 1.0 0.0e+00 0.0e+00 2.0e+04  5  9  0  0 60   5  9  0  0 60 21217       0      0 0.00e+00    0 0.00e+00  0
VecNorm            13246 1.0 1.3551e+00 1.2 1.32e+09 1.0 0.0e+00 0.0e+00 1.3e+04  3  6  0  0 40   3  6  0  0 40 19549       0      0 0.00e+00    0 0.00e+00  0
VecScale           26489 1.0 5.3056e-01 1.1 1.32e+09 1.0 0.0e+00 0.0e+00 0.0e+00  1  6  0  0  0   1  6  0  0  0 49926       0      0 0.00e+00    0 0.00e+00  0
VecCopy            59602 1.0 2.7589e+00 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  8  0  0  0  0   8  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 8 1.0 5.2920e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY            59596 1.0 3.3292e+00 1.0 5.96e+09 1.0 0.0e+00 0.0e+00 0.0e+00  9 28  0  0  0   9 28  0  0  0 35802       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             6622 1.0 9.1775e-01 1.0 6.62e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3  3  0  0  0   3  3  0  0  0 14431       0      0 0.00e+00    0 0.00e+00  0
VecPointwiseMult   19867 1.0 3.0110e+00 1.0 9.93e+08 1.0 0.0e+00 0.0e+00 0.0e+00  8  5  0  0  0   8  5  0  0  0  6598       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin    19867 1.0 2.5273e-01 1.5 0.00e+00 0.0 7.5e+05 8.0e+03 0.0e+00  1  0100100  0   1  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd      19867 1.0 1.0878e+00 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 1.4873e-05 3.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.1947e-03 3.0 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin     19867 1.0 2.2481e-01 1.6 0.00e+00 0.0 7.5e+05 8.0e+03 0.0e+00  1  0100100  0   1  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd       19867 1.0 1.0663e+00 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack             19867 1.0 6.6900e-02 9.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack           19867 1.0 9.9474e-03 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 3.5311e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 3.4751e+01 1.0 2.12e+10 1.0 7.5e+05 8.0e+03 3.3e+04 99100100100100  99100100100100 12191       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 1.0970e-06 2.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply            19867 1.0 3.0325e+00 1.0 9.93e+08 1.0 0.0e+00 0.0e+00 2.0e+00  9  5  0  0  0   9  5  0  0  0  6551       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      8013484     0.
         Vec Scatter     1              1          816     0.
              Vector    23             23      8447616     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1416     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.89e-08
Average time for MPI_Barrier(): 1.11188e-05
Average time for zero size MPI_Send(): 6.44115e-06
#PETSc Option Table entries:
-ksp_type tcqmr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type jacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.000177372 iterations 1033
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8075 with 20 processors, by luciano.siqueira Wed Sep  9 17:35:25 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           1.042e+01     1.000   1.042e+01
Objects:              3.200e+01     1.000   3.200e+01
Flop:                 4.688e+09     1.001   4.687e+09  9.374e+10
Flop/sec:             4.499e+08     1.001   4.499e+08  8.997e+09
MPI Messages:         6.204e+03     2.000   5.894e+03  1.179e+05
MPI Message Lengths:  4.961e+07     2.000   7.996e+03  9.426e+08
MPI Reductions:       5.185e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 1.0419e+01 100.0%  9.3739e+10 100.0%  1.179e+05 100.0%  7.996e+03      100.0%  5.178e+03  99.9%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 6.1674e-0387.1 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 6.1403e-03122.2 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             3100 1.0 3.3655e+00 1.0 1.39e+09 1.0 1.2e+05 8.0e+03 0.0e+00 32 30100100  0  32 30100100  0  8283       0      0 0.00e+00    0 0.00e+00  0
MatSOR              3100 1.0 4.8102e+00 1.0 1.54e+09 1.0 0.0e+00 0.0e+00 0.0e+00 46 33  0  0  0  46 33  0  0  0  6391       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 6.1967e-0360.9 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7111e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot              3099 1.0 4.1186e-01 1.2 3.10e+08 1.0 0.0e+00 0.0e+00 3.1e+03  4  7  0  0 60   4  7  0  0 60 15049       0      0 0.00e+00    0 0.00e+00  0
VecNorm             2068 1.0 3.5973e-01 1.4 2.07e+08 1.0 0.0e+00 0.0e+00 2.1e+03  3  4  0  0 40   3  4  0  0 40 11498       0      0 0.00e+00    0 0.00e+00  0
VecScale            4133 1.0 9.2281e-02 1.1 2.07e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1  4  0  0  0   1  4  0  0  0 44787       0      0 0.00e+00    0 0.00e+00  0
VecCopy             9301 1.0 4.8600e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  5  0  0  0  0   5  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 8 1.0 5.3039e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             9295 1.0 5.4610e-01 1.0 9.30e+08 1.0 0.0e+00 0.0e+00 0.0e+00  5 20  0  0  0   5 20  0  0  0 34042       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             1033 1.0 1.4767e-01 1.0 1.03e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1  2  0  0  0   1  2  0  0  0 13991       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     3100 1.0 6.1687e-02 1.8 0.00e+00 0.0 1.2e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       3100 1.0 1.9520e-01 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.7720e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.1926e-03 3.1 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      3100 1.0 5.5242e-02 2.0 0.00e+00 0.0 1.2e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        3100 1.0 1.9077e-01 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              3100 1.0 1.4666e-0211.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            3100 1.0 1.8971e-03 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 3.5591e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 1.0047e+01 1.0 4.69e+09 1.0 1.2e+05 8.0e+03 5.2e+03 96100100100100  96100100100100  9328       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 3.9400e-07 2.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply             3100 1.0 4.8163e+00 1.0 1.54e+09 1.0 0.0e+00 0.0e+00 0.0e+00 46 33  0  0  0  46 33  0  0  0  6383       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    22             22      8045888     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1416     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.57e-08
Average time for MPI_Barrier(): 1.02432e-05
Average time for zero size MPI_Send(): 6.4594e-06
#PETSc Option Table entries:
-ksp_type tcqmr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type sor
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.000147819 iterations 474
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8075 with 20 processors, by luciano.siqueira Wed Sep  9 17:35:36 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           1.006e+01     1.000   1.006e+01
Objects:              5.500e+01     1.000   5.500e+01
Flop:                 4.090e+09     1.001   4.089e+09  8.179e+10
Flop/sec:             4.066e+08     1.001   4.065e+08  8.130e+09
MPI Messages:         5.716e+03     2.000   5.430e+03  1.086e+05
MPI Message Lengths:  4.570e+07     2.000   7.996e+03  8.684e+08
MPI Reductions:       2.413e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 1.0060e+01 100.0%  8.1788e+10 100.0%  1.086e+05 100.0%  7.996e+03      100.0%  2.406e+03  99.7%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 3.5119e-0329.7 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 3.4831e-0358.6 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             2856 1.0 3.2187e+00 1.0 1.28e+09 1.0 1.1e+05 8.0e+03 0.0e+00 32 31100100  0  32 31100100  0  7979       0      0 0.00e+00    0 0.00e+00  0
MatSOR              2857 1.0 4.5203e+00 1.0 1.42e+09 1.0 0.0e+00 0.0e+00 0.0e+00 45 35  0  0  0  45 35  0  0  0  6268       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 3.5367e-0330.8 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6601e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot              1422 1.0 2.0947e-01 1.3 1.42e+08 1.0 0.0e+00 0.0e+00 1.4e+03  2  3  0  0 59   2  3  0  0 59 13577       0      0 0.00e+00    0 0.00e+00  0
VecMDot               10 1.0 4.6297e-03 1.4 5.50e+06 1.0 0.0e+00 0.0e+00 1.0e+01  0  0  0  0  0   0  0  0  0  0 23759       0      0 0.00e+00    0 0.00e+00  0
VecNorm              961 1.0 2.0707e-01 1.5 9.61e+07 1.0 0.0e+00 0.0e+00 9.6e+02  2  2  0  0 40   2  2  0  0 40  9282       0      0 0.00e+00    0 0.00e+00  0
VecScale            1908 1.0 4.4276e-02 1.1 9.54e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  2  0  0  0   0  2  0  0  0 43093       0      0 0.00e+00    0 0.00e+00  0
VecCopy             7117 1.0 3.8244e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  4  0  0  0  0   4  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet              2856 1.0 1.1251e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             4265 1.0 2.4747e-01 1.0 4.26e+08 1.0 0.0e+00 0.0e+00 0.0e+00  2 10  0  0  0   2 10  0  0  0 34469       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             3320 1.0 4.7948e-01 1.0 2.61e+08 1.0 0.0e+00 0.0e+00 0.0e+00  5  6  0  0  0   5  6  0  0  0 10881       0      0 0.00e+00    0 0.00e+00  0
VecAXPBYCZ          1423 1.0 2.5781e-01 1.0 3.56e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3  9  0  0  0   3  9  0  0  0 27598       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY              11 1.0 5.0240e-03 1.1 6.50e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 25876       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     2856 1.0 5.3977e-02 1.7 0.00e+00 0.0 1.1e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       2856 1.0 2.2504e-01 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecNormalize          11 1.0 7.3600e-03 1.2 1.65e+06 1.0 0.0e+00 0.0e+00 1.1e+01  0  0  0  0  0   0  0  0  0  0  4484       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.5930e-06 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.3145e-03 3.5 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      2856 1.0 4.7280e-02 1.8 0.00e+00 0.0 1.1e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        2856 1.0 2.2140e-01 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              2856 1.0 1.1551e-02 9.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            2856 1.0 1.5644e-03 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.4289e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  1  1  0  0  1   1  1  0  0  1  3324       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 9.6838e+00 1.0 4.09e+09 1.0 1.1e+05 8.0e+03 2.4e+03 96100100100 99  96100100100100  8445       0      0 0.00e+00    0 0.00e+00  0
KSPGMRESOrthog        10 1.0 1.2555e-02 1.1 1.10e+07 1.0 0.0e+00 0.0e+00 1.0e+01  0  0  0  0  0   0  0  0  0  0 17524       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 1.3973e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  1  1  0  0  1   1  1  0  0  1  3399       0      0 0.00e+00    0 0.00e+00  0
PCApply             1434 1.0 7.0730e+00 1.0 2.63e+09 1.0 5.4e+04 8.0e+03 0.0e+00 70 64 50 50  0  70 64 50 50  0  7424       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    41             41     15678720     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     3              3        33432     0.
      Preconditioner     3              3         2936     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.03e-08
Average time for MPI_Barrier(): 1.20806e-05
Average time for zero size MPI_Send(): 6.40745e-06
#PETSc Option Table entries:
-ksp_type tcqmr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 7.08235e-05 iterations 490
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8075 with 20 processors, by luciano.siqueira Wed Sep  9 17:35:41 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.173e+00     1.000   3.173e+00
Objects:              3.000e+01     1.000   3.000e+01
Flop:                 1.345e+09     1.001   1.345e+09  2.689e+10
Flop/sec:             4.238e+08     1.001   4.237e+08  8.474e+09
MPI Messages:         1.966e+03     2.000   1.868e+03  3.735e+04
MPI Message Lengths:  1.570e+07     2.000   7.988e+03  2.984e+08
MPI Reductions:       1.491e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.1733e+00 100.0%  2.6891e+10 100.0%  3.735e+04 100.0%  7.988e+03      100.0%  1.484e+03  99.5%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 3.5166e-0338.9 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 2.2629e-0343.0 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult              981 1.0 1.0785e+00 1.0 4.41e+08 1.0 3.7e+04 8.0e+03 0.0e+00 34 33100100  0  34 33100100  0  8179       0      0 0.00e+00    0 0.00e+00  0
MatSolve             981 1.0 1.1524e+00 1.0 4.37e+08 1.0 0.0e+00 0.0e+00 0.0e+00 36 33  0  0  0  36 33  0  0  0  7590       0      0 0.00e+00    0 0.00e+00  0
MatLUFactorNum         1 1.0 7.5332e-03 1.0 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1441       0      0 0.00e+00    0 0.00e+00  0
MatILUFactorSym        1 1.0 3.4336e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 2.3147e-0322.1 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6731e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetRowIJ            1 1.0 3.1390e-06 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetOrdering         1 1.0 7.0195e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot               981 1.0 1.5833e-01 1.2 9.81e+07 1.0 0.0e+00 0.0e+00 9.8e+02  4  7  0  0 66   4  7  0  0 66 12392       0      0 0.00e+00    0 0.00e+00  0
VecNorm              492 1.0 5.0780e-02 1.2 4.92e+07 1.0 0.0e+00 0.0e+00 4.9e+02  1  4  0  0 33   1  4  0  0 33 19378       0      0 0.00e+00    0 0.00e+00  0
VecCopy                4 1.0 2.9674e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet               984 1.0 1.9013e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             1470 1.0 8.6712e-02 1.1 1.47e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3 11  0  0  0   3 11  0  0  0 33905       0      0 0.00e+00    0 0.00e+00  0
VecWAXPY            1958 1.0 2.6771e-01 1.0 1.71e+08 1.0 0.0e+00 0.0e+00 0.0e+00  8 13  0  0  0   8 13  0  0  0 12797       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin      981 1.0 1.5141e-02 1.5 0.00e+00 0.0 3.7e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd        981 1.0 5.1426e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.2470e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.3768e-03 3.7 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin       981 1.0 1.3685e-02 1.6 0.00e+00 0.0 3.7e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd         981 1.0 5.0238e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack               981 1.0 2.7703e-03 7.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack             981 1.0 5.8302e-04 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.4811e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.8075e+00 1.0 1.34e+09 1.0 3.7e+04 8.0e+03 1.5e+03 88100100100 99  88100100100 99  9573       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                2 1.0 2.5701e-02 1.2 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0   422       0      0 0.00e+00    0 0.00e+00  0
PCSetUpOnBlocks        1 1.0 1.1731e-02 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   925       0      0 0.00e+00    0 0.00e+00  0
PCApply              981 1.0 1.1763e+00 1.0 4.37e+08 1.0 0.0e+00 0.0e+00 0.0e+00 37 33  0  0  0  37 33  0  0  0  7436       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              4     11791508     0.
         Vec Scatter     1              1          816     0.
              Vector    14             14      4031808     0.
           Index Set     5              5       608512     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     2              2         2832     0.
      Preconditioner     2              2         1944     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.62e-08
Average time for MPI_Barrier(): 1.21134e-05
Average time for zero size MPI_Send(): 6.7342e-06
#PETSc Option Table entries:
-ksp_type cgs
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type bjacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 8.26951e-08 iterations 1553
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8075 with 20 processors, by luciano.siqueira Wed Sep  9 17:35:50 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           5.525e+00     1.000   5.525e+00
Objects:              2.300e+01     1.000   2.300e+01
Flop:                 3.028e+09     1.002   3.028e+09  6.055e+10
Flop/sec:             5.482e+08     1.002   5.480e+08  1.096e+10
MPI Messages:         6.218e+03     2.000   5.907e+03  1.181e+05
MPI Message Lengths:  4.972e+07     2.000   7.996e+03  9.447e+08
MPI Reductions:       4.682e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 5.5246e+00 100.0%  6.0554e+10 100.0%  1.181e+05 100.0%  7.996e+03      100.0%  4.675e+03  99.9%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 2.9140e-0316.4 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 2.5221e-0341.8 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             3107 1.0 3.2508e+00 1.0 1.40e+09 1.0 1.2e+05 8.0e+03 0.0e+00 58 46100100  0  58 46100100  0  8594       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 2.5746e-0322.3 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6983e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot              3107 1.0 3.5439e-01 1.3 3.11e+08 1.0 0.0e+00 0.0e+00 3.1e+03  6 10  0  0 66   6 10  0  0 66 17534       0      0 0.00e+00    0 0.00e+00  0
VecNorm             1555 1.0 1.2342e-01 1.0 1.56e+08 1.0 0.0e+00 0.0e+00 1.6e+03  2  5  0  0 33   2  5  0  0 33 25198       0      0 0.00e+00    0 0.00e+00  0
VecCopy                4 1.0 2.3325e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 3 1.0 2.1720e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             4659 1.0 2.5709e-01 1.1 4.66e+08 1.0 0.0e+00 0.0e+00 0.0e+00  5 15  0  0  0   5 15  0  0  0 36245       0      0 0.00e+00    0 0.00e+00  0
VecWAXPY            6210 1.0 8.2278e-01 1.0 5.43e+08 1.0 0.0e+00 0.0e+00 0.0e+00 15 18  0  0  0  15 18  0  0  0 13208       0      0 0.00e+00    0 0.00e+00  0
VecPointwiseMult    3107 1.0 4.3479e-01 1.0 1.55e+08 1.0 0.0e+00 0.0e+00 0.0e+00  8  5  0  0  0   8  5  0  0  0  7146       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     3107 1.0 3.4617e-02 1.5 0.00e+00 0.0 1.2e+05 8.0e+03 0.0e+00  1  0100100  0   1  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       3107 1.0 1.6339e-01 2.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.7290e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.3935e-03 3.6 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      3107 1.0 3.0984e-02 1.6 0.00e+00 0.0 1.2e+05 8.0e+03 0.0e+00  1  0100100  0   1  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        3107 1.0 1.6009e-01 2.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              3107 1.0 7.3971e-03 6.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            3107 1.0 1.4400e-03 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 1.4547e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 5.1614e+00 1.0 3.03e+09 1.0 1.2e+05 8.0e+03 4.7e+03 93100100100100  93100100100100 11730       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 9.4000e-07 3.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply             3107 1.0 4.3885e-01 1.0 1.55e+08 1.0 0.0e+00 0.0e+00 2.0e+00  8  5  0  0  0   8  5  0  0  0  7080       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      8013484     0.
         Vec Scatter     1              1          816     0.
              Vector    13             13      4430336     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1416     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.25e-08
Average time for MPI_Barrier(): 1.20956e-05
Average time for zero size MPI_Send(): 6.6518e-06
#PETSc Option Table entries:
-ksp_type cgs
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type jacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 8.535e-05 iterations 546
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8075 with 20 processors, by luciano.siqueira Wed Sep  9 17:35:56 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.971e+00     1.000   3.971e+00
Objects:              2.200e+01     1.000   2.200e+01
Flop:                 1.552e+09     1.001   1.552e+09  3.104e+10
Flop/sec:             3.910e+08     1.001   3.909e+08  7.818e+09
MPI Messages:         2.190e+03     2.000   2.080e+03  4.161e+04
MPI Message Lengths:  1.750e+07     2.000   7.989e+03  3.324e+08
MPI Reductions:       1.659e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.9706e+00 100.0%  3.1043e+10 100.0%  4.161e+04 100.0%  7.989e+03      100.0%  1.652e+03  99.6%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 2.8849e-0341.8 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 2.8664e-0359.2 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             1093 1.0 1.2140e+00 1.0 4.92e+08 1.0 4.2e+04 8.0e+03 0.0e+00 30 32100100  0  30 32100100  0  8096       0      0 0.00e+00    0 0.00e+00  0
MatSOR              1093 1.0 1.7675e+00 1.0 5.42e+08 1.0 0.0e+00 0.0e+00 0.0e+00 43 35  0  0  0  43 35  0  0  0  6132       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 2.9207e-0329.7 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6217e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot              1093 1.0 2.4294e-01 1.6 1.09e+08 1.0 0.0e+00 0.0e+00 1.1e+03  5  7  0  0 66   5  7  0  0 66  8998       0      0 0.00e+00    0 0.00e+00  0
VecNorm              548 1.0 5.7198e-02 1.1 5.48e+07 1.0 0.0e+00 0.0e+00 5.5e+02  1  4  0  0 33   1  4  0  0 33 19162       0      0 0.00e+00    0 0.00e+00  0
VecCopy                4 1.0 2.5431e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 3 1.0 2.1830e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             1638 1.0 1.0672e-01 1.1 1.64e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3 11  0  0  0   3 11  0  0  0 30697       0      0 0.00e+00    0 0.00e+00  0
VecWAXPY            2182 1.0 3.1792e-01 1.0 1.91e+08 1.0 0.0e+00 0.0e+00 0.0e+00  8 12  0  0  0   8 12  0  0  0 12009       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     1093 1.0 2.1876e-02 1.7 0.00e+00 0.0 4.2e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       1093 1.0 7.8617e-02 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.6270e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.3922e-03 3.6 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      1093 1.0 1.9633e-02 1.8 0.00e+00 0.0 4.2e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        1093 1.0 7.7100e-02 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              1093 1.0 4.2066e-0310.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            1093 1.0 6.3118e-04 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 1.4685e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 3.6142e+00 1.0 1.55e+09 1.0 4.1e+04 8.0e+03 1.6e+03 91100100100 99  91100100100 99  8586       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 2.9300e-07 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply             1093 1.0 1.7698e+00 1.0 5.42e+08 1.0 0.0e+00 0.0e+00 0.0e+00 43 35  0  0  0  43 35  0  0  0  6124       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    12             12      4028608     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1416     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.79e-08
Average time for MPI_Barrier(): 1.46408e-05
Average time for zero size MPI_Send(): 6.6327e-06
#PETSc Option Table entries:
-ksp_type cgs
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type sor
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.000193207 iterations 288
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8075 with 20 processors, by luciano.siqueira Wed Sep  9 17:36:03 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           4.384e+00     1.000   4.384e+00
Objects:              4.500e+01     1.000   4.500e+01
Flop:                 1.619e+09     1.001   1.619e+09  3.238e+10
Flop/sec:             3.694e+08     1.001   3.694e+08  7.388e+09
MPI Messages:         2.332e+03     2.000   2.215e+03  4.431e+04
MPI Message Lengths:  1.863e+07     2.000   7.990e+03  3.540e+08
MPI Reductions:       9.080e+02     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 4.3837e+00 100.0%  3.2385e+10 100.0%  4.431e+04 100.0%  7.990e+03      100.0%  9.010e+02  99.2%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 4.5919e-0343.7 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 4.5286e-0383.2 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             1164 1.0 1.3402e+00 1.0 5.24e+08 1.0 4.4e+04 8.0e+03 0.0e+00 30 32100100  0  30 32100100  0  7810       0      0 0.00e+00    0 0.00e+00  0
MatSOR              1165 1.0 1.8633e+00 1.0 5.78e+08 1.0 0.0e+00 0.0e+00 0.0e+00 42 36  0  0  0  42 36  0  0  0  6200       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 4.5786e-0341.9 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.5928e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  1   1  0  0  0  1     0       0      0 0.00e+00    0 0.00e+00  0
VecDot               577 1.0 1.3900e-01 1.4 5.77e+07 1.0 0.0e+00 0.0e+00 5.8e+02  3  4  0  0 64   3  4  0  0 64  8302       0      0 0.00e+00    0 0.00e+00  0
VecMDot               10 1.0 4.7098e-03 1.4 5.50e+06 1.0 0.0e+00 0.0e+00 1.0e+01  0  0  0  0  1   0  0  0  0  1 23355       0      0 0.00e+00    0 0.00e+00  0
VecNorm              301 1.0 3.3724e-02 1.0 3.01e+07 1.0 0.0e+00 0.0e+00 3.0e+02  1  2  0  0 33   1  2  0  0 33 17851       0      0 0.00e+00    0 0.00e+00  0
VecScale              11 1.0 2.7000e-04 1.2 5.50e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 40741       0      0 0.00e+00    0 0.00e+00  0
VecCopy             1159 1.0 5.6656e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet              1159 1.0 3.3011e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY              865 1.0 6.0905e-02 1.1 8.65e+07 1.0 0.0e+00 0.0e+00 0.0e+00  1  5  0  0  0   1  5  0  0  0 28405       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             1154 1.0 1.6872e-01 1.0 8.66e+07 1.0 0.0e+00 0.0e+00 0.0e+00  4  5  0  0  0   4  5  0  0  0 10259       0      0 0.00e+00    0 0.00e+00  0
VecAXPBYCZ           577 1.0 1.0427e-01 1.0 1.44e+08 1.0 0.0e+00 0.0e+00 0.0e+00  2  9  0  0  0   2  9  0  0  0 27668       0      0 0.00e+00    0 0.00e+00  0
VecWAXPY            1150 1.0 1.7437e-01 1.0 1.01e+08 1.0 0.0e+00 0.0e+00 0.0e+00  4  6  0  0  0   4  6  0  0  0 11539       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY              11 1.0 5.0370e-03 1.1 6.50e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 25809       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     1164 1.0 2.4292e-02 1.8 0.00e+00 0.0 4.4e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       1164 1.0 1.2681e-01 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecNormalize          11 1.0 7.8035e-03 1.2 1.65e+06 1.0 0.0e+00 0.0e+00 1.1e+01  0  0  0  0  1   0  0  0  0  1  4229       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.7370e-06 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.9843e-03 4.9 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      1164 1.0 2.1590e-02 1.8 0.00e+00 0.0 4.4e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        1164 1.0 1.2528e-01 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              1164 1.0 4.4023e-0310.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            1164 1.0 6.9098e-04 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.3831e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  3  1  1  1  3   3  1  1  1  3  3434       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 4.0140e+00 1.0 1.62e+09 1.0 4.4e+04 8.0e+03 8.9e+02 92100100100 98  92100100100 99  8065       0      0 0.00e+00    0 0.00e+00  0
KSPGMRESOrthog        10 1.0 1.2496e-02 1.1 1.10e+07 1.0 0.0e+00 0.0e+00 1.0e+01  0  1  0  0  1   0  1  0  0  1 17606       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 1.3714e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  3  1  1  1  3   3  1  1  1  3  3464       0      0 0.00e+00    0 0.00e+00  0
PCApply              588 1.0 2.8885e+00 1.0 1.07e+09 1.0 2.2e+04 8.0e+03 0.0e+00 66 66 49 50  0  66 66 49 50  0  7394       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    31             31     11661440     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     3              3        33432     0.
      Preconditioner     3              3         2936     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.47e-08
Average time for MPI_Barrier(): 1.23604e-05
Average time for zero size MPI_Send(): 6.50715e-06
#PETSc Option Table entries:
-ksp_type cgs
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.00114039 iterations 479
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8075 with 20 processors, by luciano.siqueira Wed Sep  9 17:36:08 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.053e+00     1.000   3.053e+00
Objects:              2.900e+01     1.000   2.900e+01
Flop:                 1.387e+09     1.001   1.386e+09  2.773e+10
Flop/sec:             4.541e+08     1.001   4.541e+08  9.081e+09
MPI Messages:         1.922e+03     2.000   1.826e+03  3.652e+04
MPI Message Lengths:  1.535e+07     2.000   7.988e+03  2.917e+08
MPI Reductions:       1.936e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.0533e+00 100.0%  2.7729e+10 100.0%  3.652e+04 100.0%  7.988e+03      100.0%  1.929e+03  99.6%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 3.4504e-0327.9 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 3.4205e-0356.8 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult              959 1.0 1.0123e+00 1.0 4.31e+08 1.0 3.6e+04 8.0e+03 0.0e+00 33 31100100  0  33 31100100  0  8519       0      0 0.00e+00    0 0.00e+00  0
MatSolve             959 1.0 1.1030e+00 1.0 4.28e+08 1.0 0.0e+00 0.0e+00 0.0e+00 36 31  0  0  0  36 31  0  0  0  7752       0      0 0.00e+00    0 0.00e+00  0
MatLUFactorNum         1 1.0 7.5054e-03 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1446       0      0 0.00e+00    0 0.00e+00  0
MatILUFactorSym        1 1.0 3.3570e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 3.4744e-0330.3 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7036e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetRowIJ            1 1.0 2.7700e-06 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetOrdering         1 1.0 6.9410e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot               958 1.0 1.2242e-01 1.2 9.58e+07 1.0 0.0e+00 0.0e+00 9.6e+02  4  7  0  0 49   4  7  0  0 50 15651       0      0 0.00e+00    0 0.00e+00  0
VecDotNorm2          479 1.0 1.1703e-01 1.2 9.58e+07 1.0 0.0e+00 0.0e+00 4.8e+02  3  7  0  0 25   3  7  0  0 25 16371       0      0 0.00e+00    0 0.00e+00  0
VecNorm              481 1.0 5.0492e-02 1.2 4.81e+07 1.0 0.0e+00 0.0e+00 4.8e+02  1  3  0  0 25   1  3  0  0 25 19052       0      0 0.00e+00    0 0.00e+00  0
VecCopy                2 1.0 1.5655e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet               964 1.0 2.2137e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY                1 1.0 8.7348e-05 1.1 1.00e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 22897       0      0 0.00e+00    0 0.00e+00  0
VecAXPBYCZ           958 1.0 1.6313e-01 1.1 1.92e+08 1.0 0.0e+00 0.0e+00 0.0e+00  5 14  0  0  0   5 14  0  0  0 23491       0      0 0.00e+00    0 0.00e+00  0
VecWAXPY             958 1.0 1.2642e-01 1.0 9.58e+07 1.0 0.0e+00 0.0e+00 0.0e+00  4  7  0  0  0   4  7  0  0  0 15156       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin      959 1.0 1.3166e-02 1.4 0.00e+00 0.0 3.6e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd        959 1.0 5.4645e-02 3.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.8770e-06 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 9.4955e-04 2.4 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin       959 1.0 1.1901e-02 1.5 0.00e+00 0.0 3.6e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd         959 1.0 5.3702e-02 3.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack               959 1.0 2.6665e-03 6.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack             959 1.0 4.7573e-04 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.2536e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.6808e+00 1.0 1.39e+09 1.0 3.6e+04 8.0e+03 1.9e+03 88100100100 99  88100100100 99 10338       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                2 1.0 1.7476e-02 1.2 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0   621       0      0 0.00e+00    0 0.00e+00  0
PCSetUpOnBlocks        1 1.0 1.1187e-02 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   970       0      0 0.00e+00    0 0.00e+00  0
PCApply              959 1.0 1.1292e+00 1.0 4.28e+08 1.0 0.0e+00 0.0e+00 0.0e+00 36 31  0  0  0  36 31  0  0  0  7572       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              4     11791508     0.
         Vec Scatter     1              1          816     0.
              Vector    13             13      3630080     0.
           Index Set     5              5       608512     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     2              2         2840     0.
      Preconditioner     2              2         1944     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.03e-08
Average time for MPI_Barrier(): 1.08544e-05
Average time for zero size MPI_Send(): 6.1608e-06
#PETSc Option Table entries:
-ksp_type bcgs
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type bjacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.00878969 iterations 1368
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8075 with 20 processors, by luciano.siqueira Wed Sep  9 17:36:15 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           5.216e+00     1.000   5.216e+00
Objects:              2.200e+01     1.000   2.200e+01
Flop:                 2.873e+09     1.002   2.873e+09  5.745e+10
Flop/sec:             5.508e+08     1.002   5.507e+08  1.101e+10
MPI Messages:         5.478e+03     2.000   5.204e+03  1.041e+05
MPI Message Lengths:  4.380e+07     2.000   7.996e+03  8.322e+08
MPI Reductions:       5.494e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 5.2160e+00 100.0%  5.7450e+10 100.0%  1.041e+05 100.0%  7.996e+03      100.0%  5.487e+03  99.9%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 4.3056e-0339.9 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 4.2503e-0362.9 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             2737 1.0 2.9636e+00 1.0 1.23e+09 1.0 1.0e+05 8.0e+03 0.0e+00 57 43100100  0  57 43100100  0  8305       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 4.3035e-0335.4 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7050e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot              2736 1.0 2.7556e-01 1.1 2.74e+08 1.0 0.0e+00 0.0e+00 2.7e+03  5 10  0  0 50   5 10  0  0 50 19857       0      0 0.00e+00    0 0.00e+00  0
VecDotNorm2         1368 1.0 2.9725e-01 1.1 2.74e+08 1.0 0.0e+00 0.0e+00 1.4e+03  5 10  0  0 25   5 10  0  0 25 18409       0      0 0.00e+00    0 0.00e+00  0
VecNorm             1370 1.0 1.2855e-01 1.1 1.37e+08 1.0 0.0e+00 0.0e+00 1.4e+03  2  5  0  0 25   2  5  0  0 25 21314       0      0 0.00e+00    0 0.00e+00  0
VecCopy                2 1.0 1.0967e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 5 1.0 3.3008e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY                1 1.0 8.1861e-05 1.1 1.00e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 24432       0      0 0.00e+00    0 0.00e+00  0
VecAXPBYCZ          2736 1.0 4.6750e-01 1.0 5.47e+08 1.0 0.0e+00 0.0e+00 0.0e+00  9 19  0  0  0   9 19  0  0  0 23410       0      0 0.00e+00    0 0.00e+00  0
VecWAXPY            2736 1.0 3.6717e-01 1.0 2.74e+08 1.0 0.0e+00 0.0e+00 0.0e+00  7 10  0  0  0   7 10  0  0  0 14903       0      0 0.00e+00    0 0.00e+00  0
VecPointwiseMult    2737 1.0 3.9367e-01 1.0 1.37e+08 1.0 0.0e+00 0.0e+00 0.0e+00  7  5  0  0  0   7  5  0  0  0  6953       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     2737 1.0 3.2724e-02 1.5 0.00e+00 0.0 1.0e+05 8.0e+03 0.0e+00  1  0100100  0   1  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       2737 1.0 1.3472e-01 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 1.1849e-05 2.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.2424e-03 3.2 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      2737 1.0 2.9626e-02 1.6 0.00e+00 0.0 1.0e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        2737 1.0 1.3218e-01 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              2737 1.0 6.9102e-03 7.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            2737 1.0 1.2202e-03 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 1.2532e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 4.8466e+00 1.0 2.87e+09 1.0 1.0e+05 8.0e+03 5.5e+03 93100100100100  93100100100100 11851       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 1.0510e-06 3.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply             2737 1.0 3.9795e-01 1.0 1.37e+08 1.0 0.0e+00 0.0e+00 2.0e+00  8  5  0  0  0   8  5  0  0  0  6878       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      8013484     0.
         Vec Scatter     1              1          816     0.
              Vector    12             12      4028608     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1424     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.04e-08
Average time for MPI_Barrier(): 1.21074e-05
Average time for zero size MPI_Send(): 6.40665e-06
#PETSc Option Table entries:
-ksp_type bcgs
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type jacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.00987163 iterations 515
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8075 with 20 processors, by luciano.siqueira Wed Sep  9 17:36:21 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.777e+00     1.000   3.777e+00
Objects:              2.100e+01     1.000   2.100e+01
Flop:                 1.542e+09     1.001   1.542e+09  3.083e+10
Flop/sec:             4.082e+08     1.001   4.082e+08  8.164e+09
MPI Messages:         2.066e+03     2.000   1.963e+03  3.925e+04
MPI Message Lengths:  1.650e+07     2.000   7.988e+03  3.136e+08
MPI Reductions:       2.080e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.7766e+00 100.0%  3.0831e+10 100.0%  3.925e+04 100.0%  7.988e+03      100.0%  2.073e+03  99.7%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 3.7996e-0347.2 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 3.6773e-0366.3 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             1031 1.0 1.1007e+00 1.0 4.64e+08 1.0 3.9e+04 8.0e+03 0.0e+00 29 30100100  0  29 30100100  0  8422       0      0 0.00e+00    0 0.00e+00  0
MatSOR              1031 1.0 1.7019e+00 1.1 5.11e+08 1.0 0.0e+00 0.0e+00 0.0e+00 42 33  0  0  0  42 33  0  0  0  6008       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 3.7285e-0333.7 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6777e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot              1030 1.0 1.8869e-01 1.6 1.03e+08 1.0 0.0e+00 0.0e+00 1.0e+03  5  7  0  0 50   5  7  0  0 50 10917       0      0 0.00e+00    0 0.00e+00  0
VecDotNorm2          515 1.0 1.8508e-01 1.7 1.03e+08 1.0 0.0e+00 0.0e+00 5.2e+02  5  7  0  0 25   5  7  0  0 25 11130       0      0 0.00e+00    0 0.00e+00  0
VecNorm              517 1.0 5.9143e-02 1.1 5.17e+07 1.0 0.0e+00 0.0e+00 5.2e+02  1  3  0  0 25   1  3  0  0 25 17483       0      0 0.00e+00    0 0.00e+00  0
VecCopy                2 1.0 1.1867e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 5 1.0 3.4075e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY                1 1.0 8.5228e-05 1.1 1.00e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 23466       0      0 0.00e+00    0 0.00e+00  0
VecAXPBYCZ          1030 1.0 1.7712e-01 1.0 2.06e+08 1.0 0.0e+00 0.0e+00 0.0e+00  5 13  0  0  0   5 13  0  0  0 23260       0      0 0.00e+00    0 0.00e+00  0
VecWAXPY            1030 1.0 1.5042e-01 1.0 1.03e+08 1.0 0.0e+00 0.0e+00 0.0e+00  4  7  0  0  0   4  7  0  0  0 13695       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     1031 1.0 2.0280e-02 1.6 0.00e+00 0.0 3.9e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       1031 1.0 5.7161e-02 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.7470e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.4184e-03 3.6 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      1031 1.0 1.7856e-02 1.7 0.00e+00 0.0 3.9e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        1031 1.0 5.5771e-02 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              1031 1.0 4.1539e-0311.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            1031 1.0 5.6482e-04 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 1.2691e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 3.4133e+00 1.0 1.54e+09 1.0 3.9e+04 8.0e+03 2.1e+03 90100100100 99  90100100100 99  9029       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 3.3800e-07 2.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply             1031 1.0 1.7039e+00 1.1 5.11e+08 1.0 0.0e+00 0.0e+00 0.0e+00 42 33  0  0  0  42 33  0  0  0  6001       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    11             11      3626880     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1424     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.87e-08
Average time for MPI_Barrier(): 1.10636e-05
Average time for zero size MPI_Send(): 6.2884e-06
#PETSc Option Table entries:
-ksp_type bcgs
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type sor
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.000785404 iterations 301
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8075 with 20 processors, by luciano.siqueira Wed Sep  9 17:36:28 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           4.482e+00     1.000   4.482e+00
Objects:              4.400e+01     1.000   4.400e+01
Flop:                 1.737e+09     1.001   1.737e+09  3.473e+10
Flop/sec:             3.875e+08     1.001   3.874e+08  7.748e+09
MPI Messages:         2.436e+03     2.000   2.314e+03  4.628e+04
MPI Message Lengths:  1.946e+07     2.000   7.990e+03  3.698e+08
MPI Reductions:       1.247e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 4.4822e+00 100.0%  3.4730e+10 100.0%  4.628e+04 100.0%  7.990e+03      100.0%  1.240e+03  99.4%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 4.4355e-0351.8 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 4.1547e-0395.4 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             1216 1.0 1.3765e+00 1.1 5.47e+08 1.0 4.6e+04 8.0e+03 0.0e+00 30 31100100  0  30 31100100  0  7944       0      0 0.00e+00    0 0.00e+00  0
MatSOR              1217 1.0 1.9355e+00 1.1 6.03e+08 1.0 0.0e+00 0.0e+00 0.0e+00 42 35  0  0  0  42 35  0  0  0  6235       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 4.2075e-0344.3 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6580e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot               602 1.0 1.1028e-01 1.6 6.02e+07 1.0 0.0e+00 0.0e+00 6.0e+02  2  3  0  0 48   2  3  0  0 49 10918       0      0 0.00e+00    0 0.00e+00  0
VecDotNorm2          301 1.0 1.0956e-01 1.6 6.02e+07 1.0 0.0e+00 0.0e+00 3.0e+02  2  3  0  0 24   2  3  0  0 24 10990       0      0 0.00e+00    0 0.00e+00  0
VecMDot               10 1.0 3.9911e-03 1.2 5.50e+06 1.0 0.0e+00 0.0e+00 1.0e+01  0  0  0  0  1   0  0  0  0  1 27561       0      0 0.00e+00    0 0.00e+00  0
VecNorm              314 1.0 4.1917e-02 1.2 3.14e+07 1.0 0.0e+00 0.0e+00 3.1e+02  1  2  0  0 25   1  2  0  0 25 14982       0      0 0.00e+00    0 0.00e+00  0
VecScale              11 1.0 2.7613e-04 1.1 5.50e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 39837       0      0 0.00e+00    0 0.00e+00  0
VecCopy             1209 1.0 5.8604e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet              1213 1.0 4.0514e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY                2 1.0 1.6588e-04 1.1 2.00e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 24114       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             1206 1.0 1.7570e-01 1.1 9.04e+07 1.0 0.0e+00 0.0e+00 0.0e+00  4  5  0  0  0   4  5  0  0  0 10296       0      0 0.00e+00    0 0.00e+00  0
VecAXPBYCZ          1205 1.0 2.1537e-01 1.1 2.71e+08 1.0 0.0e+00 0.0e+00 0.0e+00  5 16  0  0  0   5 16  0  0  0 25179       0      0 0.00e+00    0 0.00e+00  0
VecWAXPY             602 1.0 9.2491e-02 1.1 6.02e+07 1.0 0.0e+00 0.0e+00 0.0e+00  2  3  0  0  0   2  3  0  0  0 13017       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY              11 1.0 5.0159e-03 1.0 6.50e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 25918       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     1216 1.0 2.1245e-02 1.4 0.00e+00 0.0 4.6e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       1216 1.0 1.6316e-01 7.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecNormalize          11 1.0 7.8846e-03 1.0 1.65e+06 1.0 0.0e+00 0.0e+00 1.1e+01  0  0  0  0  1   0  0  0  0  1  4185       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.5020e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.4253e-03 3.6 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      1216 1.0 1.8929e-02 1.5 0.00e+00 0.0 4.6e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        1216 1.0 1.6144e-01 7.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              1216 1.0 4.3353e-03 8.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            1216 1.0 6.6759e-04 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.4218e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  3  1  1  1  2   3  1  1  1  2  3341       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 4.1184e+00 1.0 1.74e+09 1.0 4.6e+04 8.0e+03 1.2e+03 92100100100 98  92100100100 99  8430       0      0 0.00e+00    0 0.00e+00  0
KSPGMRESOrthog        10 1.0 1.2303e-02 1.0 1.10e+07 1.0 0.0e+00 0.0e+00 1.0e+01  0  1  0  0  1   0  1  0  0  1 17881       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 1.4119e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  3  1  1  1  2   3  1  1  1  2  3364       0      0 0.00e+00    0 0.00e+00  0
PCApply              614 1.0 2.9738e+00 1.0 1.12e+09 1.0 2.3e+04 8.0e+03 0.0e+00 66 64 50 50  0  66 64 50 50  0  7504       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    30             30     11259712     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     3              3        33440     0.
      Preconditioner     3              3         2936     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.87e-08
Average time for MPI_Barrier(): 1.20012e-05
Average time for zero size MPI_Send(): 6.2072e-06
#PETSc Option Table entries:
-ksp_type bcgs
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 8.37695e-05 iterations 475
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8075 with 20 processors, by luciano.siqueira Wed Sep  9 17:36:33 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.253e+00     1.000   3.253e+00
Objects:              3.200e+01     1.000   3.200e+01
Flop:                 1.446e+09     1.001   1.446e+09  2.891e+10
Flop/sec:             4.445e+08     1.001   4.444e+08  8.888e+09
MPI Messages:         1.906e+03     2.000   1.811e+03  3.621e+04
MPI Message Lengths:  1.522e+07     2.000   7.987e+03  2.893e+08
MPI Reductions:       1.445e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.2528e+00 100.0%  2.8910e+10 100.0%  3.621e+04 100.0%  7.987e+03      100.0%  1.438e+03  99.5%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 2.9750e-0344.2 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 2.9485e-0360.2 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult              951 1.0 1.0544e+00 1.0 4.28e+08 1.0 3.6e+04 8.0e+03 0.0e+00 32 30100100  0  32 30100100  0  8110       0      0 0.00e+00    0 0.00e+00  0
MatSolve             951 1.0 1.1271e+00 1.0 4.24e+08 1.0 0.0e+00 0.0e+00 0.0e+00 34 29  0  0  0  34 29  0  0  0  7523       0      0 0.00e+00    0 0.00e+00  0
MatLUFactorNum         1 1.0 7.5856e-03 1.0 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1431       0      0 0.00e+00    0 0.00e+00  0
MatILUFactorSym        1 1.0 3.4659e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 3.0010e-0329.0 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6922e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetRowIJ            1 1.0 2.9900e-06 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetOrdering         1 1.0 6.9759e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot               950 1.0 1.3562e-01 1.3 9.50e+07 1.0 0.0e+00 0.0e+00 9.5e+02  4  7  0  0 66   4  7  0  0 66 14009       0      0 0.00e+00    0 0.00e+00  0
VecNorm              477 1.0 7.6641e-02 1.5 4.77e+07 1.0 0.0e+00 0.0e+00 4.8e+02  2  3  0  0 33   2  3  0  0 33 12448       0      0 0.00e+00    0 0.00e+00  0
VecCopy                5 1.0 3.5809e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet               955 1.0 2.1632e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             1899 1.0 1.0266e-01 1.1 1.90e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3 13  0  0  0   3 13  0  0  0 36995       0      0 0.00e+00    0 0.00e+00  0
VecAYPX              949 1.0 1.3080e-01 1.0 9.48e+07 1.0 0.0e+00 0.0e+00 0.0e+00  4  7  0  0  0   4  7  0  0  0 14496       0      0 0.00e+00    0 0.00e+00  0
VecWAXPY            1898 1.0 2.6255e-01 1.0 1.66e+08 1.0 0.0e+00 0.0e+00 0.0e+00  8 11  0  0  0   8 11  0  0  0 12649       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin      951 1.0 1.5857e-02 1.6 0.00e+00 0.0 3.6e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd        951 1.0 7.0549e-02 3.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 1.3115e-05 2.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.3445e-03 3.5 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin       951 1.0 1.4145e-02 1.7 0.00e+00 0.0 3.6e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd         951 1.0 6.9317e-02 3.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack               951 1.0 2.8319e-03 6.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack             951 1.0 4.9103e-04 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.8879e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.8800e+00 1.0 1.45e+09 1.0 3.6e+04 8.0e+03 1.4e+03 89100100100 99  89100100100 99 10034       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                2 1.0 2.5534e-02 1.2 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0   425       0      0 0.00e+00    0 0.00e+00  0
PCSetUpOnBlocks        1 1.0 1.1741e-02 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   924       0      0 0.00e+00    0 0.00e+00  0
PCApply              951 1.0 1.1538e+00 1.0 4.24e+08 1.0 0.0e+00 0.0e+00 0.0e+00 35 29  0  0  0  35 29  0  0  0  7349       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              4     11791508     0.
         Vec Scatter     1              1          816     0.
              Vector    16             16      4835264     0.
           Index Set     5              5       608512     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     2              2         2832     0.
      Preconditioner     2              2         1944     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.25e-08
Average time for MPI_Barrier(): 1.0412e-05
Average time for zero size MPI_Send(): 8.79365e-06
#PETSc Option Table entries:
-ksp_type tfqmr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type bjacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 3.10408e-07 iterations 1511
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8075 with 20 processors, by luciano.siqueira Wed Sep  9 17:36:41 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           5.958e+00     1.000   5.958e+00
Objects:              2.500e+01     1.000   2.500e+01
Flop:                 3.399e+09     1.002   3.399e+09  6.797e+10
Flop/sec:             5.706e+08     1.002   5.705e+08  1.141e+10
MPI Messages:         6.050e+03     2.000   5.748e+03  1.150e+05
MPI Message Lengths:  4.838e+07     2.000   7.996e+03  9.191e+08
MPI Reductions:       4.555e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 5.9580e+00 100.0%  6.7975e+10 100.0%  1.150e+05 100.0%  7.996e+03      100.0%  4.548e+03  99.8%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 3.7278e-0338.2 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 3.6932e-0377.0 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             3023 1.0 3.2591e+00 1.1 1.36e+09 1.0 1.1e+05 8.0e+03 0.0e+00 53 40100100  0  53 40100100  0  8341       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 3.7471e-0337.7 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6483e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot              3022 1.0 3.7158e-01 1.4 3.02e+08 1.0 0.0e+00 0.0e+00 3.0e+03  5  9  0  0 66   5  9  0  0 66 16266       0      0 0.00e+00    0 0.00e+00  0
VecNorm             1513 1.0 2.0529e-01 1.7 1.51e+08 1.0 0.0e+00 0.0e+00 1.5e+03  3  4  0  0 33   3  4  0  0 33 14740       0      0 0.00e+00    0 0.00e+00  0
VecCopy                5 1.0 3.2774e-04 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 4 1.0 2.7668e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             6043 1.0 3.0668e-01 1.1 6.04e+08 1.0 0.0e+00 0.0e+00 0.0e+00  5 18  0  0  0   5 18  0  0  0 39409       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             3021 1.0 4.0426e-01 1.0 3.02e+08 1.0 0.0e+00 0.0e+00 0.0e+00  7  9  0  0  0   7  9  0  0  0 14941       0      0 0.00e+00    0 0.00e+00  0
VecWAXPY            6042 1.0 7.9404e-01 1.0 5.29e+08 1.0 0.0e+00 0.0e+00 0.0e+00 13 16  0  0  0  13 16  0  0  0 13315       0      0 0.00e+00    0 0.00e+00  0
VecPointwiseMult    3023 1.0 4.3115e-01 1.0 1.51e+08 1.0 0.0e+00 0.0e+00 0.0e+00  7  4  0  0  0   7  4  0  0  0  7012       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     3023 1.0 3.5497e-02 1.5 0.00e+00 0.0 1.1e+05 8.0e+03 0.0e+00  1  0100100  0   1  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       3023 1.0 1.9726e-01 2.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.4600e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.5870e-03 4.2 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      3023 1.0 3.1759e-02 1.6 0.00e+00 0.0 1.1e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        3023 1.0 1.9351e-01 2.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              3023 1.0 8.1134e-03 7.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            3023 1.0 1.3934e-03 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 1.8335e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 5.5874e+00 1.0 3.40e+09 1.0 1.1e+05 8.0e+03 4.5e+03 94100100100100  94100100100100 12163       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 8.0000e-07 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply             3023 1.0 4.3527e-01 1.0 1.51e+08 1.0 0.0e+00 0.0e+00 2.0e+00  7  4  0  0  0   7  4  0  0  0  6945       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      8013484     0.
         Vec Scatter     1              1          816     0.
              Vector    15             15      5233792     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1416     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.07e-08
Average time for MPI_Barrier(): 1.62796e-05
Average time for zero size MPI_Send(): 6.2978e-06
#PETSc Option Table entries:
-ksp_type tfqmr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type jacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.000233762 iterations 533
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8075 with 20 processors, by luciano.siqueira Wed Sep  9 17:36:47 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.913e+00     1.000   3.913e+00
Objects:              2.400e+01     1.000   2.400e+01
Flop:                 1.675e+09     1.001   1.675e+09  3.349e+10
Flop/sec:             4.280e+08     1.001   4.280e+08  8.559e+09
MPI Messages:         2.138e+03     2.000   2.031e+03  4.062e+04
MPI Message Lengths:  1.708e+07     2.000   7.989e+03  3.245e+08
MPI Reductions:       1.619e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.9133e+00 100.0%  3.3495e+10 100.0%  4.062e+04 100.0%  7.989e+03      100.0%  1.612e+03  99.6%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 3.4609e-0324.8 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 3.3960e-0360.0 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             1067 1.0 1.1241e+00 1.0 4.80e+08 1.0 4.1e+04 8.0e+03 0.0e+00 28 29100100  0  28 29100100  0  8535       0      0 0.00e+00    0 0.00e+00  0
MatSOR              1067 1.0 1.6329e+00 1.0 5.29e+08 1.0 0.0e+00 0.0e+00 0.0e+00 41 32  0  0  0  41 32  0  0  0  6480       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 3.4509e-0331.2 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7102e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot              1066 1.0 1.8039e-01 1.3 1.07e+08 1.0 0.0e+00 0.0e+00 1.1e+03  4  6  0  0 66   4  6  0  0 66 11819       0      0 0.00e+00    0 0.00e+00  0
VecNorm              535 1.0 1.0176e-01 1.6 5.35e+07 1.0 0.0e+00 0.0e+00 5.4e+02  2  3  0  0 33   2  3  0  0 33 10515       0      0 0.00e+00    0 0.00e+00  0
VecCopy                5 1.0 3.4630e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 4 1.0 2.7868e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             2131 1.0 1.2868e-01 1.1 2.13e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3 13  0  0  0   3 13  0  0  0 33120       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             1065 1.0 1.4816e-01 1.0 1.06e+08 1.0 0.0e+00 0.0e+00 0.0e+00  4  6  0  0  0   4  6  0  0  0 14363       0      0 0.00e+00    0 0.00e+00  0
VecWAXPY            2130 1.0 2.9958e-01 1.0 1.86e+08 1.0 0.0e+00 0.0e+00 0.0e+00  8 11  0  0  0   8 11  0  0  0 12441       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     1067 1.0 1.7501e-02 1.5 0.00e+00 0.0 4.1e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       1067 1.0 6.5444e-02 2.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.6350e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.3070e-03 3.4 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      1067 1.0 1.5760e-02 1.5 0.00e+00 0.0 4.1e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        1067 1.0 6.4079e-02 2.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              1067 1.0 3.9695e-03 9.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            1067 1.0 6.4756e-04 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 1.8901e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 3.5416e+00 1.0 1.67e+09 1.0 4.1e+04 8.0e+03 1.6e+03 90100100100 99  90100100100 99  9454       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 5.1500e-07 3.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply             1067 1.0 1.6353e+00 1.0 5.29e+08 1.0 0.0e+00 0.0e+00 0.0e+00 41 32  0  0  0  41 32  0  0  0  6471       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    14             14      4832064     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1416     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.05e-08
Average time for MPI_Barrier(): 1.22398e-05
Average time for zero size MPI_Send(): 6.36165e-06
#PETSc Option Table entries:
-ksp_type tfqmr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type sor
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.0017322 iterations 281
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8075 with 20 processors, by luciano.siqueira Wed Sep  9 17:36:54 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           4.317e+00     1.000   4.317e+00
Objects:              4.700e+01     1.000   4.700e+01
Flop:                 1.665e+09     1.001   1.664e+09  3.329e+10
Flop/sec:             3.856e+08     1.001   3.856e+08  7.711e+09
MPI Messages:         2.276e+03     2.000   2.162e+03  4.324e+04
MPI Message Lengths:  1.818e+07     2.000   7.989e+03  3.455e+08
MPI Reductions:       8.860e+02     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 4.3168e+00 100.0%  3.3288e+10 100.0%  4.324e+04 100.0%  7.989e+03      100.0%  8.790e+02  99.2%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 4.6485e-0314.8 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 4.4960e-0395.7 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             1136 1.0 1.2632e+00 1.0 5.11e+08 1.0 4.3e+04 8.0e+03 0.0e+00 29 31100100  0  29 31100100  0  8086       0      0 0.00e+00    0 0.00e+00  0
MatSOR              1137 1.0 1.8038e+00 1.0 5.64e+08 1.0 0.0e+00 0.0e+00 0.0e+00 41 34  0  0  0  41 34  0  0  0  6251       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 4.5543e-0346.1 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7389e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  1   1  0  0  0  1     0       0      0 0.00e+00    0 0.00e+00  0
VecDot               562 1.0 9.4517e-02 1.3 5.62e+07 1.0 0.0e+00 0.0e+00 5.6e+02  2  3  0  0 63   2  3  0  0 64 11892       0      0 0.00e+00    0 0.00e+00  0
VecMDot               10 1.0 3.8019e-03 1.2 5.50e+06 1.0 0.0e+00 0.0e+00 1.0e+01  0  0  0  0  1   0  0  0  0  1 28933       0      0 0.00e+00    0 0.00e+00  0
VecNorm              294 1.0 5.9776e-02 1.5 2.94e+07 1.0 0.0e+00 0.0e+00 2.9e+02  1  2  0  0 33   1  2  0  0 33  9837       0      0 0.00e+00    0 0.00e+00  0
VecScale              11 1.0 2.4411e-04 1.1 5.50e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 45061       0      0 0.00e+00    0 0.00e+00  0
VecCopy             1132 1.0 5.6707e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet              1132 1.0 3.5163e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             1124 1.0 6.8046e-02 1.1 1.12e+08 1.0 0.0e+00 0.0e+00 0.0e+00  2  7  0  0  0   2  7  0  0  0 33036       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             1687 1.0 2.4393e-01 1.0 1.40e+08 1.0 0.0e+00 0.0e+00 0.0e+00  6  8  0  0  0   6  8  0  0  0 11516       0      0 0.00e+00    0 0.00e+00  0
VecAXPBYCZ           563 1.0 1.0136e-01 1.0 1.41e+08 1.0 0.0e+00 0.0e+00 0.0e+00  2  8  0  0  0   2  8  0  0  0 27772       0      0 0.00e+00    0 0.00e+00  0
VecWAXPY            1122 1.0 1.6759e-01 1.0 9.82e+07 1.0 0.0e+00 0.0e+00 0.0e+00  4  6  0  0  0   4  6  0  0  0 11713       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY              11 1.0 4.5500e-03 1.0 6.50e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 28571       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     1136 1.0 2.3230e-02 1.7 0.00e+00 0.0 4.3e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       1136 1.0 9.0830e-02 2.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecNormalize          11 1.0 7.9676e-03 1.0 1.65e+06 1.0 0.0e+00 0.0e+00 1.1e+01  0  0  0  0  1   0  0  0  0  1  4142       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.6590e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.2644e-03 3.1 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      1136 1.0 2.0648e-02 1.7 0.00e+00 0.0 4.3e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        1136 1.0 8.9399e-02 2.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              1136 1.0 4.0142e-03 8.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            1136 1.0 7.5435e-04 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.4285e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  3  1  1  1  3   3  1  1  1  3  3325       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 3.9440e+00 1.0 1.66e+09 1.0 4.3e+04 8.0e+03 8.7e+02 91100100100 98  91100100100 99  8437       0      0 0.00e+00    0 0.00e+00  0
KSPGMRESOrthog        10 1.0 1.1741e-02 1.1 1.10e+07 1.0 0.0e+00 0.0e+00 1.0e+01  0  1  0  0  1   0  1  0  0  1 18737       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 1.4131e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  3  1  1  1  3   3  1  1  1  3  3361       0      0 0.00e+00    0 0.00e+00  0
PCApply              574 1.0 2.7794e+00 1.0 1.04e+09 1.0 2.1e+04 8.0e+03 0.0e+00 64 63 49 50  0  64 63 49 50  0  7499       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    33             33     12464896     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     3              3        33432     0.
      Preconditioner     3              3         2936     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.97e-08
Average time for MPI_Barrier(): 1.30644e-05
Average time for zero size MPI_Send(): 6.51645e-06
#PETSc Option Table entries:
-ksp_type tfqmr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.00115167 iterations 573
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8075 with 20 processors, by luciano.siqueira Wed Sep  9 17:36:58 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           2.072e+00     1.000   2.072e+00
Objects:              2.900e+01     1.000   2.900e+01
Flop:                 9.164e+08     1.001   9.163e+08  1.833e+10
Flop/sec:             4.423e+08     1.001   4.422e+08  8.844e+09
MPI Messages:         1.154e+03     2.000   1.096e+03  2.193e+04
MPI Message Lengths:  9.208e+06     2.000   7.979e+03  1.750e+08
MPI Reductions:       1.166e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 2.0720e+00 100.0%  1.8325e+10 100.0%  2.193e+04 100.0%  7.979e+03      100.0%  1.159e+03  99.4%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 4.1611e-0338.5 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 4.1402e-0379.7 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult              575 1.0 6.2602e-01 1.0 2.59e+08 1.0 2.2e+04 8.0e+03 0.0e+00 30 28100100  0  30 28100100  0  8259       0      0 0.00e+00    0 0.00e+00  0
MatSolve             574 1.0 6.8065e-01 1.0 2.56e+08 1.0 0.0e+00 0.0e+00 0.0e+00 32 28  0  0  0  32 28  0  0  0  7519       0      0 0.00e+00    0 0.00e+00  0
MatLUFactorNum         1 1.0 7.5959e-03 1.0 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1429       0      0 0.00e+00    0 0.00e+00  0
MatILUFactorSym        1 1.0 3.4240e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 4.1939e-0339.6 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6419e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetRowIJ            1 1.0 2.8040e-06 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetOrdering         1 1.0 6.9379e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot               573 1.0 8.4373e-02 1.7 5.73e+07 1.0 0.0e+00 0.0e+00 5.7e+02  3  6  0  0 49   3  6  0  0 49 13582       0      0 0.00e+00    0 0.00e+00  0
VecNorm                1 1.0 8.7400e-05 1.1 1.00e+05 1.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0 22883       0      0 0.00e+00    0 0.00e+00  0
VecCopy                3 1.0 2.3865e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet               577 1.0 1.0296e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             1147 1.0 5.9384e-02 1.1 1.15e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3 13  0  0  0   3 13  0  0  0 38630       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             1144 1.0 1.5148e-01 1.1 1.14e+08 1.0 0.0e+00 0.0e+00 0.0e+00  7 12  0  0  0   7 12  0  0  0 15105       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin      575 1.0 9.1235e-03 1.5 0.00e+00 0.0 2.2e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd        575 1.0 3.7728e-02 2.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecReduceArith      1148 1.0 8.5161e-02 1.1 1.15e+08 1.0 0.0e+00 0.0e+00 0.0e+00  4 13  0  0  0   4 13  0  0  0 26961       0      0 0.00e+00    0 0.00e+00  0
VecReduceComm        574 1.0 2.6824e-02 2.5 0.00e+00 0.0 0.0e+00 0.0e+00 5.7e+02  1  0  0  0 49   1  0  0  0 50     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.6320e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.2098e-03 3.1 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin       575 1.0 8.3216e-03 1.6 0.00e+00 0.0 2.2e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd         575 1.0 3.7107e-02 2.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack               575 1.0 1.7208e-03 8.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack             575 1.0 3.2382e-04 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.2664e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 1.7008e+00 1.0 9.16e+08 1.0 2.2e+04 8.0e+03 1.1e+03 82100 99100 98  82100 99100 99 10766       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                2 1.0 2.6551e-02 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0   409       0      0 0.00e+00    0 0.00e+00  0
PCSetUpOnBlocks        1 1.0 1.1777e-02 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0   922       0      0 0.00e+00    0 0.00e+00  0
PCApply              574 1.0 6.9478e-01 1.0 2.56e+08 1.0 0.0e+00 0.0e+00 0.0e+00 33 28  0  0  0  33 28  0  0  0  7366       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              4     11791508     0.
         Vec Scatter     1              1          816     0.
              Vector    13             13      3630080     0.
           Index Set     5              5       608512     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     2              2         2832     0.
      Preconditioner     2              2         1944     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 2.97e-08
Average time for MPI_Barrier(): 1.34824e-05
Average time for zero size MPI_Send(): 6.28985e-06
#PETSc Option Table entries:
-ksp_type cr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type bjacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.000525242 iterations 1635
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8075 with 20 processors, by luciano.siqueira Wed Sep  9 17:37:03 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.346e+00     1.000   3.346e+00
Objects:              2.200e+01     1.000   2.200e+01
Flop:                 1.963e+09     1.002   1.962e+09  3.925e+10
Flop/sec:             5.866e+08     1.002   5.865e+08  1.173e+10
MPI Messages:         3.278e+03     2.000   3.114e+03  6.228e+04
MPI Message Lengths:  2.620e+07     2.000   7.993e+03  4.978e+08
MPI Reductions:       3.292e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.3460e+00 100.0%  3.9250e+10 100.0%  6.228e+04 100.0%  7.993e+03      100.0%  3.285e+03  99.8%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 3.1337e-03 4.2 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 2.1265e-0341.3 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             1637 1.0 1.7706e+00 1.0 7.36e+08 1.0 6.2e+04 8.0e+03 0.0e+00 52 38100100  0  52 38100100  0  8313       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 2.1811e-0320.2 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7545e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot              1635 1.0 1.6348e-01 1.2 1.63e+08 1.0 0.0e+00 0.0e+00 1.6e+03  4  8  0  0 50   4  8  0  0 50 20002       0      0 0.00e+00    0 0.00e+00  0
VecNorm                1 1.0 1.1515e-04 1.3 1.00e+05 1.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0 17368       0      0 0.00e+00    0 0.00e+00  0
VecCopy                3 1.0 2.1822e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 3 1.0 2.1563e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             3271 1.0 1.5903e-01 1.1 3.27e+08 1.0 0.0e+00 0.0e+00 0.0e+00  5 17  0  0  0   5 17  0  0  0 41136       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             3268 1.0 4.2656e-01 1.0 3.27e+08 1.0 0.0e+00 0.0e+00 0.0e+00 13 17  0  0  0  13 17  0  0  0 15322       0      0 0.00e+00    0 0.00e+00  0
VecPointwiseMult    1636 1.0 2.3718e-01 1.1 8.18e+07 1.0 0.0e+00 0.0e+00 0.0e+00  7  4  0  0  0   7  4  0  0  0  6898       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     1637 1.0 2.1413e-02 1.9 0.00e+00 0.0 6.2e+04 8.0e+03 0.0e+00  1  0100100  0   1  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       1637 1.0 1.2329e-01 3.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecReduceArith      3272 1.0 2.1868e-01 1.0 3.27e+08 1.0 0.0e+00 0.0e+00 0.0e+00  6 17  0  0  0   6 17  0  0  0 29925       0      0 0.00e+00    0 0.00e+00  0
VecReduceComm       1636 1.0 6.4455e-02 2.6 0.00e+00 0.0 0.0e+00 0.0e+00 1.6e+03  1  0  0  0 50   1  0  0  0 50     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.9040e-06 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.6148e-03 4.2 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      1637 1.0 1.9231e-02 2.0 0.00e+00 0.0 6.2e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        1637 1.0 1.2178e-01 3.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              1637 1.0 4.0712e-03 8.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            1637 1.0 9.3528e-04 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 1.2679e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.9786e+00 1.0 1.96e+09 1.0 6.2e+04 8.0e+03 3.3e+03 89100100100 99  89100100100100 13173       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 8.0200e-07 2.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply             1636 1.0 2.4168e-01 1.1 8.18e+07 1.0 0.0e+00 0.0e+00 2.0e+00  7  4  0  0  0   7  4  0  0  0  6769       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      8013484     0.
         Vec Scatter     1              1          816     0.
              Vector    12             12      4028608     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1416     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.67e-08
Average time for MPI_Barrier(): 1.23642e-05
Average time for zero size MPI_Send(): 7.2517e-06
#PETSc Option Table entries:
-ksp_type cr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type jacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.000702134 iterations 672
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8075 with 20 processors, by luciano.siqueira Wed Sep  9 17:37:08 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           2.594e+00     1.000   2.594e+00
Objects:              2.100e+01     1.000   2.100e+01
Flop:                 1.107e+09     1.001   1.107e+09  2.215e+10
Flop/sec:             4.269e+08     1.001   4.268e+08  8.536e+09
MPI Messages:         1.352e+03     2.000   1.284e+03  2.569e+04
MPI Message Lengths:  1.079e+07     2.000   7.982e+03  2.050e+08
MPI Reductions:       1.364e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 2.5944e+00 100.0%  2.2147e+10 100.0%  2.569e+04 100.0%  7.982e+03      100.0%  1.357e+03  99.5%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 5.3783e-0348.4 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 4.3617e-0390.5 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult              674 1.0 7.1263e-01 1.0 3.03e+08 1.0 2.6e+04 8.0e+03 0.0e+00 27 27100100  0  27 27100100  0  8505       0      0 0.00e+00    0 0.00e+00  0
MatSOR               673 1.0 1.0334e+00 1.0 3.34e+08 1.0 0.0e+00 0.0e+00 0.0e+00 39 30  0  0  0  39 30  0  0  0  6458       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 4.4098e-0343.6 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6039e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDot               672 1.0 1.1438e-01 1.6 6.72e+07 1.0 0.0e+00 0.0e+00 6.7e+02  4  6  0  0 49   4  6  0  0 50 11750       0      0 0.00e+00    0 0.00e+00  0
VecNorm                1 1.0 8.7221e-05 1.2 1.00e+05 1.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0 22930       0      0 0.00e+00    0 0.00e+00  0
VecCopy                3 1.0 2.1617e-04 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet                 3 1.0 2.1690e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             1345 1.0 1.0655e-01 1.1 1.34e+08 1.0 0.0e+00 0.0e+00 0.0e+00  4 12  0  0  0   4 12  0  0  0 25245       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             1342 1.0 1.7951e-01 1.0 1.34e+08 1.0 0.0e+00 0.0e+00 0.0e+00  7 12  0  0  0   7 12  0  0  0 14952       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin      674 1.0 1.4521e-02 1.6 0.00e+00 0.0 2.6e+04 8.0e+03 0.0e+00  1  0100100  0   1  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd        674 1.0 3.9460e-02 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecReduceArith      1346 1.0 1.0443e-01 1.2 1.35e+08 1.0 0.0e+00 0.0e+00 0.0e+00  4 12  0  0  0   4 12  0  0  0 25779       0      0 0.00e+00    0 0.00e+00  0
VecReduceComm        673 1.0 4.8193e-02 2.3 0.00e+00 0.0 0.0e+00 0.0e+00 6.7e+02  1  0  0  0 49   1  0  0  0 50     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.9530e-06 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.6073e-03 4.1 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin       674 1.0 1.3242e-02 1.7 0.00e+00 0.0 2.6e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd         674 1.0 3.8574e-02 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack               674 1.0 2.9441e-03 9.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack             674 1.0 3.9119e-04 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 1.2561e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.2299e+00 1.0 1.11e+09 1.0 2.6e+04 8.0e+03 1.3e+03 86100100100 99  86100100100 99  9926       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 3.3000e-07 2.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply              673 1.0 1.0348e+00 1.0 3.34e+08 1.0 0.0e+00 0.0e+00 0.0e+00 39 30  0  0  0  39 30  0  0  0  6450       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    11             11      3626880     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1416     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.22e-08
Average time for MPI_Barrier(): 1.18284e-05
Average time for zero size MPI_Send(): 6.24105e-06
#PETSc Option Table entries:
-ksp_type cr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type sor
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.000656407 iterations 374
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8075 with 20 processors, by luciano.siqueira Wed Sep  9 17:37:12 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           2.987e+00     1.000   2.987e+00
Objects:              4.400e+01     1.000   4.400e+01
Flop:                 1.145e+09     1.001   1.145e+09  2.290e+10
Flop/sec:             3.835e+08     1.001   3.834e+08  7.668e+09
MPI Messages:         1.526e+03     2.000   1.450e+03  2.899e+04
MPI Message Lengths:  1.218e+07     2.000   7.984e+03  2.315e+08
MPI Reductions:       7.910e+02     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 2.9870e+00 100.0%  2.2905e+10 100.0%  2.899e+04 100.0%  7.984e+03      100.0%  7.840e+02  99.1%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 4.1978e-0327.1 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 4.1157e-0378.9 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult              761 1.0 8.2401e-01 1.0 3.42e+08 1.0 2.9e+04 8.0e+03 0.0e+00 27 30100100  0  27 30100100  0  8304       0      0 0.00e+00    0 0.00e+00  0
MatSOR               761 1.0 1.1816e+00 1.0 3.77e+08 1.0 0.0e+00 0.0e+00 0.0e+00 39 33  0  0  0  39 33  0  0  0  6387       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 4.1651e-0340.3 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6539e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  1  0  0  0  1   1  0  0  0  1     0       0      0 0.00e+00    0 0.00e+00  0
VecDot               374 1.0 7.5448e-02 1.6 3.74e+07 1.0 0.0e+00 0.0e+00 3.7e+02  2  3  0  0 47   2  3  0  0 48  9914       0      0 0.00e+00    0 0.00e+00  0
VecMDot               10 1.0 3.6431e-03 1.1 5.50e+06 1.0 0.0e+00 0.0e+00 1.0e+01  0  0  0  0  1   0  0  0  0  1 30194       0      0 0.00e+00    0 0.00e+00  0
VecNorm               12 1.0 7.2721e-03 1.1 1.20e+06 1.0 0.0e+00 0.0e+00 1.2e+01  0  0  0  0  2   0  0  0  0  2  3300       0      0 0.00e+00    0 0.00e+00  0
VecScale              11 1.0 2.5083e-04 1.1 5.50e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 43854       0      0 0.00e+00    0 0.00e+00  0
VecCopy              754 1.0 3.5587e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet               755 1.0 2.0594e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY              750 1.0 7.3401e-02 1.1 7.50e+07 1.0 0.0e+00 0.0e+00 0.0e+00  2  7  0  0  0   2  7  0  0  0 20436       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             1496 1.0 2.0462e-01 1.0 1.31e+08 1.0 0.0e+00 0.0e+00 0.0e+00  7 11  0  0  0   7 11  0  0  0 12790       0      0 0.00e+00    0 0.00e+00  0
VecAXPBYCZ           375 1.0 6.4687e-02 1.0 9.38e+07 1.0 0.0e+00 0.0e+00 0.0e+00  2  8  0  0  0   2  8  0  0  0 28986       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY              11 1.0 4.5847e-03 1.0 6.50e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0 28355       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin      761 1.0 1.5671e-02 1.4 0.00e+00 0.0 2.9e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd        761 1.0 5.4665e-02 2.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecReduceArith       750 1.0 5.5516e-02 1.1 7.50e+07 1.0 0.0e+00 0.0e+00 0.0e+00  2  7  0  0  0   2  7  0  0  0 27019       0      0 0.00e+00    0 0.00e+00  0
VecReduceComm        375 1.0 2.8041e-02 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 3.8e+02  1  0  0  0 47   1  0  0  0 48     0       0      0 0.00e+00    0 0.00e+00  0
VecNormalize          11 1.0 7.4557e-03 1.1 1.65e+06 1.0 0.0e+00 0.0e+00 1.1e+01  0  0  0  0  1   0  0  0  0  1  4426       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 1.0044e-05 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.4784e-03 3.8 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin       761 1.0 1.4013e-02 1.4 0.00e+00 0.0 2.9e+04 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd         761 1.0 5.3504e-02 2.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack               761 1.0 2.8692e-03 7.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack             761 1.0 6.9415e-04 2.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.4266e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  5  2  1  1  3   5  2  1  1  3  3330       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 2.6154e+00 1.0 1.14e+09 1.0 2.9e+04 8.0e+03 7.7e+02 87100100100 98  87100100100 98  8753       0      0 0.00e+00    0 0.00e+00  0
KSPGMRESOrthog        10 1.0 1.1656e-02 1.0 1.10e+07 1.0 0.0e+00 0.0e+00 1.0e+01  0  1  0  0  1   0  1  0  0  1 18874       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 1.4170e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  5  2  1  1  3   5  2  1  1  3  3352       0      0 0.00e+00    0 0.00e+00  0
PCApply              386 1.0 1.8044e+00 1.0 6.96e+08 1.0 1.4e+04 8.0e+03 0.0e+00 60 61 49 49  0  60 61 49 49  0  7714       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    30             30     11259712     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     3              3        33432     0.
      Preconditioner     3              3         2936     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.23e-08
Average time for MPI_Barrier(): 1.1748e-05
Average time for zero size MPI_Send(): 6.3872e-06
#PETSc Option Table entries:
-ksp_type cr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.0311837 iterations 7307
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8075 with 20 processors, by luciano.siqueira Wed Sep  9 17:37:56 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           4.092e+01     1.000   4.092e+01
Objects:              8.400e+01     1.000   8.400e+01
Flop:                 4.268e+10     1.000   4.268e+10  8.536e+11
Flop/sec:             1.043e+09     1.000   1.043e+09  2.086e+10
MPI Messages:         1.462e+04     2.000   1.389e+04  2.778e+05
MPI Message Lengths:  1.170e+08     2.000   7.998e+03  2.222e+09
MPI Reductions:       2.170e+04     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 4.0917e+01 100.0%  8.5362e+11 100.0%  2.778e+05 100.0%  7.998e+03      100.0%  2.169e+04 100.0%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 4.8616e-0341.7 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 4.8066e-0395.8 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             7309 1.0 8.3161e+00 1.0 3.29e+09 1.0 2.8e+05 8.0e+03 0.0e+00 20  8100100  0  20  8100100  0  7903       0      0 0.00e+00    0 0.00e+00  0
MatSolve            7307 1.0 8.4782e+00 1.0 3.26e+09 1.0 0.0e+00 0.0e+00 0.0e+00 20  8  0  0  0  20  8  0  0  0  7684       0      0 0.00e+00    0 0.00e+00  0
MatLUFactorNum         1 1.0 7.5480e-03 1.0 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1438       0      0 0.00e+00    0 0.00e+00  0
MatILUFactorSym        1 1.0 3.4809e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 4.8641e-0347.8 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.7250e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetRowIJ            1 1.0 2.7020e-06 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatGetOrdering         1 1.0 6.9816e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDotNorm2         7307 1.0 2.0932e+00 1.2 1.46e+09 1.0 0.0e+00 0.0e+00 7.3e+03  5  3  0  0 34   5  3  0  0 34 13963       0      0 0.00e+00    0 0.00e+00  0
VecMDot             7063 1.0 5.8507e+00 1.0 1.06e+10 1.0 0.0e+00 0.0e+00 7.1e+03 14 25  0  0 33  14 25  0  0 33 36180       0      0 0.00e+00    0 0.00e+00  0
VecNorm             7309 1.0 6.4851e-01 1.1 7.31e+08 1.0 0.0e+00 0.0e+00 7.3e+03  2  2  0  0 34   2  2  0  0 34 22541       0      0 0.00e+00    0 0.00e+00  0
VecScale           14614 1.0 3.3574e-01 1.1 7.31e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1  2  0  0  0   1  2  0  0  0 43528       0      0 0.00e+00    0 0.00e+00  0
VecSet              7310 1.0 4.5645e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY            14615 1.0 7.0796e-01 1.0 1.46e+09 1.0 0.0e+00 0.0e+00 0.0e+00  2  3  0  0  0   2  3  0  0  0 41287       0      0 0.00e+00    0 0.00e+00  0
VecAYPX                1 1.0 1.7404e-04 1.1 5.00e+04 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  5746       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY           14126 1.0 1.4309e+01 1.0 2.12e+10 1.0 0.0e+00 0.0e+00 0.0e+00 35 50  0  0  0  35 50  0  0  0 29587       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     7309 1.0 1.1180e-01 1.6 0.00e+00 0.0 2.8e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       7309 1.0 8.5470e-01 2.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.6230e-06 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.6104e-03 3.7 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      7309 1.0 9.8131e-02 1.8 0.00e+00 0.0 2.8e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        7309 1.0 8.4486e-01 2.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              7309 1.0 2.5994e-02 8.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            7309 1.0 3.9210e-03 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.4284e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 4.0545e+01 1.0 4.27e+10 1.0 2.8e+05 8.0e+03 2.2e+04 99100100100100  99100100100100 21053       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                2 1.0 2.7001e-02 1.0 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   402       0      0 0.00e+00    0 0.00e+00  0
PCSetUpOnBlocks        1 1.0 1.1787e-02 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   921       0      0 0.00e+00    0 0.00e+00  0
PCApply             7307 1.0 9.0095e+00 1.0 3.26e+09 1.0 0.0e+00 0.0e+00 0.0e+00 22  8  0  0  0  22  8  0  0  0  7231       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     4              4     11791508     0.
         Vec Scatter     1              1          816     0.
              Vector    68             68     25725120     0.
           Index Set     5              5       608512     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     2              2         2896     0.
      Preconditioner     2              2         1944     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.4e-08
Average time for MPI_Barrier(): 1.35676e-05
Average time for zero size MPI_Send(): 6.5164e-06
#PETSc Option Table entries:
-ksp_type gcr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type bjacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 186.34 iterations 10000
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8075 with 20 processors, by luciano.siqueira Wed Sep  9 17:38:42 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           4.387e+01     1.000   4.387e+01
Objects:              7.700e+01     1.000   7.700e+01
Flop:                 5.447e+10     1.000   5.447e+10  1.089e+12
Flop/sec:             1.242e+09     1.000   1.242e+09  2.483e+10
MPI Messages:         2.001e+04     2.000   1.901e+04  3.802e+05
MPI Message Lengths:  1.600e+08     2.000   7.999e+03  3.041e+09
MPI Reductions:       2.969e+04     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 4.3871e+01 100.0%  1.0893e+12 100.0%  3.802e+05 100.0%  7.999e+03      100.0%  2.968e+04 100.0%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 3.5970e-0347.9 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 3.5655e-0374.9 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult            10002 1.0 1.0703e+01 1.0 4.50e+09 1.0 3.8e+05 8.0e+03 0.0e+00 24  8100100  0  24  8100100  0  8403       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 3.6202e-0335.9 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6686e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDotNorm2        10000 1.0 2.4447e+00 1.1 2.00e+09 1.0 0.0e+00 0.0e+00 1.0e+04  5  4  0  0 34   5  4  0  0 34 16362       0      0 0.00e+00    0 0.00e+00  0
VecMDot             9666 1.0 7.6477e+00 1.0 1.45e+10 1.0 0.0e+00 0.0e+00 9.7e+03 17 27  0  0 33  17 27  0  0 33 37893       0      0 0.00e+00    0 0.00e+00  0
VecNorm            10002 1.0 8.6237e-01 1.1 1.00e+09 1.0 0.0e+00 0.0e+00 1.0e+04  2  2  0  0 34   2  2  0  0 34 23197       0      0 0.00e+00    0 0.00e+00  0
VecScale           20000 1.0 4.6867e-01 1.0 1.00e+09 1.0 0.0e+00 0.0e+00 0.0e+00  1  2  0  0  0   1  2  0  0  0 42674       0      0 0.00e+00    0 0.00e+00  0
VecSet                 3 1.0 2.1839e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY            20001 1.0 9.7107e-01 1.0 2.00e+09 1.0 0.0e+00 0.0e+00 0.0e+00  2  4  0  0  0   2  4  0  0  0 41194       0      0 0.00e+00    0 0.00e+00  0
VecAYPX                1 1.0 1.6881e-04 1.1 5.00e+04 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  5924       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY           19332 1.0 1.9263e+01 1.0 2.90e+10 1.0 0.0e+00 0.0e+00 0.0e+00 44 53  0  0  0  44 53  0  0  0 30088       0      0 0.00e+00    0 0.00e+00  0
VecPointwiseMult   10000 1.0 1.5452e+00 1.0 5.00e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3  1  0  0  0   3  1  0  0  0  6472       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin    10002 1.0 1.5289e-01 1.6 0.00e+00 0.0 3.8e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd      10002 1.0 4.7449e-01 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.5940e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.1757e-03 3.0 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin     10002 1.0 1.3162e-01 1.7 0.00e+00 0.0 3.8e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd       10002 1.0 4.6138e-01 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack             10002 1.0 3.7902e-02 6.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack           10002 1.0 5.3316e-03 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 1.4952e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 4.3501e+01 1.0 5.45e+10 1.0 3.8e+05 8.0e+03 3.0e+04 99100100100100  99100100100100 25042       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 1.3920e-06 2.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply            10000 1.0 1.5575e+00 1.0 5.00e+08 1.0 0.0e+00 0.0e+00 2.0e+00  4  1  0  0  0   4  1  0  0  0  6421       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      8013484     0.
         Vec Scatter     1              1          816     0.
              Vector    67             67     26123648     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1480     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.14e-08
Average time for MPI_Barrier(): 1.17954e-05
Average time for zero size MPI_Send(): 6.37715e-06
#PETSc Option Table entries:
-ksp_type gcr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type jacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.0316741 iterations 6946
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8075 with 20 processors, by luciano.siqueira Wed Sep  9 17:39:26 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           4.173e+01     1.000   4.173e+01
Objects:              7.600e+01     1.000   7.600e+01
Flop:                 4.092e+10     1.000   4.092e+10  8.183e+11
Flop/sec:             9.805e+08     1.000   9.805e+08  1.961e+10
MPI Messages:         1.390e+04     2.000   1.320e+04  2.641e+05
MPI Message Lengths:  1.112e+08     2.000   7.998e+03  2.112e+09
MPI Reductions:       2.063e+04     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 4.1733e+01 100.0%  8.1834e+11 100.0%  2.641e+05 100.0%  7.998e+03      100.0%  2.062e+04 100.0%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 5.8051e-0338.7 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 5.7023e-03124.1 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             6948 1.0 7.8170e+00 1.0 3.13e+09 1.0 2.6e+05 8.0e+03 0.0e+00 18  8100100  0  18  8100100  0  7992       0      0 0.00e+00    0 0.00e+00  0
MatSOR              6946 1.0 1.1224e+01 1.0 3.44e+09 1.0 0.0e+00 0.0e+00 0.0e+00 27  8  0  0  0  27  8  0  0  0  6137       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 5.7541e-0358.3 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6141e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDotNorm2         6946 1.0 1.9225e+00 1.2 1.39e+09 1.0 0.0e+00 0.0e+00 6.9e+03  4  3  0  0 34   4  3  0  0 34 14452       0      0 0.00e+00    0 0.00e+00  0
VecMDot             6714 1.0 5.8095e+00 1.0 1.01e+10 1.0 0.0e+00 0.0e+00 6.7e+03 14 25  0  0 33  14 25  0  0 33 34634       0      0 0.00e+00    0 0.00e+00  0
VecNorm             6948 1.0 6.4211e-01 1.1 6.95e+08 1.0 0.0e+00 0.0e+00 6.9e+03  1  2  0  0 34   1  2  0  0 34 21641       0      0 0.00e+00    0 0.00e+00  0
VecScale           13892 1.0 3.4060e-01 1.1 6.95e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1  2  0  0  0   1  2  0  0  0 40787       0      0 0.00e+00    0 0.00e+00  0
VecSet                 3 1.0 2.1792e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY            13893 1.0 7.2022e-01 1.1 1.39e+09 1.0 0.0e+00 0.0e+00 0.0e+00  2  3  0  0  0   2  3  0  0  0 38580       0      0 0.00e+00    0 0.00e+00  0
VecAYPX                1 1.0 1.5959e-04 1.0 5.00e+04 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  6266       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY           13428 1.0 1.3637e+01 1.0 2.01e+10 1.0 0.0e+00 0.0e+00 0.0e+00 32 49  0  0  0  32 49  0  0  0 29508       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     6948 1.0 1.1825e-01 1.6 0.00e+00 0.0 2.6e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       6948 1.0 7.7891e-01 2.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.6500e-06 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.2134e-03 3.1 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      6948 1.0 1.0178e-01 1.7 0.00e+00 0.0 2.6e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        6948 1.0 7.7004e-01 2.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              6948 1.0 2.8387e-0210.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            6948 1.0 3.8976e-03 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               1 1.0 1.4469e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 4.1363e+01 1.0 4.09e+10 1.0 2.6e+05 8.0e+03 2.1e+04 99100100100100  99100100100100 19784       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 5.3800e-07 2.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
PCApply             6946 1.0 1.1238e+01 1.0 3.44e+09 1.0 0.0e+00 0.0e+00 0.0e+00 27  8  0  0  0  27  8  0  0  0  6129       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    66             66     25721920     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     1              1         1480     0.
      Preconditioner     1              1          872     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.05e-08
Average time for MPI_Barrier(): 1.39348e-05
Average time for zero size MPI_Send(): 6.2846e-06
#PETSc Option Table entries:
-ksp_type gcr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type sor
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

Norm of error 0.0315697 iterations 3254
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./solver on a arch-linux2-c-opt named sdumont8075 with 20 processors, by luciano.siqueira Wed Sep  9 17:39:59 2020
Using 24 OpenMP threads
Using Petsc Development GIT revision: v3.13.4-619-g721b214e6c  GIT Date: 2020-08-02 05:18:40 +0000

                         Max       Max/Min     Avg       Total
Time (sec):           3.136e+01     1.000   3.136e+01
Objects:              9.900e+01     1.000   9.900e+01
Flop:                 2.355e+10     1.001   2.355e+10  4.711e+11
Flop/sec:             7.512e+08     1.001   7.511e+08  1.502e+10
MPI Messages:         1.304e+04     2.000   1.239e+04  2.478e+05
MPI Message Lengths:  1.043e+08     2.000   7.998e+03  1.982e+09
MPI Reductions:       9.698e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.1358e+01 100.0%  4.7106e+11 100.0%  2.478e+05 100.0%  7.998e+03      100.0%  9.691e+03  99.9%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
   GPU Mflop/s: 10e-6 * (sum of flop on GPU over all processors)/(max GPU time over all processors)
   CpuToGpu Count: total number of CPU to GPU copies per processor
   CpuToGpu Size (Mbytes): 10e-6 * (total size of CPU to GPU copies per processor)
   GpuToCpu Count: total number of GPU to CPU copies per processor
   GpuToCpu Size (Mbytes): 10e-6 * (total size of GPU to CPU copies per processor)
   GPU %F: percent flops on GPU in this event
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total   GPU    - CpuToGpu -   - GpuToCpu - GPU
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s Mflop/s Count   Size   Count   Size  %F
---------------------------------------------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          2 1.0 4.2998e-0351.5 0.00e+00 0.0 3.8e+01 4.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
BuildTwoSidedF         1 1.0 4.2442e-0388.1 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatMult             6520 1.0 7.7945e+00 1.1 2.93e+09 1.0 2.5e+05 8.0e+03 0.0e+00 24 12100100  0  24 12100100  0  7522       0      0 0.00e+00    0 0.00e+00  0
MatSOR              6519 1.0 1.0670e+01 1.1 3.23e+09 1.0 0.0e+00 0.0e+00 0.0e+00 33 14  0  0  0  33 14  0  0  0  6058       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyBegin       1 1.0 4.2989e-0342.1 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
MatAssemblyEnd         1 1.0 2.6747e-02 1.0 0.00e+00 0.0 7.6e+01 2.0e+03 5.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecDotNorm2         3254 1.0 1.0081e+00 1.3 6.51e+08 1.0 0.0e+00 0.0e+00 3.3e+03  3  3  0  0 34   3  3  0  0 34 12912       0      0 0.00e+00    0 0.00e+00  0
VecMDot             3155 1.0 2.8803e+00 1.1 4.71e+09 1.0 0.0e+00 0.0e+00 3.2e+03  9 20  0  0 33   9 20  0  0 33 32722       0      0 0.00e+00    0 0.00e+00  0
VecNorm             3267 1.0 3.0834e-01 1.1 3.27e+08 1.0 0.0e+00 0.0e+00 3.3e+03  1  1  0  0 34   1  1  0  0 34 21191       0      0 0.00e+00    0 0.00e+00  0
VecScale            6519 1.0 1.6543e-01 1.1 3.26e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1  1  0  0  0   1  1  0  0  0 39406       0      0 0.00e+00    0 0.00e+00  0
VecCopy             6509 1.0 2.8954e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecSet              6513 1.0 2.5710e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecAXPY             6510 1.0 3.3282e-01 1.0 6.51e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1  3  0  0  0   1  3  0  0  0 39121       0      0 0.00e+00    0 0.00e+00  0
VecAYPX             6509 1.0 9.4219e-01 1.1 4.88e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3  2  0  0  0   3  2  0  0  0 10362       0      0 0.00e+00    0 0.00e+00  0
VecAXPBYCZ          3254 1.0 6.0429e-01 1.1 8.14e+08 1.0 0.0e+00 0.0e+00 0.0e+00  2  3  0  0  0   2  3  0  0  0 26924       0      0 0.00e+00    0 0.00e+00  0
VecMAXPY            6301 1.0 6.5409e+00 1.0 9.42e+09 1.0 0.0e+00 0.0e+00 0.0e+00 20 40  0  0  0  20 40  0  0  0 28806       0      0 0.00e+00    0 0.00e+00  0
VecScatterBegin     6520 1.0 1.0395e-01 1.4 0.00e+00 0.0 2.5e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
VecScatterEnd       6520 1.0 1.1300e+00 4.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
VecNormalize          11 1.0 7.3698e-03 1.1 1.65e+06 1.0 0.0e+00 0.0e+00 1.1e+01  0  0  0  0  0   0  0  0  0  0  4478       0      0 0.00e+00    0 0.00e+00  0
SFSetGraph             1 1.0 9.4970e-06 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFSetUp                1 1.0 1.2689e-03 3.2 0.00e+00 0.0 7.6e+01 2.0e+03 1.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpBegin      6520 1.0 9.3873e-02 1.5 0.00e+00 0.0 2.5e+05 8.0e+03 0.0e+00  0  0100100  0   0  0100100  0     0       0      0 0.00e+00    0 0.00e+00  0
SFBcastOpEnd        6520 1.0 1.1209e+00 4.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFPack              6520 1.0 2.2781e-02 7.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
SFUnpack            6520 1.0 3.7924e-03 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0       0      0 0.00e+00    0 0.00e+00  0
KSPSetUp               2 1.0 1.5077e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.5e+01  0  0  0  0  0   0  0  0  0  0  3150       0      0 0.00e+00    0 0.00e+00  0
KSPSolve               1 1.0 3.0979e+01 1.0 2.36e+10 1.0 2.5e+05 8.0e+03 9.7e+03 99100100100100  99100100100100 15205       0      0 0.00e+00    0 0.00e+00  0
KSPGMRESOrthog        10 1.0 1.2012e-02 1.1 1.10e+07 1.0 0.0e+00 0.0e+00 1.0e+01  0  0  0  0  0   0  0  0  0  0 18315       0      0 0.00e+00    0 0.00e+00  0
PCSetUp                1 1.0 1.3701e-01 1.0 2.38e+07 1.0 3.8e+02 8.0e+03 2.3e+01  0  0  0  0  0   0  0  0  0  0  3467       0      0 0.00e+00    0 0.00e+00  0
PCApply             3265 1.0 1.6416e+01 1.0 6.00e+09 1.0 1.2e+05 8.0e+03 0.0e+00 52 25 50 50  0  52 25 50 50  0  7306       0      0 0.00e+00    0 0.00e+00  0
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3      9213484     0.
         Vec Scatter     1              1          816     0.
              Vector    85             85     33354752     0.
           Index Set     2              2         5800     0.
   Star Forest Graph     1              1         1136     0.
       Krylov Solver     3              3        33496     0.
      Preconditioner     3              3         2936     0.
              Viewer     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 3.22e-08
Average time for MPI_Barrier(): 1.36292e-05
Average time for zero size MPI_Send(): 6.62245e-06
#PETSc Option Table entries:
-ksp_type gcr
-log_view
-m 1000
-mat_type aij
-n 1000
-pc_type mg
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=no --with-openmp=1 --with-cuda=1 --download-superlu_dist --download-mumps --download-hypre --download-scalapack --download-spai --download-parms --download-slepc --download-openmpi=yes --download-openmpi-configure-arguments=--with-cuda=/usr/local/cuda CUDAOPTFLAGS= COPTFLAGS= CXXOPTFLAGS= FOPTFLAGS=
-----------------------------------------
Libraries compiled on 2020-08-02 15:41:14 on petsc-gpu 
Machine characteristics: Linux-5.4.0-42-generic-x86_64-with-debian-10.5
Using PETSc directory: /opt/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: /opt/petsc/arch-linux2-c-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -fopenmp   
Using Fortran compiler: /opt/petsc/arch-linux2-c-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -fopenmp    
-----------------------------------------

Using include paths: -I/opt/petsc/include -I/opt/petsc/arch-linux2-c-opt/include -I/usr/local/cuda/include
-----------------------------------------

Using C linker: /opt/petsc/arch-linux2-c-opt/bin/mpicc
Using Fortran linker: /opt/petsc/arch-linux2-c-opt/bin/mpif90
Using libraries: -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/opt/petsc/arch-linux2-c-opt/lib -L/opt/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/cuda/lib64 -L/usr/local/cuda/lib64 -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lsuperlu_dist -lparms -lspai -llapack -lblas -lX11 -lm -lcufft -lcublas -lcudart -lcusparse -lcusolver -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lstdc++ -ldl
-----------------------------------------

